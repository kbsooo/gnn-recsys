{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN ê¸°ë°˜ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ\n",
    "# V11A_FAST: In-batch Negative Mining (ë¹ ë¥¸ ë²„ì „)\n",
    "# Step 2 - Breakthrough experiment (MacBook M4 ìµœì í™”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "í™˜ê²½ ì„¤ì •\n",
      "============================================================\n",
      "PyTorch ë²„ì „: 2.9.0\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: False\n",
      "MPS ì‚¬ìš© ê°€ëŠ¥: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"í™˜ê²½ ì„¤ì •\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ë¡œì»¬ í™˜ê²½ ê²½ë¡œ ì„¤ì • (MacBook M4 ìµœì í™”)\n# notebooks í´ë”ì—ì„œ ì‹¤í–‰ëœë‹¤ê³  ê°€ì •\nBASE_PATH = '..'\nprint(f\"ğŸ’» ë¡œì»¬ í™˜ê²½ (MacBook M4)\")\nprint(f\"   Base path: {BASE_PATH}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BASE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (V11A_FAST - In-batch Negative Mining)\u001b[39;00m\n\u001b[32m      2\u001b[39m CONFIG = {\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Filtering ê¸°ì¤€ (V6ì™€ ë™ì¼)\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmin_user_interactions\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m30\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmin_item_interactions\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m10\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrating_threshold\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Split ë¹„ìœ¨ (V6ì™€ ë™ì¼)\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrain_ratio\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.70\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvalid_ratio\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.15\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtest_ratio\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.15\u001b[39m,\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# ëª¨ë¸ íŒŒë¼ë¯¸í„° (V9cì™€ ë™ì¼)\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33membedding_dim\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m64\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_layers\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m2\u001b[39m,\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# í•™ìŠµ íŒŒë¼ë¯¸í„°\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.001\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1e-4\u001b[39m,\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m512\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m100\u001b[39m,\n\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpatience\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m20\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mneg_ratio\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m6\u001b[39m,\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# InfoNCE íŒŒë¼ë¯¸í„°\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.2\u001b[39m,\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# í‰ê°€\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m10\u001b[39m,\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# ì‹œìŠ¤í…œ\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.backends.mps.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     33\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m42\u001b[39m,\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# ê²½ë¡œ (BASE_PATH ê¸°ë°˜)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdata_dir\u001b[39m\u001b[33m'\u001b[39m: os.path.join(\u001b[43mBASE_PATH\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprocessed_dir\u001b[39m\u001b[33m'\u001b[39m: os.path.join(BASE_PATH, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_dir\u001b[39m\u001b[33m'\u001b[39m: os.path.join(BASE_PATH, \u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mresult_dir\u001b[39m\u001b[33m'\u001b[39m: os.path.join(BASE_PATH, \u001b[33m'\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     40\u001b[39m }\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# í´ë” ìƒì„±\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dir_path \u001b[38;5;129;01min\u001b[39;00m [CONFIG[\u001b[33m'\u001b[39m\u001b[33mprocessed_dir\u001b[39m\u001b[33m'\u001b[39m], CONFIG[\u001b[33m'\u001b[39m\u001b[33mmodel_dir\u001b[39m\u001b[33m'\u001b[39m], CONFIG[\u001b[33m'\u001b[39m\u001b[33mresult_dir\u001b[39m\u001b[33m'\u001b[39m]]:\n",
      "\u001b[31mNameError\u001b[39m: name 'BASE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (V11A_FAST - In-batch Negative Mining)\n",
    "CONFIG = {\n",
    "    # Filtering ê¸°ì¤€ (V6ì™€ ë™ì¼)\n",
    "    'min_user_interactions': 30,\n",
    "    'min_item_interactions': 10,\n",
    "    'rating_threshold': None,\n",
    "    \n",
    "    # Split ë¹„ìœ¨ (V6ì™€ ë™ì¼)\n",
    "    'train_ratio': 0.70,\n",
    "    'valid_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    \n",
    "    # ëª¨ë¸ íŒŒë¼ë¯¸í„° (V9cì™€ ë™ì¼)\n",
    "    'embedding_dim': 64,\n",
    "    'n_layers': 2,\n",
    "    \n",
    "    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'batch_size': 512,\n",
    "    'epochs': 100,\n",
    "    'patience': 20,\n",
    "    'neg_ratio': 6,\n",
    "    \n",
    "    # InfoNCE íŒŒë¼ë¯¸í„°\n",
    "    'temperature': 0.2,\n",
    "    \n",
    "    # í‰ê°€\n",
    "    'top_k': 10,\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ\n",
    "    'device': 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'),\n",
    "    'seed': 42,\n",
    "    \n",
    "    # ê²½ë¡œ (BASE_PATH ê¸°ë°˜)\n",
    "    'data_dir': os.path.join(BASE_PATH, 'data'),\n",
    "    'processed_dir': os.path.join(BASE_PATH, 'data', 'processed'),\n",
    "    'model_dir': os.path.join(BASE_PATH, 'models'),\n",
    "    'result_dir': os.path.join(BASE_PATH, 'results'),\n",
    "}\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "for dir_path in [CONFIG['processed_dir'], CONFIG['model_dir'], CONFIG['result_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Random seed ê³ ì •\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V11A_FAST ì„¤ì • ì™„ë£Œ! (In-batch Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"ë°ì´í„° ê²½ë¡œ: {CONFIG['processed_dir']}\")\n",
    "print(\"\\nâš¡ í•µì‹¬ ë³€ê²½: Random â†’ In-batch Negative (FAST!)\")\n",
    "print(f\"  V9cê¹Œì§€: ì™„ì „ ëœë¤ ìƒ˜í”Œë§\")\n",
    "print(f\"  V11a: Semi-hard (ëŠë¦¼) + In-batch\")\n",
    "print(f\"  V11a_fast: In-batchë§Œ ì‚¬ìš© (ë¹ ë¦„!)\")\n",
    "print(f\"\\nì „ëµ:\")\n",
    "print(f\"  1. In-batch negatives (100%): ê°™ì€ ë°°ì¹˜ ë‚´ ë‹¤ë¥¸ ìœ ì €ì˜ positive items\")\n",
    "print(f\"     â†’ ë‹¤ë¥¸ ìœ ì €ëŠ” ì¢‹ì•„í•˜ì§€ë§Œ ì´ ìœ ì €ëŠ” ì‹«ì–´í•˜ëŠ” items\")\n",
    "print(f\"     â†’ ëª¨ë¸ forward ë¶ˆí•„ìš”! ì†ë„ V9cì™€ ê±°ì˜ ë™ì¼\")\n",
    "print(f\"  2. ë¶€ì¡±í•˜ë©´ randomìœ¼ë¡œ ì±„ì›€\")\n",
    "print(f\"\\nì¥ì :\")\n",
    "print(f\"  - Collaborative filtering ë³¸ì§ˆ í•™ìŠµ (ë‹¤ë¥¸ ìœ ì € ì·¨í–¥ êµ¬ë¶„)\")\n",
    "print(f\"  - ëª¨ë¸ forward ì—†ì–´ì„œ ë§¤ìš° ë¹ ë¦„ (V9c ì†ë„)\")\n",
    "print(f\"  - MacBook M4ì—ì„œë„ ë¹ ë¥´ê²Œ ì‹¤í–‰ ê°€ëŠ¥\")\n",
    "print(f\"\\nì˜ˆìƒ íš¨ê³¼:\")\n",
    "print(f\"  - ëœë¤ë³´ë‹¤ ë” ì˜ë¯¸ìˆëŠ” negatives\")\n",
    "print(f\"  - ì˜ˆìƒ ê°œì„ : +3~5% Recall@10\")\n",
    "print(f\"\\nê¸°íƒ€ ì„¤ì • (V9cì™€ ë™ì¼):\")\n",
    "print(f\"  Loss: InfoNCE (temperature={CONFIG['temperature']})\")\n",
    "print(f\"  Layers: {CONFIG['n_layers']}\")\n",
    "print(f\"  Embedding dim: {CONFIG['embedding_dim']}\")\n",
    "print(f\"  Negative ratio: {CONFIG['neg_ratio']}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ¯ ëª©í‘œ: Recall@10 > 17% (V9c ëŒ€ë¹„ ê°œì„  + ë¹ ë¥¸ ì†ë„!)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (V11A - Hard Negative Mining)\n",
    "CONFIG = {\n",
    "    # Filtering ê¸°ì¤€ (V6ì™€ ë™ì¼)\n",
    "    'min_user_interactions': 30,\n",
    "    'min_item_interactions': 10,\n",
    "    'rating_threshold': None,\n",
    "    \n",
    "    # Split ë¹„ìœ¨ (V6ì™€ ë™ì¼)\n",
    "    'train_ratio': 0.70,\n",
    "    'valid_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    \n",
    "    # ëª¨ë¸ íŒŒë¼ë¯¸í„° (V9cì™€ ë™ì¼)\n",
    "    'embedding_dim': 64,\n",
    "    'n_layers': 2,\n",
    "    \n",
    "    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'batch_size': 512,\n",
    "    'epochs': 100,\n",
    "    'patience': 20,\n",
    "    'neg_ratio': 6,\n",
    "    \n",
    "    # InfoNCE íŒŒë¼ë¯¸í„°\n",
    "    'temperature': 0.2,\n",
    "    \n",
    "    # â­ Hard Negative Mining íŒŒë¼ë¯¸í„°\n",
    "    'hard_neg_ratio': 0.5,  # 50% hard negatives, 50% random\n",
    "    \n",
    "    # í‰ê°€\n",
    "    'top_k': 10,\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ\n",
    "    'device': 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'),\n",
    "    'seed': 42,\n",
    "    \n",
    "    # ê²½ë¡œ (BASE_PATH ê¸°ë°˜)\n",
    "    'data_dir': os.path.join(BASE_PATH, 'data'),\n",
    "    'processed_dir': os.path.join(BASE_PATH, 'data', 'processed'),\n",
    "    'model_dir': os.path.join(BASE_PATH, 'models'),\n",
    "    'result_dir': os.path.join(BASE_PATH, 'results'),\n",
    "}\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "for dir_path in [CONFIG['processed_dir'], CONFIG['model_dir'], CONFIG['result_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Random seed ê³ ì •\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V11A ì„¤ì • ì™„ë£Œ! (Hard Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"ë°ì´í„° ê²½ë¡œ: {CONFIG['processed_dir']}\")\n",
    "print(\"\\nğŸ”¥ í•µì‹¬ ë³€ê²½: Random Negative â†’ Hard Negative Mining\")\n",
    "print(f\"  V9cê¹Œì§€: ì™„ì „ ëœë¤ ìƒ˜í”Œë§ (ë„ˆë¬´ ì‰¬ìš´ negative)\")\n",
    "print(f\"  V11a: ëª¨ë¸ ê¸°ë°˜ hard negative ì„ íƒ\")\n",
    "print(f\"\\nì „ëµ:\")\n",
    "print(f\"  1. Semi-hard negatives (50%): ëª¨ë¸ ì ìˆ˜ ì¤‘ê°„ ë²”ìœ„ [0.3-0.7]\")\n",
    "print(f\"     â†’ ë„ˆë¬´ ì‰½ì§€ë„, ë„ˆë¬´ ì–´ë µì§€ë„ ì•Šì€ negatives\")\n",
    "print(f\"  2. In-batch negatives (50%): ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ì˜ positive items\")\n",
    "print(f\"     â†’ ë‹¤ë¥¸ ìœ ì €ëŠ” ì¢‹ì•„í•˜ì§€ë§Œ ì´ ìœ ì €ëŠ” ì‹«ì–´í•˜ëŠ” items\")\n",
    "print(f\"\\nê¸°ëŒ€ íš¨ê³¼:\")\n",
    "print(f\"  - ëœë¤ë³´ë‹¤ í•™ìŠµì´ ì–´ë ¤ì›Œì ¸ì„œ ë” ê°•í•œ discriminative power\")\n",
    "print(f\"  - Embedding spaceê°€ ë” êµ¬ì¡°í™”ë¨\")\n",
    "print(f\"  - ì˜ˆìƒ ê°œì„ : +4~8% Recall@10\")\n",
    "print(f\"\\nê¸°íƒ€ ì„¤ì • (V9cì™€ ë™ì¼):\")\n",
    "print(f\"  Loss: InfoNCE (temperature={CONFIG['temperature']})\")\n",
    "print(f\"  Layers: {CONFIG['n_layers']}\")\n",
    "print(f\"  Embedding dim: {CONFIG['embedding_dim']}\")\n",
    "print(f\"  Negative ratio: {CONFIG['neg_ratio']}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ¯ ëª©í‘œ: Recall@10 > 18% (V9cì˜ 15.78% ëŒ€ë¹„ breakthrough!)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (V11A - Hard Negative Mining)\n",
    "CONFIG = {\n",
    "    # Filtering ê¸°ì¤€ (V6ì™€ ë™ì¼)\n",
    "    'min_user_interactions': 30,\n",
    "    'min_item_interactions': 10,\n",
    "    'rating_threshold': None,\n",
    "    \n",
    "    # Split ë¹„ìœ¨ (V6ì™€ ë™ì¼)\n",
    "    'train_ratio': 0.70,\n",
    "    'valid_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    \n",
    "    # ëª¨ë¸ íŒŒë¼ë¯¸í„° (V9cì™€ ë™ì¼)\n",
    "    'embedding_dim': 64,\n",
    "    'n_layers': 2,\n",
    "    \n",
    "    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'batch_size': 512,\n",
    "    'epochs': 100,\n",
    "    'patience': 20,\n",
    "    'neg_ratio': 6,\n",
    "    \n",
    "    # InfoNCE íŒŒë¼ë¯¸í„°\n",
    "    'temperature': 0.2,\n",
    "    \n",
    "    # â­ Hard Negative Mining íŒŒë¼ë¯¸í„°\n",
    "    'hard_neg_ratio': 0.5,  # 50% hard negatives, 50% random\n",
    "    \n",
    "    # í‰ê°€\n",
    "    'top_k': 10,\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ\n",
    "    'device': 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'),\n",
    "    'seed': 42,\n",
    "    \n",
    "    # ê²½ë¡œ\n",
    "    'data_dir': str(DATA_ROOT),\n",
    "    'processed_dir': str(PROCESSED_ROOT),\n",
    "    'model_dir': str(MODEL_ROOT),\n",
    "    'result_dir': str(RESULT_ROOT),\n",
    "}\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "for dir_path in [CONFIG['processed_dir'], CONFIG['model_dir'], CONFIG['result_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Random seed ê³ ì •\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V11A ì„¤ì • ì™„ë£Œ! (Hard Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(\"\\nğŸ”¥ í•µì‹¬ ë³€ê²½: Random Negative â†’ Hard Negative Mining\")\n",
    "print(f\"  V9cê¹Œì§€: ì™„ì „ ëœë¤ ìƒ˜í”Œë§ (ë„ˆë¬´ ì‰¬ìš´ negative)\")\n",
    "print(f\"  V11a: ëª¨ë¸ ê¸°ë°˜ hard negative ì„ íƒ\")\n",
    "print(f\"\\nì „ëµ:\")\n",
    "print(f\"  1. Semi-hard negatives (50%): ëª¨ë¸ ì ìˆ˜ ì¤‘ê°„ ë²”ìœ„ [0.3-0.7]\")\n",
    "print(f\"     â†’ ë„ˆë¬´ ì‰½ì§€ë„, ë„ˆë¬´ ì–´ë µì§€ë„ ì•Šì€ negatives\")\n",
    "print(f\"  2. In-batch negatives (50%): ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ì˜ positive items\")\n",
    "print(f\"     â†’ ë‹¤ë¥¸ ìœ ì €ëŠ” ì¢‹ì•„í•˜ì§€ë§Œ ì´ ìœ ì €ëŠ” ì‹«ì–´í•˜ëŠ” items\")\n",
    "print(f\"\\nê¸°ëŒ€ íš¨ê³¼:\")\n",
    "print(f\"  - ëœë¤ë³´ë‹¤ í•™ìŠµì´ ì–´ë ¤ì›Œì ¸ì„œ ë” ê°•í•œ discriminative power\")\n",
    "print(f\"  - Embedding spaceê°€ ë” êµ¬ì¡°í™”ë¨\")\n",
    "print(f\"  - ì˜ˆìƒ ê°œì„ : +4~8% Recall@10\")\n",
    "print(f\"\\nê¸°íƒ€ ì„¤ì • (V9cì™€ ë™ì¼):\")\n",
    "print(f\"  Loss: InfoNCE (temperature={CONFIG['temperature']})\")\n",
    "print(f\"  Layers: {CONFIG['n_layers']}\")\n",
    "print(f\"  Embedding dim: {CONFIG['embedding_dim']}\")\n",
    "print(f\"  Negative ratio: {CONFIG['neg_ratio']}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ¯ ëª©í‘œ: Recall@10 > 18% (V9cì˜ 15.78% ëŒ€ë¹„ breakthrough!)\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V6ì—ì„œ ì „ì²˜ë¦¬í•œ ë°ì´í„° ë¡œë“œ\n",
    "print(\"=\" * 60)\n",
    "print(\"V6 ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'train_split_v6.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'valid_split_v6.csv'))\n",
    "test_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'test_split_v6.csv'))\n",
    "\n",
    "with open(os.path.join(CONFIG['processed_dir'], 'id_mappings_v6.pkl'), 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "n_users = len(mappings['user_id_map'])\n",
    "n_items = len(mappings['item_id_map'])\n",
    "\n",
    "print(f\"\\në°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"  Users: {n_users}\")\n",
    "print(f\"  Items: {n_items}\")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Valid: {len(valid_df):,}\")\n",
    "print(f\"  Test:  {len(test_df):,}\")\n",
    "print(\"\\nâœ… V6ì™€ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš© â†’ ê³µì •í•œ ë¹„êµ ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Mount (Colab only)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print('âœ… Google Drive mounted')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('âš ï¸  Local mode') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â­ In-batch Negative Mining (FAST!)\n",
    "def create_user_item_dict(df):\n",
    "    \"\"\"Userë³„ë¡œ ìƒí˜¸ì‘ìš©í•œ item ì§‘í•© ìƒì„±\"\"\"\n",
    "    user_items = defaultdict(set)\n",
    "    for _, row in df.iterrows():\n",
    "        user_items[row['user_id']].add(row['item_id'])\n",
    "    return user_items\n",
    "\n",
    "def inbatch_negative_sampling(batch_df, user_items_dict, n_items, neg_ratio=6):\n",
    "    \"\"\"\n",
    "    In-batch Negative Mining (FAST!)\n",
    "    \n",
    "    ì „ëµ:\n",
    "    - ê°™ì€ ë°°ì¹˜ ë‚´ ë‹¤ë¥¸ ìœ ì €ë“¤ì˜ positive itemsë¥¼ negativeë¡œ ì‚¬ìš©\n",
    "    - \"ë‹¤ë¥¸ ìœ ì €ëŠ” ì¢‹ì•„í•˜ì§€ë§Œ ì´ ìœ ì €ëŠ” ì‹«ì–´í•˜ëŠ” items\"\n",
    "    - Collaborative filteringì˜ í•µì‹¬: ìœ ì € ê°„ ì·¨í–¥ ì°¨ì´ í•™ìŠµ\n",
    "    \n",
    "    Random sampling ëŒ€ë¹„ ì¥ì :\n",
    "    - Random: ëŒ€ë¶€ë¶„ ì•„ë¬´ë„ ì•ˆ ë³¸ ë¬´ê´€í•œ items (ë„ˆë¬´ ì‰¬ì›€)\n",
    "    - In-batch: ëˆ„êµ°ê°€ëŠ” ì¢‹ì•„í•˜ëŠ” items (ì˜ë¯¸ìˆëŠ” negative)\n",
    "    - ëª¨ë¸ forward ë¶ˆí•„ìš” â†’ ì†ë„ê°€ V9cì™€ ê±°ì˜ ë™ì¼!\n",
    "    \n",
    "    ì†ë„:\n",
    "    - V9c (random): ë°°ì¹˜ë‹¹ ~0.1ì´ˆ\n",
    "    - V11a (hard): ë°°ì¹˜ë‹¹ ~2ì´ˆ (ëª¨ë¸ forward ë•Œë¬¸)\n",
    "    - V11a_fast (in-batch): ë°°ì¹˜ë‹¹ ~0.1ì´ˆ (ëª¨ë¸ forward ì—†ìŒ!)\n",
    "    \"\"\"\n",
    "    pos_users = batch_df['user_id'].values\n",
    "    neg_users = []\n",
    "    neg_items = []\n",
    "    \n",
    "    # ë°°ì¹˜ ë‚´ ëª¨ë“  ìœ ì €ì˜ positive items ìˆ˜ì§‘ (í•œ ë²ˆë§Œ)\n",
    "    batch_user_items = defaultdict(list)\n",
    "    for user_id in pos_users:\n",
    "        batch_user_items[user_id] = list(user_items_dict[user_id])\n",
    "    \n",
    "    # ê° ìœ ì €ë³„ë¡œ in-batch negatives ìƒì„±\n",
    "    for idx, user_id in enumerate(pos_users):\n",
    "        user_pos_items = user_items_dict[user_id]\n",
    "        \n",
    "        # ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ë“¤ì˜ positive items ìˆ˜ì§‘\n",
    "        candidates = []\n",
    "        for other_idx, other_user in enumerate(pos_users):\n",
    "            if other_idx != idx:  # ìê¸° ìì‹  ì œì™¸\n",
    "                other_items = batch_user_items[other_user]\n",
    "                # í˜„ì¬ ìœ ì €ì˜ positiveê°€ ì•„ë‹Œ ê²ƒë§Œ\n",
    "                candidates.extend([i for i in other_items if i not in user_pos_items])\n",
    "        \n",
    "        # neg_ratioê°œ ìƒ˜í”Œë§\n",
    "        if len(candidates) >= neg_ratio:\n",
    "            sampled = np.random.choice(candidates, neg_ratio, replace=False)\n",
    "        else:\n",
    "            # ë¶€ì¡±í•˜ë©´ randomìœ¼ë¡œ ì±„ì›€\n",
    "            sampled = list(candidates) if candidates else []\n",
    "            while len(sampled) < neg_ratio:\n",
    "                rand_item = random.randint(0, n_items - 1)\n",
    "                if rand_item not in user_pos_items and rand_item not in sampled:\n",
    "                    sampled.append(rand_item)\n",
    "        \n",
    "        for neg_item in sampled:\n",
    "            neg_users.append(user_id)\n",
    "            neg_items.append(neg_item)\n",
    "    \n",
    "    return np.array(neg_users), np.array(neg_items)\n",
    "\n",
    "train_user_items = create_user_item_dict(train_df)\n",
    "train_valid_user_items = create_user_item_dict(train_valid_df)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"In-batch Negative Mining í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âš¡ ì†ë„ ë¹„êµ:\")\n",
    "print(\"\\n[Random Sampling (V9c)]\")\n",
    "print(\"  - ë°°ì¹˜ë‹¹ ~0.1ì´ˆ\")\n",
    "print(\"  - ë„ˆë¬´ ì‰¬ìš´ negatives (ë¬´ê´€í•œ items)\")\n",
    "print(\"\\n[Hard Negative Mining (V11a) - ëŠë¦¼!]\")\n",
    "print(\"  - ë°°ì¹˜ë‹¹ ~2ì´ˆ (20ë°° ëŠë¦¼!)\")\n",
    "print(\"  - ë§¤ ë°°ì¹˜ë§ˆë‹¤ ëª¨ë¸ forward pass\")\n",
    "print(\"  - ëª¨ë“  item ì ìˆ˜ ê³„ì‚°\")\n",
    "print(\"\\n[In-batch Negative (V11a_fast) - ë¹ ë¦„!]\")\n",
    "print(\"  - ë°°ì¹˜ë‹¹ ~0.1ì´ˆ (V9cì™€ ë™ì¼!)\")\n",
    "print(\"  - ëª¨ë¸ forward ë¶ˆí•„ìš”\")\n",
    "print(\"  - ê°™ì€ ë°°ì¹˜ ë‚´ ë‹¤ë¥¸ ìœ ì € positiveë§Œ ì‚¬ìš©\")\n",
    "print(\"  - ì˜ë¯¸ìˆëŠ” negatives + ë¹ ë¥¸ ì†ë„!\")\n",
    "print(\"\\nğŸ¯ MacBook M4 ìµœì í™”: V9c ì†ë„ë¡œ ë” ì¢‹ì€ ì„±ëŠ¥!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph êµ¬ì„±\n",
    "print(\"=\" * 60)\n",
    "print(\"Graph êµ¬ì„±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_graph(df, n_users, n_items):\n",
    "    \"\"\"User-Item Bipartite Graph ìƒì„±\"\"\"\n",
    "    user_ids = df['user_id'].values\n",
    "    item_ids = df['item_id'].values + n_users\n",
    "    \n",
    "    edge_index = torch.tensor([\n",
    "        np.concatenate([user_ids, item_ids]),\n",
    "        np.concatenate([item_ids, user_ids])\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    print(f\"Node ìˆ˜: {n_users + n_items} (User: {n_users}, Item: {n_items})\")\n",
    "    print(f\"Edge ìˆ˜: {edge_index.shape[1]:,} (ì–‘ë°©í–¥)\")\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "print(\"\\n[1] Train Graph (í•™ìŠµìš©)\")\n",
    "train_edge_index = create_graph(train_df, n_users, n_items)\n",
    "\n",
    "print(\"\\n[2] Train+Valid Graph (Test í‰ê°€ìš©)\")\n",
    "train_valid_df = pd.concat([train_df, valid_df])\n",
    "train_valid_edge_index = create_graph(train_valid_df, n_users, n_items)\n",
    "\n",
    "print(\"\\nâœ… Graph ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training í•¨ìˆ˜ (In-batch Negative Mining ì ìš©)\n",
    "def train_one_epoch(model, edge_index, train_df, user_items_dict, n_items, \n",
    "                    optimizer, batch_size, neg_ratio, temperature, device):\n",
    "    \"\"\"1 epoch training with InfoNCE + In-batch Negative Mining (FAST!)\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_df_shuffled = train_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for start_idx in range(0, len(train_df_shuffled), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(train_df_shuffled))\n",
    "        batch_df = train_df_shuffled.iloc[start_idx:end_idx]\n",
    "        \n",
    "        pos_users = torch.tensor(batch_df['user_id'].values, dtype=torch.long).to(device)\n",
    "        pos_items = torch.tensor(batch_df['item_id'].values, dtype=torch.long).to(device)\n",
    "        \n",
    "        # â­ In-batch Negative Mining ì‚¬ìš©! (ëª¨ë¸ forward ë¶ˆí•„ìš”)\n",
    "        neg_users_np, neg_items_np = inbatch_negative_sampling(\n",
    "            batch_df, user_items_dict, n_items, neg_ratio\n",
    "        )\n",
    "        neg_users = torch.tensor(neg_users_np, dtype=torch.long).to(device)\n",
    "        neg_items = torch.tensor(neg_items_np, dtype=torch.long).to(device)\n",
    "        \n",
    "        pos_scores = model.predict(pos_users, pos_items, edge_index.to(device))\n",
    "        neg_scores = model.predict(neg_users, neg_items, edge_index.to(device))\n",
    "        \n",
    "        # InfoNCE Loss ì‚¬ìš©\n",
    "        loss = infonce_loss(pos_scores, neg_scores, neg_ratio, temperature)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "print(\"Training í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ! (InfoNCE + In-batch Negative - FAST!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGCN ëª¨ë¸ (V6ì™€ ë™ì¼)\n",
    "class LightGCNConv(MessagePassing):\n",
    "    \"\"\"LightGCN Convolution Layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "    \n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    \"\"\"LightGCN for Recommendation\"\"\"\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.1)\n",
    "        \n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, edge_index):\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        all_emb = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            all_emb = conv(all_emb, edge_index)\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.stack(embs, dim=0).mean(dim=0)\n",
    "        \n",
    "        user_final = final_emb[:self.n_users]\n",
    "        item_final = final_emb[self.n_users:]\n",
    "        \n",
    "        return user_final, item_final\n",
    "    \n",
    "    def predict(self, users, items, edge_index):\n",
    "        user_emb, item_emb = self.forward(edge_index)\n",
    "        user_emb = user_emb[users]\n",
    "        item_emb = item_emb[items]\n",
    "        scores = (user_emb * item_emb).sum(dim=1)\n",
    "        return scores\n",
    "\n",
    "print(\"LightGCN ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì¸ Training Loop (V11A_FAST - In-batch Negative Mining)\n",
    "model = LightGCN(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    embedding_dim=CONFIG['embedding_dim'],\n",
    "    n_layers=CONFIG['n_layers']\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'valid_precision': [],\n",
    "    'valid_recall': [],\n",
    "    'valid_ndcg': [],\n",
    "}\n",
    "\n",
    "best_recall = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training ì‹œì‘ (V11A_FAST - In-batch Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: LightGCN\")\n",
    "print(f\"  - Users: {n_users}, Items: {n_items}\")\n",
    "print(f\"  - Embedding dim: {CONFIG['embedding_dim']}\")\n",
    "print(f\"  - Layers: {CONFIG['n_layers']}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  - Total params: {total_params:,}\")\n",
    "print(f\"\\nâš¡ í•µì‹¬ ë³€ê²½: Random â†’ In-batch Negative (FAST!)\")\n",
    "print(f\"  V9c: Random sampling\")\n",
    "print(f\"  V11a: Hard negative (ë„ˆë¬´ ëŠë¦¼)\")\n",
    "print(f\"  V11a_fast: In-batchë§Œ (V9c ì†ë„!)\")\n",
    "print(f\"  â†’ MacBook M4 ìµœì í™”!\")\n",
    "print(f\"\\nLoss Function: InfoNCE (temperature={CONFIG['temperature']})\")\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Negative ratio: {CONFIG['neg_ratio']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_edge_index, train_df, train_user_items,\n",
    "        n_items, optimizer, CONFIG['batch_size'], \n",
    "        CONFIG['neg_ratio'], CONFIG['temperature'], CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    val_metrics = evaluate_model(\n",
    "        model, train_edge_index, valid_df, train_user_items,\n",
    "        n_items, k=CONFIG['top_k'], device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['valid_precision'].append(val_metrics[f'precision@{CONFIG[\"top_k\"]}'])\n",
    "    history['valid_recall'].append(val_metrics[f'recall@{CONFIG[\"top_k\"]}'])\n",
    "    history['valid_ndcg'].append(val_metrics[f'ndcg@{CONFIG[\"top_k\"]}'])\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{CONFIG['epochs']} | \"\n",
    "              f\"Loss: {train_loss:.4f} | \"\n",
    "              f\"P@{CONFIG['top_k']}: {val_metrics[f'precision@{CONFIG[\"top_k\"]}']:.4f} | \"\n",
    "              f\"R@{CONFIG['top_k']}: {val_metrics[f'recall@{CONFIG[\"top_k\"]}']:.4f} | \"\n",
    "              f\"NDCG@{CONFIG['top_k']}: {val_metrics[f'ndcg@{CONFIG[\"top_k\"]}']:.4f}\")\n",
    "    \n",
    "    current_recall = val_metrics[f'recall@{CONFIG[\"top_k\"]}']\n",
    "    if current_recall > best_recall:\n",
    "        best_recall = current_recall\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_fast_best.pth'))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training ì™„ë£Œ! (V11A_FAST - In-batch Negative Mining)\")\n",
    "print(f\"Best Recall@{CONFIG['top_k']}: {best_recall:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train Loss (InfoNCE + In-batch)', linewidth=2, color='#e74c3c')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Curve (V11A_FAST - In-batch Negative)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['valid_precision'], label=f'Precision@{CONFIG[\"top_k\"]}', linewidth=2)\n",
    "axes[1].plot(history['valid_recall'], label=f'Recall@{CONFIG[\"top_k\"]}', linewidth=2)\n",
    "axes[1].plot(history['valid_ndcg'], label=f'NDCG@{CONFIG[\"top_k\"]}', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Validation Metrics (V11A_FAST)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['result_dir'], 'training_curves_v11a_fast.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training í•¨ìˆ˜ (Hard Negative Mining ì ìš©)\n",
    "def train_one_epoch(model, edge_index, train_df, user_items_dict, n_items, \n",
    "                    optimizer, batch_size, neg_ratio, temperature, device):\n",
    "    \"\"\"1 epoch training with InfoNCE + Hard Negative Mining\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_df_shuffled = train_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for start_idx in range(0, len(train_df_shuffled), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(train_df_shuffled))\n",
    "        batch_df = train_df_shuffled.iloc[start_idx:end_idx]\n",
    "        \n",
    "        pos_users = torch.tensor(batch_df['user_id'].values, dtype=torch.long).to(device)\n",
    "        pos_items = torch.tensor(batch_df['item_id'].values, dtype=torch.long).to(device)\n",
    "        \n",
    "        # â­ Hard Negative Mining ì‚¬ìš©!\n",
    "        neg_users_np, neg_items_np = hard_negative_sampling(\n",
    "            model, edge_index, batch_df, user_items_dict, n_items, device, neg_ratio\n",
    "        )\n",
    "        neg_users = torch.tensor(neg_users_np, dtype=torch.long).to(device)\n",
    "        neg_items = torch.tensor(neg_items_np, dtype=torch.long).to(device)\n",
    "        \n",
    "        pos_scores = model.predict(pos_users, pos_items, edge_index.to(device))\n",
    "        neg_scores = model.predict(neg_users, neg_items, edge_index.to(device))\n",
    "        \n",
    "        # InfoNCE Loss ì‚¬ìš©\n",
    "        loss = infonce_loss(pos_scores, neg_scores, neg_ratio, temperature)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "print(\"Training í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ! (InfoNCE + Hard Negative Mining)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set í‰ê°€\n",
    "model.load_state_dict(torch.load(os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_fast_best.pth')))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Test Set í‰ê°€ (V11A_FAST - In-batch Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "v11a_fast_results = {}\n",
    "\n",
    "for k in [5, 10, 20]:\n",
    "    test_metrics = evaluate_model(\n",
    "        model, \n",
    "        train_valid_edge_index,\n",
    "        test_df, \n",
    "        train_valid_user_items,\n",
    "        n_items, \n",
    "        k=k, \n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    v11a_fast_results[k] = test_metrics\n",
    "    \n",
    "    print(f\"\\nTop-{k} ì¶”ì²œ:\")\n",
    "    print(f\"  Precision@{k}: {test_metrics[f'precision@{k}']:.4f}\")\n",
    "    print(f\"  Recall@{k}:    {test_metrics[f'recall@{k}']:.4f}\")\n",
    "    print(f\"  NDCG@{k}:      {test_metrics[f'ndcg@{k}']:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì¸ Training Loop (V11A - Hard Negative Mining)\n",
    "model = LightGCN(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    embedding_dim=CONFIG['embedding_dim'],\n",
    "    n_layers=CONFIG['n_layers']\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'valid_precision': [],\n",
    "    'valid_recall': [],\n",
    "    'valid_ndcg': [],\n",
    "}\n",
    "\n",
    "best_recall = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training ì‹œì‘ (V11A - Hard Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: LightGCN\")\n",
    "print(f\"  - Users: {n_users}, Items: {n_items}\")\n",
    "print(f\"  - Embedding dim: {CONFIG['embedding_dim']}\")\n",
    "print(f\"  - Layers: {CONFIG['n_layers']}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  - Total params: {total_params:,}\")\n",
    "print(f\"\\nğŸ”¥ í•µì‹¬ ë³€ê²½: Random â†’ Hard Negative Mining\")\n",
    "print(f\"  V9c: Random sampling (ì‰¬ìš´ negatives)\")\n",
    "print(f\"  V11a: Semi-hard (50%) + In-batch (50%)\")\n",
    "print(f\"  â†’ 4~8% Recall ê°œì„  ì˜ˆìƒ\")\n",
    "print(f\"\\nLoss Function: InfoNCE (temperature={CONFIG['temperature']})\")\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Negative ratio: {CONFIG['neg_ratio']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_edge_index, train_df, train_user_items,\n",
    "        n_items, optimizer, CONFIG['batch_size'], \n",
    "        CONFIG['neg_ratio'], CONFIG['temperature'], CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    val_metrics = evaluate_model(\n",
    "        model, train_edge_index, valid_df, train_user_items,\n",
    "        n_items, k=CONFIG['top_k'], device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['valid_precision'].append(val_metrics[f'precision@{CONFIG[\"top_k\"]}'])\n",
    "    history['valid_recall'].append(val_metrics[f'recall@{CONFIG[\"top_k\"]}'])\n",
    "    history['valid_ndcg'].append(val_metrics[f'ndcg@{CONFIG[\"top_k\"]}'])\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{CONFIG['epochs']} | \"\n",
    "              f\"Loss: {train_loss:.4f} | \"\n",
    "              f\"P@{CONFIG['top_k']}: {val_metrics[f'precision@{CONFIG[\"top_k\"]}']:.4f} | \"\n",
    "              f\"R@{CONFIG['top_k']}: {val_metrics[f'recall@{CONFIG[\"top_k\"]}']:.4f} | \"\n",
    "              f\"NDCG@{CONFIG['top_k']}: {val_metrics[f'ndcg@{CONFIG[\"top_k\"]}']:.4f}\")\n",
    "    \n",
    "    current_recall = val_metrics[f'recall@{CONFIG[\"top_k\"]}']\n",
    "    if current_recall > best_recall:\n",
    "        best_recall = current_recall\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_best.pth'))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training ì™„ë£Œ! (V11A - Hard Negative Mining)\")\n",
    "print(f\"Best Recall@{CONFIG['top_k']}: {best_recall:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"V11A_FAST vs V9C ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# V9c ê²°ê³¼ (baseline)\n",
    "v9c_results = {\n",
    "    10: {'precision@10': 0.2756, 'recall@10': 0.1578, 'ndcg@10': 0.3129}\n",
    "}\n",
    "\n",
    "print(\"\\ní•µì‹¬ ë³€ê²½ì‚¬í•­:\")\n",
    "print(f\"  V9c: Random negative sampling\")\n",
    "print(f\"  V11a_fast: In-batch negative mining (FAST!)\")\n",
    "\n",
    "print(\"\\nTest Set ì„±ëŠ¥ (Recall@10):\")\n",
    "print(f\"  V9c:        {v9c_results[10]['recall@10']:.4f} (15.78%)\")\n",
    "print(f\"  V11a_fast:  {v11a_fast_results[10]['recall@10']:.4f} ({v11a_fast_results[10]['recall@10']*100:.2f}%)\")\n",
    "\n",
    "improvement = (v11a_fast_results[10]['recall@10'] - v9c_results[10]['recall@10']) / v9c_results[10]['recall@10'] * 100\n",
    "print(f\"  ë³€í™”: {improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\nì „ì²´ ì§€í‘œ ë¹„êµ (Top-10):\")\n",
    "for metric in ['precision@10', 'recall@10', 'ndcg@10']:\n",
    "    v9c_val = v9c_results[10][metric]\n",
    "    v11a_fast_val = v11a_fast_results[10][metric]\n",
    "    change = (v11a_fast_val - v9c_val) / v9c_val * 100\n",
    "    symbol = 'âœ…' if change > 0 else 'âŒ'\n",
    "    print(f\"  {metric:15s}: V9c={v9c_val:.4f}, V11a_fast={v11a_fast_val:.4f} ({change:+.1f}%) {symbol}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ì‹¤í—˜ ê²°ê³¼ í•´ì„:\")\n",
    "if improvement > 3:\n",
    "    print(\"ğŸ‰ ì„±ê³µ!\")\n",
    "    print(\"   In-batch negative miningì´ íš¨ê³¼ì !\")\n",
    "    print(\"   â†’ Randomë³´ë‹¤ ì˜ë¯¸ìˆëŠ” negatives\")\n",
    "    print(\"   â†’ V9cì™€ ê°™ì€ ì†ë„ë¡œ ë” ì¢‹ì€ ì„±ëŠ¥\")\n",
    "    print(\"\\nâš¡ ì†ë„: V9cì™€ ê±°ì˜ ë™ì¼ (MacBook M4 OK!)\")\n",
    "elif improvement > 0:\n",
    "    print(\"âœ… ì†Œí­ ê°œì„ . In-batch negative íš¨ê³¼ ìˆìŒ\")\n",
    "    print(\"   â†’ V9c ì†ë„ë¡œ ì¡°ê¸ˆ ë” ë‚˜ì€ ì„±ëŠ¥\")\n",
    "elif improvement > -2:\n",
    "    print(\"â– ë¹„ìŠ·í•œ ìˆ˜ì¤€\")\n",
    "    print(\"   â†’ In-batchë„ randomê³¼ ë¹„ìŠ·í•œ ë‚œì´ë„\")\n",
    "else:\n",
    "    print(\"âŒ ì„±ëŠ¥ í•˜ë½\")\n",
    "    print(\"   â†’ Randomì´ ì´ ë°ì´í„°ì…‹ì— ë” ì í•©\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"V11A vs V9C ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# V9c ê²°ê³¼ (baseline)\n",
    "v9c_results = {\n",
    "    10: {'precision@10': 0.2756, 'recall@10': 0.1578, 'ndcg@10': 0.3129}\n",
    "}\n",
    "\n",
    "print(\"\\ní•µì‹¬ ë³€ê²½ì‚¬í•­:\")\n",
    "print(f\"  V9c: Random negative sampling\")\n",
    "print(f\"  V11a: Hard negative mining (Semi-hard 50% + In-batch 50%)\")\n",
    "\n",
    "print(\"\\nTest Set ì„±ëŠ¥ (Recall@10):\")\n",
    "print(f\"  V9c:  {v9c_results[10]['recall@10']:.4f} (15.78%)\")\n",
    "print(f\"  V11a: {v11a_results[10]['recall@10']:.4f} ({v11a_results[10]['recall@10']*100:.2f}%)\")\n",
    "\n",
    "improvement = (v11a_results[10]['recall@10'] - v9c_results[10]['recall@10']) / v9c_results[10]['recall@10'] * 100\n",
    "print(f\"  ë³€í™”: {improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\nì „ì²´ ì§€í‘œ ë¹„êµ (Top-10):\")\n",
    "for metric in ['precision@10', 'recall@10', 'ndcg@10']:\n",
    "    v9c_val = v9c_results[10][metric]\n",
    "    v11a_val = v11a_results[10][metric]\n",
    "    change = (v11a_val - v9c_val) / v9c_val * 100\n",
    "    symbol = 'âœ…' if change > 0 else 'âŒ'\n",
    "    print(f\"  {metric:15s}: V9c={v9c_val:.4f}, V11a={v11a_val:.4f} ({change:+.1f}%) {symbol}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ì‹¤í—˜ ê²°ê³¼ í•´ì„:\")\n",
    "if improvement > 5:\n",
    "    print(\"ğŸ‰ BREAKTHROUGH ì„±ê³µ!\")\n",
    "    print(\"   Hard negative miningì´ í•µì‹¬ bottleneckì´ì—ˆìŒ\")\n",
    "    print(\"   â†’ Random samplingì´ ë„ˆë¬´ ì‰¬ìš´ negativesë§Œ ì œê³µ\")\n",
    "    print(\"   â†’ Semi-hard + In-batchë¡œ í•™ìŠµ íš¨ìœ¨ ëŒ€í­ í–¥ìƒ\")\n",
    "    print(\"\\në‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(\"   - V10a (embedding capacity)ì™€ ë¹„êµ\")\n",
    "    print(\"   - ìµœì  ëª¨ë¸ ì„ íƒ í›„ ìµœì¢… í‰ê°€\")\n",
    "elif improvement > 2:\n",
    "    print(\"âœ… ì†Œí­ ê°œì„ . Hard negative mining íš¨ê³¼ ìˆìŒ\")\n",
    "    print(\"   â†’ V10aì™€ ë¹„êµ í›„ ìµœì¢… ê²°ì •\")\n",
    "elif improvement > -2:\n",
    "    print(\"â– ë¹„ìŠ·í•œ ìˆ˜ì¤€. Hard negative íš¨ê³¼ ë¯¸ë¯¸\")\n",
    "    print(\"   â†’ V10a (embedding capacity) ì‹œë„\")\n",
    "else:\n",
    "    print(\"âŒ ì„±ëŠ¥ í•˜ë½. ì´ ë°ì´í„°ì…‹ì—ì„œëŠ” randomì´ ë” ë‚˜ìŒ\")\n",
    "    print(\"   â†’ V10aë¡œ ë‹¤ë¥¸ ë°©í–¥ íƒìƒ‰\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set í‰ê°€\n",
    "model.load_state_dict(torch.load(os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_best.pth')))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Test Set í‰ê°€ (V11A - Hard Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "v11a_results = {}\n",
    "\n",
    "for k in [5, 10, 20]:\n",
    "    test_metrics = evaluate_model(\n",
    "        model, \n",
    "        train_valid_edge_index,\n",
    "        test_df, \n",
    "        train_valid_user_items,\n",
    "        n_items, \n",
    "        k=k, \n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    v11a_results[k] = test_metrics\n",
    "    \n",
    "    print(f\"\\nTop-{k} ì¶”ì²œ:\")\n",
    "    print(f\"  Precision@{k}: {test_metrics[f'precision@{k}']:.4f}\")\n",
    "    print(f\"  Recall@{k}:    {test_metrics[f'recall@{k}']:.4f}\")\n",
    "    print(f\"  NDCG@{k}:      {test_metrics[f'ndcg@{k}']:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8a vs V6 ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"V11A vs V9C ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# V9c ê²°ê³¼ (baseline)\n",
    "v9c_results = {\n",
    "    10: {'precision@10': 0.2756, 'recall@10': 0.1578, 'ndcg@10': 0.3129}\n",
    "}\n",
    "\n",
    "print(\"\\ní•µì‹¬ ë³€ê²½ì‚¬í•­:\")\n",
    "print(f\"  V9c: Random negative sampling\")\n",
    "print(f\"  V11a: Hard negative mining (Semi-hard 50% + In-batch 50%)\")\n",
    "\n",
    "print(\"\\nTest Set ì„±ëŠ¥ (Recall@10):\")\n",
    "print(f\"  V9c:  {v9c_results[10]['recall@10']:.4f} (15.78%)\")\n",
    "print(f\"  V11a: {v11a_results[10]['recall@10']:.4f} ({v11a_results[10]['recall@10']*100:.2f}%)\")\n",
    "\n",
    "improvement = (v11a_results[10]['recall@10'] - v9c_results[10]['recall@10']) / v9c_results[10]['recall@10'] * 100\n",
    "print(f\"  ë³€í™”: {improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\nì „ì²´ ì§€í‘œ ë¹„êµ (Top-10):\")\n",
    "for metric in ['precision@10', 'recall@10', 'ndcg@10']:\n",
    "    v9c_val = v9c_results[10][metric]\n",
    "    v11a_val = v11a_results[10][metric]\n",
    "    change = (v11a_val - v9c_val) / v9c_val * 100\n",
    "    symbol = 'âœ…' if change > 0 else 'âŒ'\n",
    "    print(f\"  {metric:15s}: V9c={v9c_val:.4f}, V11a={v11a_val:.4f} ({change:+.1f}%) {symbol}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ì‹¤í—˜ ê²°ê³¼ í•´ì„:\")\n",
    "if improvement > 5:\n",
    "    print(\"ğŸ‰ BREAKTHROUGH ì„±ê³µ!\")\n",
    "    print(\"   Hard negative miningì´ í•µì‹¬ bottleneckì´ì—ˆìŒ\")\n",
    "    print(\"   â†’ Random samplingì´ ë„ˆë¬´ ì‰¬ìš´ negativesë§Œ ì œê³µ\")\n",
    "    print(\"   â†’ Semi-hard + In-batchë¡œ í•™ìŠµ íš¨ìœ¨ ëŒ€í­ í–¥ìƒ\")\n",
    "    print(\"\\në‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(\"   - V10a (embedding capacity)ì™€ ë¹„êµ\")\n",
    "    print(\"   - ìµœì  ëª¨ë¸ ì„ íƒ í›„ ìµœì¢… í‰ê°€\")\n",
    "elif improvement > 2:\n",
    "    print(\"âœ… ì†Œí­ ê°œì„ . Hard negative mining íš¨ê³¼ ìˆìŒ\")\n",
    "    print(\"   â†’ V10aì™€ ë¹„êµ í›„ ìµœì¢… ê²°ì •\")\n",
    "elif improvement > -2:\n",
    "    print(\"â– ë¹„ìŠ·í•œ ìˆ˜ì¤€. Hard negative íš¨ê³¼ ë¯¸ë¯¸\")\n",
    "    print(\"   â†’ V10a (embedding capacity) ì‹œë„\")\n",
    "else:\n",
    "    print(\"âŒ ì„±ëŠ¥ í•˜ë½. ì´ ë°ì´í„°ì…‹ì—ì„œëŠ” randomì´ ë” ë‚˜ìŒ\")\n",
    "    print(\"   â†’ V10aë¡œ ë‹¤ë¥¸ ë°©í–¥ íƒìƒ‰\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}