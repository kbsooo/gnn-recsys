{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCN V7 - Multi-Enhancement Version\n",
    "\n",
    "## 개선 사항 (V6 대비)\n",
    "1. **80/10/10 Split**: 더 많은 학습 데이터 (70% → 80%)\n",
    "2. **Hard Negative Mining**: In-batch + Random negatives\n",
    "3. **Larger Model**: emb_dim 64, layers 2\n",
    "4. **Layer Normalization**: 학습 안정성\n",
    "5. **More Negatives**: NUM_NEG 8\n",
    "\n",
    "V6에서 유지:\n",
    "- Confidence-Weighted BPR Loss\n",
    "- Nuclear Norm Regularization\n",
    "- Incoherence Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Device: {device} ({torch.cuda.get_device_name()})')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f'Device: {device}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로딩 및 80/10/10 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ratings: 105,139\n",
      "Positive ratings (>= 4.0): 51,830\n",
      "Users: 668, Items: 10321\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "user2idx = {u: i for i, u in enumerate(sorted(df['user'].unique()))}\n",
    "item2idx = {it: i for i, it in enumerate(sorted(df['item'].unique()))}\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "n_users, n_items = len(user2idx), len(item2idx)\n",
    "\n",
    "df['user_idx'] = df['user'].map(user2idx)\n",
    "df['item_idx'] = df['item'].map(item2idx)\n",
    "df['label'] = (df['rating'] >= 4.0).astype(int)\n",
    "\n",
    "positive_df = df[df['label'] == 1].copy()\n",
    "\n",
    "print(f\"Total ratings: {len(df):,}\")\n",
    "print(f\"Positive ratings (>= 4.0): {len(positive_df):,}\")\n",
    "print(f\"Users: {n_users}, Items: {n_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/10/10 Split:\n",
      "  Train: 41,214 (79.5%)\n",
      "  Valid: 5,140 (9.9%)\n",
      "  Test:  5,476 (10.6%)\n"
     ]
    }
   ],
   "source": [
    "# 80/10/10 Split (더 많은 학습 데이터)\n",
    "train_data, val_data, test_data = [], [], []\n",
    "\n",
    "for user_idx in range(n_users):\n",
    "    user_pos = positive_df[positive_df['user_idx'] == user_idx]\n",
    "    n_pos = len(user_pos)\n",
    "    \n",
    "    if n_pos >= 3:\n",
    "        user_pos = user_pos.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        \n",
    "        # 80 / 10 / 10 split\n",
    "        train_end = int(0.8 * n_pos)\n",
    "        val_end = int(0.9 * n_pos)\n",
    "        \n",
    "        train_end = max(1, train_end)\n",
    "        val_end = max(train_end + 1, val_end)\n",
    "        \n",
    "        train_data.append(user_pos.iloc[:train_end])\n",
    "        val_data.append(user_pos.iloc[train_end:val_end])\n",
    "        test_data.append(user_pos.iloc[val_end:])\n",
    "        \n",
    "    elif n_pos == 2:\n",
    "        user_pos = user_pos.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        train_data.append(user_pos.iloc[:1])\n",
    "        val_data.append(user_pos.iloc[1:])\n",
    "        \n",
    "    elif n_pos == 1:\n",
    "        train_data.append(user_pos)\n",
    "\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "val_df = pd.concat(val_data, ignore_index=True)\n",
    "test_df = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "print(f\"80/10/10 Split:\")\n",
    "print(f\"  Train: {len(train_df):,} ({100*len(train_df)/len(positive_df):.1f}%)\")\n",
    "print(f\"  Valid: {len(val_df):,} ({100*len(val_df)/len(positive_df):.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df):,} ({100*len(test_df)/len(positive_df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computed negative candidates\n"
     ]
    }
   ],
   "source": [
    "train_users = torch.LongTensor(train_df['user_idx'].values)\n",
    "train_items = torch.LongTensor(train_df['item_idx'].values)\n",
    "val_users = torch.LongTensor(val_df['user_idx'].values)\n",
    "val_items = torch.LongTensor(val_df['item_idx'].values)\n",
    "test_users = torch.LongTensor(test_df['user_idx'].values)\n",
    "test_items = torch.LongTensor(test_df['item_idx'].values)\n",
    "\n",
    "user_pos_items_train = defaultdict(set)\n",
    "for u, i in zip(train_df['user_idx'].values, train_df['item_idx'].values):\n",
    "    user_pos_items_train[int(u)].add(int(i))\n",
    "\n",
    "user_neg_candidates = {}\n",
    "for u in range(n_users):\n",
    "    pos = user_pos_items_train[u]\n",
    "    user_neg_candidates[u] = np.array(list(set(range(n_items)) - pos))\n",
    "\n",
    "print(f\"Pre-computed negative candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 82,428 edges\n"
     ]
    }
   ],
   "source": [
    "def build_graph(train_df):\n",
    "    users = train_df['user_idx'].values\n",
    "    items = train_df['item_idx'].values\n",
    "    \n",
    "    edge_u2i = np.array([users, items + n_users])\n",
    "    edge_i2u = np.array([items + n_users, users])\n",
    "    edge_index = torch.LongTensor(np.concatenate([edge_u2i, edge_i2u], axis=1))\n",
    "    \n",
    "    num_nodes = n_users + n_items\n",
    "    deg = torch.zeros(num_nodes).scatter_add(0, edge_index[0], torch.ones(edge_index.shape[1]))\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    edge_weight = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]\n",
    "    \n",
    "    return edge_index.to(device), edge_weight.to(device)\n",
    "\n",
    "edge_index, edge_weight = build_graph(train_df)\n",
    "print(f\"Graph: {edge_index.shape[1]:,} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced LightGCN with Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedLightGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    LightGCN with:\n",
    "    - Layer Normalization for stability\n",
    "    - Nuclear Norm Regularization\n",
    "    - Incoherence Penalty\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_items, emb_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        \n",
    "        # Layer Normalization (optional, per layer)\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(emb_dim) for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, edge_index, edge_weight):\n",
    "        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for layer_idx in range(self.n_layers):\n",
    "            row, col = edge_index\n",
    "            messages = all_emb[col] * edge_weight.unsqueeze(1)\n",
    "            all_emb = torch.zeros_like(all_emb).scatter_add(\n",
    "                0, row.unsqueeze(1).expand(-1, self.emb_dim), messages\n",
    "            )\n",
    "            # Layer Normalization\n",
    "            all_emb = self.layer_norms[layer_idx](all_emb)\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.mean(torch.stack(embs), dim=0)\n",
    "        return final_emb[:self.n_users], final_emb[self.n_users:]\n",
    "    \n",
    "    def nuclear_norm_reg(self):\n",
    "        u_norm = torch.norm(self.user_emb.weight, p='fro') ** 2\n",
    "        v_norm = torch.norm(self.item_emb.weight, p='fro') ** 2\n",
    "        return (u_norm + v_norm) / 2\n",
    "    \n",
    "    def incoherence_penalty(self):\n",
    "        n_sample = min(100, self.n_users)\n",
    "        idx = torch.randperm(self.n_users, device=self.user_emb.weight.device)[:n_sample]\n",
    "        U_sample = self.user_emb.weight[idx]\n",
    "        U_norm = F.normalize(U_sample, p=2, dim=1)\n",
    "        gram = torch.mm(U_norm, U_norm.t())\n",
    "        mask = 1 - torch.eye(n_sample, device=gram.device)\n",
    "        off_diag = gram * mask\n",
    "        penalty = torch.sum(off_diag ** 2) / (n_sample * (n_sample - 1))\n",
    "        return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "  emb_dim: 32\n",
      "  n_layers: 1\n",
      "  Parameters: 351,712\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Model Configuration\n",
    "EMB_DIM = 32      # 32 → 64 (더 풍부한 표현)\n",
    "N_LAYERS = 1      # 1 → 2 (더 깊은 propagation)\n",
    "\n",
    "model = EnhancedLightGCN(n_users, n_items, EMB_DIM, N_LAYERS).to(device)\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  emb_dim: {EMB_DIM}\")\n",
    "print(f\"  n_layers: {N_LAYERS}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hard Negative Mining + Confidence Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence weights computed\n"
     ]
    }
   ],
   "source": [
    "# Confidence 계산 (V6과 동일)\n",
    "user_activity = train_df.groupby('user_idx').size().values\n",
    "item_popularity = train_df.groupby('item_idx').size().reindex(range(n_items), fill_value=0).values\n",
    "\n",
    "user_confidence = np.log1p(user_activity)\n",
    "item_confidence = np.log1p(item_popularity)\n",
    "\n",
    "user_confidence = 0.5 + (user_confidence - user_confidence.min()) / (user_confidence.max() - user_confidence.min())\n",
    "item_confidence = 0.5 + (item_confidence - item_confidence.min()) / (item_confidence.max() - item_confidence.min() + 1e-8)\n",
    "\n",
    "user_conf_tensor = torch.FloatTensor(user_confidence).to(device)\n",
    "item_conf_tensor = torch.FloatTensor(item_confidence).to(device)\n",
    "\n",
    "print(f\"Confidence weights computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Negative Mining ready\n"
     ]
    }
   ],
   "source": [
    "def hard_negative_sampling(pos_users, pos_items, num_neg=8):\n",
    "    \"\"\"\n",
    "    Hard Negative Mining:\n",
    "    - 50% in-batch negatives (다른 user의 positive items)\n",
    "    - 50% random negatives\n",
    "    \"\"\"\n",
    "    batch_size = len(pos_users)\n",
    "    neg_items = np.zeros((batch_size, num_neg), dtype=np.int64)\n",
    "    \n",
    "    n_inbatch = num_neg // 2\n",
    "    n_random = num_neg - n_inbatch\n",
    "    \n",
    "    pos_items_np = pos_items.cpu().numpy() if torch.is_tensor(pos_items) else pos_items\n",
    "    pos_users_np = pos_users.cpu().numpy() if torch.is_tensor(pos_users) else pos_users\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        user_idx = int(pos_users_np[i])\n",
    "        user_pos_set = user_pos_items_train[user_idx]\n",
    "        \n",
    "        # In-batch negatives\n",
    "        in_batch_candidates = []\n",
    "        for j in range(batch_size):\n",
    "            if j != i:\n",
    "                item_j = int(pos_items_np[j])\n",
    "                if item_j not in user_pos_set:\n",
    "                    in_batch_candidates.append(item_j)\n",
    "        \n",
    "        if len(in_batch_candidates) >= n_inbatch:\n",
    "            selected_inbatch = np.random.choice(in_batch_candidates, size=n_inbatch, replace=False)\n",
    "        else:\n",
    "            selected_inbatch = np.array(in_batch_candidates) if in_batch_candidates else np.array([], dtype=np.int64)\n",
    "            n_random += (n_inbatch - len(selected_inbatch))\n",
    "        \n",
    "        # Random negatives\n",
    "        cands = user_neg_candidates[user_idx]\n",
    "        if len(selected_inbatch) > 0:\n",
    "            cands = np.setdiff1d(cands, selected_inbatch)\n",
    "        selected_random = np.random.choice(cands, size=min(n_random, len(cands)), replace=False)\n",
    "        \n",
    "        all_neg = np.concatenate([selected_inbatch, selected_random])\n",
    "        \n",
    "        while len(all_neg) < num_neg:\n",
    "            extra = np.random.choice(user_neg_candidates[user_idx], size=1)\n",
    "            all_neg = np.concatenate([all_neg, extra])\n",
    "        \n",
    "        neg_items[i] = all_neg[:num_neg]\n",
    "    \n",
    "    return torch.LongTensor(neg_items)\n",
    "\n",
    "print(\"Hard Negative Mining ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_weighted_bpr_loss(pos_scores, neg_scores, user_ids, pos_item_ids):\n",
    "    confidence = user_conf_tensor[user_ids] * item_conf_tensor[pos_item_ids]\n",
    "    diff = pos_scores.unsqueeze(1) - neg_scores\n",
    "    bpr = -torch.log(torch.sigmoid(diff) + 1e-8).mean(dim=1)\n",
    "    weighted_loss = (bpr * confidence).sum() / confidence.sum()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ready\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, eval_users, eval_items, k=10, n_neg=99, sample_size=None):\n",
    "    model.eval()\n",
    "    u_emb, i_emb = model(edge_index, edge_weight)\n",
    "    \n",
    "    if sample_size and len(eval_users) > sample_size:\n",
    "        sample_idx = np.random.choice(len(eval_users), sample_size, replace=False)\n",
    "    else:\n",
    "        sample_idx = np.arange(len(eval_users))\n",
    "    \n",
    "    hits, ndcgs = [], []\n",
    "    \n",
    "    for idx in sample_idx:\n",
    "        user_idx = int(eval_users[idx])\n",
    "        pos_item = int(eval_items[idx])\n",
    "        \n",
    "        cands = user_neg_candidates[user_idx]\n",
    "        if len(cands) < n_neg:\n",
    "            continue\n",
    "        \n",
    "        neg_items = np.random.choice(cands, size=n_neg, replace=False)\n",
    "        candidates = np.concatenate([[pos_item], neg_items])\n",
    "        \n",
    "        u_t = torch.full((len(candidates),), user_idx, dtype=torch.long, device=device)\n",
    "        i_t = torch.LongTensor(candidates).to(device)\n",
    "        scores = (u_emb[u_t] * i_emb[i_t]).sum(dim=1).cpu().numpy()\n",
    "        \n",
    "        rank = (scores > scores[0]).sum() + 1\n",
    "        hits.append(1.0 if rank <= k else 0.0)\n",
    "        ndcgs.append(1.0 / np.log2(rank + 1) if rank <= k else 0.0)\n",
    "    \n",
    "    return np.mean(hits), np.mean(ndcgs)\n",
    "\n",
    "print(\"Evaluation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EnhancedLightGCN V7\n",
      "Config: emb=32, layers=1, BS=1024, LR=0.001, NEG=8\n",
      "Regularization: Nuclear=1e-06, Incoh=0.001\n",
      "Features: Hard Negative Mining + Confidence Weighting + Layer Norm\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "LR = 1e-3           # 5e-3 → 1e-3 (더 안정적)\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "NUM_NEG = 8         # 4 → 8 (더 많은 negative)\n",
    "\n",
    "# Regularization\n",
    "LAMBDA_NUCLEAR = 1e-6\n",
    "LAMBDA_INCOH = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "history = {\n",
    "    'loss': [], 'bpr_loss': [], 'nuclear_loss': [], 'incoh_loss': [],\n",
    "    'val_hit@10': [], 'val_ndcg@10': []\n",
    "}\n",
    "\n",
    "best_val_hit = 0\n",
    "best_val_ndcg = 0\n",
    "n_train = len(train_users)\n",
    "\n",
    "print(f\"Training EnhancedLightGCN V7\")\n",
    "print(f\"Config: emb={EMB_DIM}, layers={N_LAYERS}, BS={BATCH_SIZE}, LR={LR}, NEG={NUM_NEG}\")\n",
    "print(f\"Regularization: Nuclear={LAMBDA_NUCLEAR}, Incoh={LAMBDA_INCOH}\")\n",
    "print(f\"Features: Hard Negative Mining + Confidence Weighting + Layer Norm\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 | Loss: 0.1089 (BPR: 0.1088) | Val Hit@10: 0.7220 | Val NDCG@10: 0.4705\n",
      "Epoch  10 | Loss: 0.0701 (BPR: 0.0700) | Val Hit@10: 0.7127 | Val NDCG@10: 0.4593\n",
      "Epoch  15 | Loss: 0.0540 (BPR: 0.0539) | Val Hit@10: 0.6937 | Val NDCG@10: 0.4498\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m pos_i = train_items[batch_idx].to(device)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Hard Negative Mining\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m neg_i = \u001b[43mhard_negative_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_NEG\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[32m     20\u001b[39m u_emb, i_emb = model(edge_index, edge_weight)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mhard_negative_sampling\u001b[39m\u001b[34m(pos_users, pos_items, num_neg)\u001b[39m\n\u001b[32m     35\u001b[39m cands = user_neg_candidates[user_idx]\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(selected_inbatch) > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     cands = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetdiff1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_inbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m selected_random = np.random.choice(cands, size=\u001b[38;5;28mmin\u001b[39m(n_random, \u001b[38;5;28mlen\u001b[39m(cands)), replace=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     40\u001b[39m all_neg = np.concatenate([selected_inbatch, selected_random])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/학교/2025-2/그래프신경망과빅데이터/gnn-recsys/.venv/lib/python3.12/site-packages/numpy/lib/_arraysetops_impl.py:1258\u001b[39m, in \u001b[36msetdiff1d\u001b[39m\u001b[34m(ar1, ar2, assume_unique)\u001b[39m\n\u001b[32m   1256\u001b[39m     ar1 = np.asarray(ar1).ravel()\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1258\u001b[39m     ar1 = \u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1259\u001b[39m     ar2 = unique(ar2)\n\u001b[32m   1260\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ar1[_in1d(ar1, ar2, assume_unique=\u001b[38;5;28;01mTrue\u001b[39;00m, invert=\u001b[38;5;28;01mTrue\u001b[39;00m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/학교/2025-2/그래프신경망과빅데이터/gnn-recsys/.venv/lib/python3.12/site-packages/numpy/lib/_arraysetops_impl.py:294\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, axis, equal_nan, sorted)\u001b[39m\n\u001b[32m    292\u001b[39m ar = np.asanyarray(ar)\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     ret = \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/학교/2025-2/그래프신경망과빅데이터/gnn-recsys/.venv/lib/python3.12/site-packages/numpy/lib/_arraysetops_impl.py:371\u001b[39m, in \u001b[36m_unique1d\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, equal_nan, inverse_shape, axis, sorted)\u001b[39m\n\u001b[32m    368\u001b[39m conv = _array_converter(ar)\n\u001b[32m    369\u001b[39m ar_, = conv\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (hash_unique := \u001b[43m_unique_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar_\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msorted\u001b[39m:\n\u001b[32m    373\u001b[39m         hash_unique.sort()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    perm = torch.randperm(n_train)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_bpr = 0\n",
    "    epoch_nuclear = 0\n",
    "    epoch_incoh = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, n_train, BATCH_SIZE):\n",
    "        batch_idx = perm[i:i+BATCH_SIZE]\n",
    "        pos_u = train_users[batch_idx].to(device)\n",
    "        pos_i = train_items[batch_idx].to(device)\n",
    "        \n",
    "        # Hard Negative Mining\n",
    "        neg_i = hard_negative_sampling(pos_u, pos_i, NUM_NEG).to(device)\n",
    "        \n",
    "        # Forward\n",
    "        u_emb, i_emb = model(edge_index, edge_weight)\n",
    "        \n",
    "        # Scores\n",
    "        pos_scores = (u_emb[pos_u] * i_emb[pos_i]).sum(dim=1)\n",
    "        neg_u_expanded = pos_u.unsqueeze(1).expand(-1, NUM_NEG)\n",
    "        neg_scores = (u_emb[neg_u_expanded] * i_emb[neg_i]).sum(dim=2)\n",
    "        \n",
    "        # Losses\n",
    "        bpr_loss = confidence_weighted_bpr_loss(pos_scores, neg_scores, pos_u, pos_i)\n",
    "        nuclear_loss = LAMBDA_NUCLEAR * model.nuclear_norm_reg()\n",
    "        incoh_loss = LAMBDA_INCOH * model.incoherence_penalty()\n",
    "        \n",
    "        total_loss = bpr_loss + nuclear_loss + incoh_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_bpr += bpr_loss.item()\n",
    "        epoch_nuclear += nuclear_loss.item()\n",
    "        epoch_incoh += incoh_loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    history['loss'].append(epoch_loss / n_batches)\n",
    "    history['bpr_loss'].append(epoch_bpr / n_batches)\n",
    "    history['nuclear_loss'].append(epoch_nuclear / n_batches)\n",
    "    history['incoh_loss'].append(epoch_incoh / n_batches)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        val_hit, val_ndcg = evaluate(model, val_users, val_items, sample_size=3000)\n",
    "        history['val_hit@10'].append(val_hit)\n",
    "        history['val_ndcg@10'].append(val_ndcg)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {epoch_loss/n_batches:.4f} \"\n",
    "              f\"(BPR: {epoch_bpr/n_batches:.4f}) \"\n",
    "              f\"| Val Hit@10: {val_hit:.4f} | Val NDCG@10: {val_ndcg:.4f}\")\n",
    "        \n",
    "        if val_hit > best_val_hit:\n",
    "            best_val_hit = val_hit\n",
    "            best_val_ndcg = val_ndcg\n",
    "            torch.save(model.state_dict(), 'best_lightgcn_v7.pt')\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {elapsed/60:.1f} minutes\")\n",
    "print(f\"Best Validation: Hit@10={best_val_hit:.4f}, NDCG@10={best_val_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch   5 | Loss: 0.1089 (BPR: 0.1088) | Val Hit@10: 0.7220 | Val NDCG@10: 0.4705\n",
    "Epoch  10 | Loss: 0.0701 (BPR: 0.0700) | Val Hit@10: 0.7127 | Val NDCG@10: 0.4593\n",
    "Epoch  15 | Loss: 0.0540 (BPR: 0.0539) | Val Hit@10: 0.6937 | Val NDCG@10: 0.4498\n",
    "Epoch  20 | Loss: 0.0449 (BPR: 0.0448) | Val Hit@10: 0.7060 | Val NDCG@10: 0.4494\n",
    "Epoch  25 | Loss: 0.0398 (BPR: 0.0397) | Val Hit@10: 0.6963 | Val NDCG@10: 0.4448\n",
    "Epoch  30 | Loss: 0.0365 (BPR: 0.0364) | Val Hit@10: 0.6970 | Val NDCG@10: 0.4454\n",
    "Epoch  35 | Loss: 0.0339 (BPR: 0.0338) | Val Hit@10: 0.6903 | Val NDCG@10: 0.4397\n",
    "Epoch  40 | Loss: 0.0314 (BPR: 0.0313) | Val Hit@10: 0.6873 | Val NDCG@10: 0.4432\n",
    "Epoch  45 | Loss: 0.0299 (BPR: 0.0298) | Val Hit@10: 0.6880 | Val NDCG@10: 0.4357\n",
    "Epoch  50 | Loss: 0.0292 (BPR: 0.0290) | Val Hit@10: 0.6940 | Val NDCG@10: 0.4392\n",
    "\n",
    "Training completed in 46.2 minutes\n",
    "Best Validation: Hit@10=0.7220, NDCG@10=0.4705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['loss'], 'b-', linewidth=2)\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "epochs_val = np.arange(5, EPOCHS+1, 5)[:len(history['val_hit@10'])]\n",
    "axes[1].plot(epochs_val, history['val_hit@10'], 'go-', linewidth=2)\n",
    "axes[1].axhline(y=0.7821, color='red', linestyle='--', label='V6 Test')\n",
    "axes[1].set_title('Validation Hit@10')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(epochs_val, history['val_ndcg@10'], 'ro-', linewidth=2)\n",
    "axes[2].axhline(y=0.5217, color='blue', linestyle='--', label='V6 Test')\n",
    "axes[2].set_title('Validation NDCG@10')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('EnhancedLightGCN V7 Training', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Set 최종 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_lightgcn_v7.pt'))\n",
    "model.eval()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_hit, test_ndcg = evaluate(model, test_users, test_items, sample_size=None)\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Hit@10:  {test_hit:.4f}\")\n",
    "print(f\"  NDCG@10: {test_ndcg:.4f}\")\n",
    "\n",
    "print(f\"\\nValidation Set Results:\")\n",
    "print(f\"  Hit@10:  {best_val_hit:.4f}\")\n",
    "print(f\"  NDCG@10: {best_val_ndcg:.4f}\")\n",
    "\n",
    "print(f\"\\nV6 → V7 Comparison:\")\n",
    "print(f\"  V6 Test: Hit@10=0.7821, NDCG@10=0.5217\")\n",
    "print(f\"  V7 Test: Hit@10={test_hit:.4f}, NDCG@10={test_ndcg:.4f}\")\n",
    "print(f\"  Δ Hit@10:  {100*(test_hit - 0.7821)/0.7821:+.2f}%\")\n",
    "print(f\"  Δ NDCG@10: {100*(test_ndcg - 0.5217)/0.5217:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold & O/X 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    u_emb, i_emb = model(edge_index, edge_weight)\n",
    "\n",
    "val_pos_scores = (u_emb[val_users.to(device)] * i_emb[val_items.to(device)]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "val_neg_scores = []\n",
    "for user_idx in val_df['user_idx'].unique():\n",
    "    n_pos = (val_df['user_idx'] == user_idx).sum()\n",
    "    cands = user_neg_candidates[int(user_idx)]\n",
    "    neg_items = np.random.choice(cands, size=min(n_pos, len(cands)), replace=False)\n",
    "    \n",
    "    u_t = torch.full((len(neg_items),), user_idx, dtype=torch.long, device=device)\n",
    "    i_t = torch.LongTensor(neg_items).to(device)\n",
    "    scores = (u_emb[u_t] * i_emb[i_t]).sum(dim=1).cpu().numpy()\n",
    "    val_neg_scores.extend(scores)\n",
    "\n",
    "all_scores = np.concatenate([val_pos_scores, np.array(val_neg_scores)])\n",
    "all_labels = np.concatenate([np.ones(len(val_pos_scores)), np.zeros(len(val_neg_scores))])\n",
    "\n",
    "print(f\"Threshold tuning on {len(all_scores)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.percentile(all_scores, [30, 40, 50, 60, 70, 80, 90])\n",
    "\n",
    "print(\"Threshold Tuning:\")\n",
    "print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_prec, best_th, best_f1 = 0, 0, 0\n",
    "for th in thresholds:\n",
    "    preds = (all_scores >= th).astype(int)\n",
    "    tp = ((preds == 1) & (all_labels == 1)).sum()\n",
    "    fp = ((preds == 1) & (all_labels == 0)).sum()\n",
    "    fn = ((preds == 0) & (all_labels == 1)).sum()\n",
    "    \n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "    \n",
    "    print(f\"{th:<12.4f} {prec:<12.4f} {rec:<12.4f} {f1:<12.4f}\")\n",
    "    \n",
    "    if prec >= 0.6 and prec > best_prec:\n",
    "        best_prec, best_th, best_f1 = prec, th, f1\n",
    "    elif best_prec < 0.6 and f1 > best_f1:\n",
    "        best_f1, best_th, best_prec = f1, th, prec\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Selected: {best_th:.4f} (Precision: {best_prec:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ox(test_pairs_df):\n",
    "    results = []\n",
    "    \n",
    "    known_mask = test_pairs_df['user'].isin(user2idx) & test_pairs_df['item'].isin(item2idx)\n",
    "    known_df = test_pairs_df[known_mask]\n",
    "    unknown_df = test_pairs_df[~known_mask]\n",
    "    \n",
    "    if len(known_df) > 0:\n",
    "        u_idx = torch.LongTensor([user2idx[u] for u in known_df['user']]).to(device)\n",
    "        i_idx = torch.LongTensor([item2idx[i] for i in known_df['item']]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = (u_emb[u_idx] * i_emb[i_idx]).sum(dim=1).cpu().numpy()\n",
    "        \n",
    "        for (_, row), score in zip(known_df.iterrows(), scores):\n",
    "            results.append({\n",
    "                'user': row['user'], 'item': row['item'],\n",
    "                'recommend': 'O' if score >= best_th else 'X'\n",
    "            })\n",
    "    \n",
    "    for _, row in unknown_df.iterrows():\n",
    "        results.append({'user': row['user'], 'item': row['item'], 'recommend': 'X'})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "preds = predict_ox(val_df[['user', 'item']])\n",
    "o_ratio = (preds['recommend'] == 'O').mean()\n",
    "print(f\"\\nO ratio: {100*o_ratio:.1f}%\")\n",
    "print(preds.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 최종 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EnhancedLightGCN V7 Final Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nEnhancements over V6:\")\n",
    "print(f\"  1. Data Split: 70/15/15 → 80/10/10 (+10% training data)\")\n",
    "print(f\"  2. Hard Negative Mining: In-batch + Random\")\n",
    "print(f\"  3. Model Size: emb={EMB_DIM}, layers={N_LAYERS}\")\n",
    "print(f\"  4. Layer Normalization: Stability\")\n",
    "print(f\"  5. More Negatives: NUM_NEG={NUM_NEG}\")\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  emb_dim: {EMB_DIM}\")\n",
    "print(f\"  n_layers: {N_LAYERS}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nData Split:\")\n",
    "print(f\"  Train: {len(train_df):,} ({100*len(train_df)/len(positive_df):.1f}%)\")\n",
    "print(f\"  Valid: {len(val_df):,} ({100*len(val_df)/len(positive_df):.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df):,} ({100*len(test_df)/len(positive_df):.1f}%)\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Validation: Hit@10={best_val_hit:.4f}, NDCG@10={best_val_ndcg:.4f}\")\n",
    "print(f\"  Test:       Hit@10={test_hit:.4f}, NDCG@10={test_ndcg:.4f}\")\n",
    "print(f\"  Precision:  {best_prec:.4f}\")\n",
    "print(f\"  O ratio:    {100*o_ratio:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test inference\n",
    "# final_test_df = pd.read_csv('../data/test.csv')\n",
    "# final_preds = predict_ox(final_test_df)\n",
    "# final_preds.to_csv('predictions_v7.csv', index=False)\n",
    "\n",
    "print(\"Ready for final test inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
