{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCC2 - Combined Collaborative Approach (Ensemble)\n",
    "\n",
    "## Strategy\n",
    "- **Ensemble**: Combine CCA2 and CCB2 scores with weighted sum\n",
    "  - CCA2 score: Connection probability\n",
    "  - CCB2 score: Rating prediction (normalized to [0, 1])\n",
    "  - final_score = α * CCA_score_norm + β * CCB_score_norm\n",
    "- **Recommend**: if final_score > threshold\n",
    "\n",
    "## Key Features:\n",
    "- ✅ Combines both perspectives simultaneously\n",
    "- ✅ More flexible than Two-Stage (no hard filtering)\n",
    "- ✅ Adjustable weights (α, β) for different strategies\n",
    "- ✅ Single threshold for final decision\n",
    "\n",
    "## Hyperparameters:\n",
    "- `ALPHA`: Weight for CCA score (connection)\n",
    "- `BETA`: Weight for CCB score (rating quality)\n",
    "- `ENSEMBLE_THRESHOLD`: Final score threshold for recommendation\n",
    "- `GOOD_RATING_THRESHOLD`: 4.0 (ground truth definition)\n",
    "\n",
    "## Weight Experiments:\n",
    "- (α=0.3, β=0.7): CCB 중시 → Rating quality focus\n",
    "- (α=0.5, β=0.5): 균형 → Balanced approach\n",
    "- (α=0.7, β=0.3): CCA 중시 → Connection focus\n",
    "\n",
    "## Expected Performance:\n",
    "- Higher Recall than CCC1 (less conservative)\n",
    "- AUC-ROC target: 0.93+\n",
    "- F1 target: 0.80+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device setup (CUDA > MPS > CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Device: {device} ({torch.cuda.get_device_name()})')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f'Device: {device}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "### CCC2 Strategy:\n",
    "- Same as CCC1 (CCB-style split)\n",
    "- Positive: Rating >= 4\n",
    "- Train on all interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions: 105,139\n",
      "Unique users: 668\n",
      "Unique items: 10321\n",
      "\n",
      "Good purchases (rating >= 4.0): 51,830 (49.3%)\n",
      "\n",
      "Users: 668, Items: 10321\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "print(f\"Total interactions: {len(df):,}\")\n",
    "print(f\"Unique users: {df['user'].nunique()}\")\n",
    "print(f\"Unique items: {df['item'].nunique()}\")\n",
    "\n",
    "# Define good purchases\n",
    "GOOD_RATING_THRESHOLD = 4.0\n",
    "n_good_purchases = (df['rating'] >= GOOD_RATING_THRESHOLD).sum()\n",
    "print(f\"\\nGood purchases (rating >= {GOOD_RATING_THRESHOLD}): {n_good_purchases:,} ({100*n_good_purchases/len(df):.1f}%)\")\n",
    "\n",
    "# ID mapping\n",
    "user2idx = {u: i for i, u in enumerate(sorted(df['user'].unique()))}\n",
    "item2idx = {it: i for i, it in enumerate(sorted(df['item'].unique()))}\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "n_users, n_items = len(user2idx), len(item2idx)\n",
    "\n",
    "df['user_idx'] = df['user'].map(user2idx)\n",
    "df['item_idx'] = df['item'].map(item2idx)\n",
    "\n",
    "print(f\"\\nUsers: {n_users}, Items: {n_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User별 K값 계산\n",
    "user_interaction_count = df.groupby('user_idx').size().to_dict()\n",
    "\n",
    "MAX_K = 100\n",
    "\n",
    "def get_k_for_user(count):\n",
    "    if count <= 10:\n",
    "        return 2\n",
    "    k = max(2, int(count * 0.2))\n",
    "    return min(k, MAX_K)\n",
    "\n",
    "user_k = {u: get_k_for_user(c) for u, c in user_interaction_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 89,294\n",
      "Val: 7,480\n",
      "Test: 8,365\n"
     ]
    }
   ],
   "source": [
    "# Train/Val/Test Split (same as CCC1)\n",
    "train_data, val_data, test_data = [], [], []\n",
    "\n",
    "for user_idx in range(n_users):\n",
    "    user_df = df[df['user_idx'] == user_idx]\n",
    "    \n",
    "    good_purchases = user_df[user_df['rating'] >= GOOD_RATING_THRESHOLD][['user_idx', 'item_idx', 'rating']]\n",
    "    bad_purchases = user_df[user_df['rating'] < GOOD_RATING_THRESHOLD][['user_idx', 'item_idx', 'rating']]\n",
    "    \n",
    "    if len(bad_purchases) > 0:\n",
    "        train_data.append(bad_purchases[['user_idx', 'item_idx']])\n",
    "    \n",
    "    n_good = len(good_purchases)\n",
    "    \n",
    "    if n_good >= 3:\n",
    "        good_purchases = good_purchases.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        train_end = int(0.7 * n_good)\n",
    "        val_end = train_end + int(0.15 * n_good)\n",
    "        \n",
    "        train_end = max(1, train_end)\n",
    "        val_end = max(train_end + 1, val_end)\n",
    "        \n",
    "        train_data.append(good_purchases.iloc[:train_end][['user_idx', 'item_idx']])\n",
    "        val_data.append(good_purchases.iloc[train_end:val_end][['user_idx', 'item_idx']])\n",
    "        test_data.append(good_purchases.iloc[val_end:][['user_idx', 'item_idx']])\n",
    "    elif n_good == 2:\n",
    "        good_purchases = good_purchases.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        train_data.append(good_purchases.iloc[:1][['user_idx', 'item_idx']])\n",
    "        val_data.append(good_purchases.iloc[1:][['user_idx', 'item_idx']])\n",
    "    elif n_good == 1:\n",
    "        train_data.append(good_purchases[['user_idx', 'item_idx']])\n",
    "\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "val_df = pd.concat(val_data, ignore_index=True) if val_data else pd.DataFrame(columns=['user_idx', 'item_idx'])\n",
    "test_df = pd.concat(test_data, ignore_index=True) if test_data else pd.DataFrame(columns=['user_idx', 'item_idx'])\n",
    "\n",
    "print(f\"Train: {len(train_df):,}\")\n",
    "print(f\"Val: {len(val_df):,}\")\n",
    "print(f\"Test: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User train items\n",
    "user_train_items = defaultdict(set)\n",
    "for u, i in zip(train_df['user_idx'].values, train_df['item_idx'].values):\n",
    "    user_train_items[int(u)].add(int(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes defined\n"
     ]
    }
   ],
   "source": [
    "# Model definitions\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "    \n",
    "    def forward(self, edge_index, edge_weight):\n",
    "        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for _ in range(self.n_layers):\n",
    "            row, col = edge_index\n",
    "            messages = all_emb[col] * edge_weight.unsqueeze(1)\n",
    "            all_emb = torch.zeros_like(all_emb).scatter_add(\n",
    "                0, row.unsqueeze(1).expand(-1, self.emb_dim), messages\n",
    "            )\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.mean(torch.stack(embs), dim=0)\n",
    "        return final_emb[:self.n_users], final_emb[self.n_users:]\n",
    "\n",
    "\n",
    "class LightGCN_with_Rating(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        \n",
    "        self.rating_mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, edge_index, edge_weight):\n",
    "        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for _ in range(self.n_layers):\n",
    "            row, col = edge_index\n",
    "            messages = all_emb[col] * edge_weight.unsqueeze(1)\n",
    "            all_emb = torch.zeros_like(all_emb).scatter_add(\n",
    "                0, row.unsqueeze(1).expand(-1, self.emb_dim), messages\n",
    "            )\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.mean(torch.stack(embs), dim=0)\n",
    "        return final_emb[:self.n_users], final_emb[self.n_users:]\n",
    "    \n",
    "    def predict_rating(self, user_idx, item_idx, edge_index, edge_weight):\n",
    "        u_emb, i_emb = self.forward(edge_index, edge_weight)\n",
    "        interaction = u_emb[user_idx] * i_emb[item_idx]\n",
    "        rating_logit = self.rating_mlp(interaction).squeeze(-1)\n",
    "        predicted_rating = torch.sigmoid(rating_logit) * 4.5 + 0.5\n",
    "        return predicted_rating\n",
    "\n",
    "\n",
    "print(\"Model classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs built\n"
     ]
    }
   ],
   "source": [
    "# Build graphs\n",
    "\n",
    "def build_unweighted_graph():\n",
    "    users = train_df['user_idx'].values\n",
    "    items = train_df['item_idx'].values\n",
    "    \n",
    "    edge_u2i = np.array([users, items + n_users])\n",
    "    edge_i2u = np.array([items + n_users, users])\n",
    "    edge_index = torch.LongTensor(np.concatenate([edge_u2i, edge_i2u], axis=1))\n",
    "    \n",
    "    num_nodes = n_users + n_items\n",
    "    deg = torch.zeros(num_nodes).scatter_add(0, edge_index[0], torch.ones(edge_index.shape[1]))\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    \n",
    "    edge_weight = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]\n",
    "    \n",
    "    return edge_index.to(device), edge_weight.to(device)\n",
    "\n",
    "\n",
    "def build_rating_weighted_graph():\n",
    "    users = train_df['user_idx'].values\n",
    "    items = train_df['item_idx'].values\n",
    "    \n",
    "    ratings = []\n",
    "    for u, i in zip(users, items):\n",
    "        rating = df[(df['user_idx'] == u) & (df['item_idx'] == i)]['rating'].values\n",
    "        ratings.append(rating[0] if len(rating) > 0 else 3)\n",
    "    ratings = np.array(ratings)\n",
    "    \n",
    "    rating_factors = 0.4 + 0.15 * ratings\n",
    "    \n",
    "    edge_u2i = np.array([users, items + n_users])\n",
    "    edge_i2u = np.array([items + n_users, users])\n",
    "    edge_index = torch.LongTensor(np.concatenate([edge_u2i, edge_i2u], axis=1))\n",
    "    \n",
    "    rating_factors_both = np.concatenate([rating_factors, rating_factors])\n",
    "    \n",
    "    num_nodes = n_users + n_items\n",
    "    deg = torch.zeros(num_nodes).scatter_add(0, edge_index[0], torch.ones(edge_index.shape[1]))\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    \n",
    "    base_weight = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]\n",
    "    rating_weight = torch.FloatTensor(rating_factors_both)\n",
    "    edge_weight = base_weight * rating_weight\n",
    "    \n",
    "    return edge_index.to(device), edge_weight.to(device)\n",
    "\n",
    "\n",
    "cca_edge_index, cca_edge_weight = build_unweighted_graph()\n",
    "ccb_edge_index, ccb_edge_weight = build_rating_weighted_graph()\n",
    "\n",
    "print(f\"Graphs built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "\n",
    "EMB_DIM = 32\n",
    "N_LAYERS = 2\n",
    "\n",
    "cca_model = LightGCN(n_users, n_items, EMB_DIM, N_LAYERS).to(device)\n",
    "cca_model.load_state_dict(torch.load('../cc_models/cca2_best.pt'))\n",
    "cca_model.eval()\n",
    "\n",
    "ccb_model = LightGCN_with_Rating(n_users, n_items, EMB_DIM, N_LAYERS).to(device)\n",
    "ccb_model.load_state_dict(torch.load('../cc_models/ccb2_best.pt'))\n",
    "ccb_model.eval()\n",
    "\n",
    "print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score Normalization\n",
    "\n",
    "To combine CCA and CCB scores, we need to normalize them to [0, 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating score ranges for normalization...\n",
      "\n",
      "CCA score range: [-0.6239, 3.0902]\n",
      "CCB rating range: [0.5000, 5.0000]\n",
      "\n",
      "Normalization functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Get all embeddings for score range calculation\n",
    "print(\"Calculating score ranges for normalization...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    cca_u_emb, cca_i_emb = cca_model(cca_edge_index, cca_edge_weight)\n",
    "\n",
    "# Sample 1000 user-item pairs to estimate score ranges\n",
    "sample_size = 1000\n",
    "sample_users = np.random.choice(n_users, sample_size)\n",
    "sample_items = np.random.choice(n_items, sample_size)\n",
    "\n",
    "cca_scores = []\n",
    "ccb_ratings = []\n",
    "\n",
    "for u, i in zip(sample_users, sample_items):\n",
    "    with torch.no_grad():\n",
    "        cca_score = (cca_u_emb[u] * cca_i_emb[i]).sum().item()\n",
    "        cca_scores.append(cca_score)\n",
    "        \n",
    "        u_t = torch.tensor([u], dtype=torch.long).to(device)\n",
    "        i_t = torch.tensor([i], dtype=torch.long).to(device)\n",
    "        ccb_rating = ccb_model.predict_rating(u_t, i_t, ccb_edge_index, ccb_edge_weight).item()\n",
    "        ccb_ratings.append(ccb_rating)\n",
    "\n",
    "CCA_MIN = np.min(cca_scores)\n",
    "CCA_MAX = np.max(cca_scores)\n",
    "CCB_MIN = 0.5  # Known range\n",
    "CCB_MAX = 5.0  # Known range\n",
    "\n",
    "print(f\"\\nCCA score range: [{CCA_MIN:.4f}, {CCA_MAX:.4f}]\")\n",
    "print(f\"CCB rating range: [{CCB_MIN:.4f}, {CCB_MAX:.4f}]\")\n",
    "\n",
    "\n",
    "def normalize_cca_score(score):\n",
    "    \"\"\"Normalize CCA score to [0, 1]\"\"\"\n",
    "    if CCA_MAX == CCA_MIN:\n",
    "        return 0.5\n",
    "    return (score - CCA_MIN) / (CCA_MAX - CCA_MIN)\n",
    "\n",
    "\n",
    "def normalize_ccb_rating(rating):\n",
    "    \"\"\"Normalize CCB rating to [0, 1]\"\"\"\n",
    "    return (rating - CCB_MIN) / (CCB_MAX - CCB_MIN)\n",
    "\n",
    "\n",
    "print(\"\\nNormalization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Prediction\n",
    "\n",
    "Combine CCA and CCB scores with weighted sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Hyperparameters:\n",
      "  α (CCA weight): 0.5\n",
      "  β (CCB weight): 0.5\n",
      "  Threshold: 0.5\n",
      "\n",
      "Strategy: final_score = 0.5 * CCA_norm + 0.5 * CCB_norm\n"
     ]
    }
   ],
   "source": [
    "# Ensemble hyperparameters\n",
    "ALPHA = 0.5  # Weight for CCA (connection)\n",
    "BETA = 0.5   # Weight for CCB (rating)\n",
    "ENSEMBLE_THRESHOLD = 0.5  # Threshold for final score\n",
    "\n",
    "print(f\"Ensemble Hyperparameters:\")\n",
    "print(f\"  α (CCA weight): {ALPHA}\")\n",
    "print(f\"  β (CCB weight): {BETA}\")\n",
    "print(f\"  Threshold: {ENSEMBLE_THRESHOLD}\")\n",
    "print(f\"\\nStrategy: final_score = {ALPHA} * CCA_norm + {BETA} * CCB_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble prediction function ready!\n"
     ]
    }
   ],
   "source": [
    "def predict_ensemble(test_input_df, alpha=0.5, beta=0.5, threshold=0.5, verbose=True, show_details=False):\n",
    "    \"\"\"\n",
    "    ★ CCC2: Ensemble Recommendation\n",
    "    \n",
    "    Combine CCA and CCB scores with weighted sum.\n",
    "    final_score = α * normalize(CCA_score) + β * normalize(CCB_rating)\n",
    "    \n",
    "    Args:\n",
    "        test_input_df: Test data (user, item columns)\n",
    "        alpha: Weight for CCA score\n",
    "        beta: Weight for CCB rating\n",
    "        threshold: Final score threshold for recommendation\n",
    "        verbose: Print AGENTS.md format\n",
    "        show_details: Show individual scores\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame with recommendations\n",
    "    \"\"\"\n",
    "    cca_model.eval()\n",
    "    ccb_model.eval()\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        cca_u_emb, cca_i_emb = cca_model(cca_edge_index, cca_edge_weight)\n",
    "    \n",
    "    results = []\n",
    "    stats = {'total_o': 0, 'total_items': 0}\n",
    "\n",
    "    for _, row in test_input_df.iterrows():\n",
    "        user = row['user']\n",
    "        item = row['item']\n",
    "        stats['total_items'] += 1\n",
    "        \n",
    "        # Unknown user/item → X\n",
    "        if user not in user2idx or item not in item2idx:\n",
    "            results.append({\n",
    "                'user': user,\n",
    "                'item': item,\n",
    "                'recommend': 'X',\n",
    "                'reason': 'unknown'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        user_idx = user2idx[user]\n",
    "        item_idx = item2idx[item]\n",
    "        \n",
    "        # Train item → X\n",
    "        if item_idx in user_train_items[user_idx]:\n",
    "            results.append({\n",
    "                'user': user,\n",
    "                'item': item,\n",
    "                'recommend': 'X',\n",
    "                'reason': 'in_train'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Calculate CCA score\n",
    "        with torch.no_grad():\n",
    "            cca_score = (cca_u_emb[user_idx] * cca_i_emb[item_idx]).sum().item()\n",
    "        \n",
    "        # Calculate CCB rating\n",
    "        with torch.no_grad():\n",
    "            u_t = torch.tensor([user_idx], dtype=torch.long).to(device)\n",
    "            i_t = torch.tensor([item_idx], dtype=torch.long).to(device)\n",
    "            ccb_rating = ccb_model.predict_rating(u_t, i_t, ccb_edge_index, ccb_edge_weight).item()\n",
    "        \n",
    "        # Normalize scores\n",
    "        cca_norm = normalize_cca_score(cca_score)\n",
    "        ccb_norm = normalize_ccb_rating(ccb_rating)\n",
    "        \n",
    "        # Ensemble score\n",
    "        final_score = alpha * cca_norm + beta * ccb_norm\n",
    "        \n",
    "        # Decision\n",
    "        if final_score >= threshold:\n",
    "            recommend = 'O'\n",
    "            stats['total_o'] += 1\n",
    "        else:\n",
    "            recommend = 'X'\n",
    "        \n",
    "        results.append({\n",
    "            'user': user,\n",
    "            'item': item,\n",
    "            'recommend': recommend,\n",
    "            'cca_score': cca_score,\n",
    "            'ccb_rating': ccb_rating,\n",
    "            'cca_norm': cca_norm,\n",
    "            'ccb_norm': ccb_norm,\n",
    "            'final_score': final_score\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print results\n",
    "    if verbose:\n",
    "        print(\"=\" * 80)\n",
    "        if show_details:\n",
    "            print(f\"{'user':<8} {'item':<8} {'CCA':<8} {'CCB':<8} {'CCA_n':<8} {'CCB_n':<8} {'Final':<8} {'Rec':<4}\")\n",
    "            for _, r in results_df.iterrows():\n",
    "                if 'cca_score' in r:\n",
    "                    print(f\"{r['user']:<8} {r['item']:<8} {r['cca_score']:<8.3f} {r['ccb_rating']:<8.2f} \"\n",
    "                          f\"{r['cca_norm']:<8.3f} {r['ccb_norm']:<8.3f} {r['final_score']:<8.3f} {r['recommend']:<4}\")\n",
    "                else:\n",
    "                    print(f\"{r['user']:<8} {r['item']:<8} {'N/A':<8} {'N/A':<8} {'N/A':<8} {'N/A':<8} {'N/A':<8} {r['recommend']:<4}\")\n",
    "        else:\n",
    "            print(f\"{'user':<10} {'item':<10} {'recommend':<10}\")\n",
    "            for _, r in results_df.iterrows():\n",
    "                print(f\"{r['user']:<10} {r['item']:<10} {r['recommend']:<10}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total recommends = {stats['total_o']}/{stats['total_items']}\")\n",
    "        print(f\"Not recommend = {stats['total_items'] - stats['total_o']}/{stats['total_items']}\")\n",
    "        print()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "print(\"Ensemble prediction function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Prediction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample1.csv Test (CCC2 - Ensemble):\n",
      "α=0.5, β=0.5, threshold=0.5\n",
      "\n",
      "================================================================================\n",
      "user     item     CCA      CCB      CCA_n    CCB_n    Final    Rec \n",
      "109      3745     1.801    3.24     0.653    0.609    0.631    O   \n",
      "88       4447     1.672    3.21     0.618    0.602    0.610    O   \n",
      "71       4306     1.856    4.73     0.668    0.939    0.804    O   \n",
      "66       1747     1.675    3.45     0.619    0.656    0.638    O   \n",
      "15       66934    0.612    4.99     0.333    0.999    0.666    O   \n",
      "================================================================================\n",
      "Total recommends = 5/5\n",
      "Not recommend = 0/5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with sample1.csv\n",
    "sample1 = pd.read_csv('../data/sample1.csv')\n",
    "\n",
    "print(\"Sample1.csv Test (CCC2 - Ensemble):\")\n",
    "print(f\"α={ALPHA}, β={BETA}, threshold={ENSEMBLE_THRESHOLD}\")\n",
    "print()\n",
    "predictions1 = predict_ensemble(sample1, ALPHA, BETA, ENSEMBLE_THRESHOLD, verbose=True, show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample2.csv Test (CCC2 - Ensemble):\n",
      "α=0.5, β=0.5, threshold=0.5\n",
      "\n",
      "================================================================================\n",
      "user     item     CCA      CCB      CCA_n    CCB_n    Final    Rec \n",
      "109.0    3745.0   1.801    3.24     0.653    0.609    0.631    O   \n",
      "88.0     4447.0   1.672    3.21     0.618    0.602    0.610    O   \n",
      "71.0     4306.0   1.856    4.73     0.668    0.939    0.804    O   \n",
      "66.0     1747.0   1.675    3.45     0.619    0.656    0.638    O   \n",
      "15.0     66934.0  0.612    4.99     0.333    0.999    0.666    O   \n",
      "================================================================================\n",
      "Total recommends = 5/5\n",
      "Not recommend = 0/5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with sample2.csv\n",
    "sample2 = pd.read_csv('../data/sample2.csv')\n",
    "\n",
    "print(\"Sample2.csv Test (CCC2 - Ensemble):\")\n",
    "print(f\"α={ALPHA}, β={BETA}, threshold={ENSEMBLE_THRESHOLD}\")\n",
    "print()\n",
    "predictions2 = predict_ensemble(sample2, ALPHA, BETA, ENSEMBLE_THRESHOLD, verbose=True, show_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set: 7480 samples\n",
      "\n",
      "Validation Performance (CCC2 - Ensemble):\n",
      "  Accuracy: 0.7876\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.7876\n",
      "  F1 Score: 0.8812\n",
      "  O ratio: 78.8%\n"
     ]
    }
   ],
   "source": [
    "# Convert val_df to test format\n",
    "val_test_df = val_df.copy()\n",
    "val_test_df['user'] = val_test_df['user_idx'].map(idx2user)\n",
    "val_test_df['item'] = val_test_df['item_idx'].map(idx2item)\n",
    "val_test_df = val_test_df[['user', 'item']]\n",
    "\n",
    "print(f\"Evaluating on validation set: {len(val_test_df)} samples\")\n",
    "\n",
    "val_predictions = predict_ensemble(val_test_df, ALPHA, BETA, ENSEMBLE_THRESHOLD, verbose=False)\n",
    "\n",
    "# All items in val are positive (rating >= 4)\n",
    "val_labels = np.ones(len(val_predictions))\n",
    "val_preds = (val_predictions['recommend'] == 'O').astype(int).values\n",
    "\n",
    "val_acc = (val_preds == val_labels).mean()\n",
    "val_prec = precision_score(val_labels, val_preds, zero_division=0)\n",
    "val_rec = recall_score(val_labels, val_preds, zero_division=0)\n",
    "val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "\n",
    "print(f\"\\nValidation Performance (CCC2 - Ensemble):\")\n",
    "print(f\"  Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  Precision: {val_prec:.4f}\")\n",
    "print(f\"  Recall: {val_rec:.4f}\")\n",
    "print(f\"  F1 Score: {val_f1:.4f}\")\n",
    "print(f\"  O ratio: {val_preds.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set: 8365 samples\n",
      "\n",
      "Test Performance (CCC2 - Ensemble):\n",
      "  Accuracy: 0.7811\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.7811\n",
      "  F1 Score: 0.8771\n",
      "  O ratio: 78.1%\n"
     ]
    }
   ],
   "source": [
    "# Convert test_df to test format\n",
    "test_test_df = test_df.copy()\n",
    "test_test_df['user'] = test_test_df['user_idx'].map(idx2user)\n",
    "test_test_df['item'] = test_test_df['item_idx'].map(idx2item)\n",
    "test_test_df = test_test_df[['user', 'item']]\n",
    "\n",
    "print(f\"Evaluating on test set: {len(test_test_df)} samples\")\n",
    "\n",
    "test_predictions = predict_ensemble(test_test_df, ALPHA, BETA, ENSEMBLE_THRESHOLD, verbose=False)\n",
    "\n",
    "test_labels = np.ones(len(test_predictions))\n",
    "test_preds = (test_predictions['recommend'] == 'O').astype(int).values\n",
    "\n",
    "test_acc = (test_preds == test_labels).mean()\n",
    "test_prec = precision_score(test_labels, test_preds, zero_division=0)\n",
    "test_rec = recall_score(test_labels, test_preds, zero_division=0)\n",
    "test_f1 = f1_score(test_labels, test_preds, zero_division=0)\n",
    "\n",
    "print(f\"\\nTest Performance (CCC2 - Ensemble):\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Precision: {test_prec:.4f}\")\n",
    "print(f\"  Recall: {test_rec:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "print(f\"  O ratio: {test_preds.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. AUC-ROC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating AUC-ROC with negative samples...\n",
      "\n",
      "Validation AUC-ROC: 0.9366\n",
      "  Positive scores: mean=0.6099, std=0.1618\n",
      "  Negative scores: mean=0.2630, std=0.1249\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating AUC-ROC with negative samples...\")\n",
    "\n",
    "# Positive and negative samples\n",
    "val_pos_users = val_df['user_idx'].values\n",
    "val_pos_items = val_df['item_idx'].values\n",
    "\n",
    "val_test_edges = set()\n",
    "for u, i in zip(val_df['user_idx'].values, val_df['item_idx'].values):\n",
    "    val_test_edges.add((int(u), int(i)))\n",
    "for u, i in zip(test_df['user_idx'].values, test_df['item_idx'].values):\n",
    "    val_test_edges.add((int(u), int(i)))\n",
    "\n",
    "n_neg = len(val_df)\n",
    "neg_users, neg_items = [], []\n",
    "attempts = 0\n",
    "max_attempts = n_neg * 100\n",
    "\n",
    "while len(neg_users) < n_neg and attempts < max_attempts:\n",
    "    u = np.random.randint(0, n_users)\n",
    "    i = np.random.randint(0, n_items)\n",
    "    attempts += 1\n",
    "    \n",
    "    if i not in user_train_items[u] and (u, i) not in val_test_edges:\n",
    "        neg_users.append(u)\n",
    "        neg_items.append(i)\n",
    "\n",
    "# Score positive samples\n",
    "pos_scores = []\n",
    "with torch.no_grad():\n",
    "    cca_u_emb, cca_i_emb = cca_model(cca_edge_index, cca_edge_weight)\n",
    "\n",
    "for u_idx, i_idx in zip(val_pos_users, val_pos_items):\n",
    "    with torch.no_grad():\n",
    "        cca_score = (cca_u_emb[u_idx] * cca_i_emb[i_idx]).sum().item()\n",
    "        u_t = torch.tensor([u_idx], dtype=torch.long).to(device)\n",
    "        i_t = torch.tensor([i_idx], dtype=torch.long).to(device)\n",
    "        ccb_rating = ccb_model.predict_rating(u_t, i_t, ccb_edge_index, ccb_edge_weight).item()\n",
    "    \n",
    "    cca_norm = normalize_cca_score(cca_score)\n",
    "    ccb_norm = normalize_ccb_rating(ccb_rating)\n",
    "    final_score = ALPHA * cca_norm + BETA * ccb_norm\n",
    "    pos_scores.append(final_score)\n",
    "\n",
    "# Score negative samples\n",
    "neg_scores = []\n",
    "for u_idx, i_idx in zip(neg_users, neg_items):\n",
    "    with torch.no_grad():\n",
    "        cca_score = (cca_u_emb[u_idx] * cca_i_emb[i_idx]).sum().item()\n",
    "        u_t = torch.tensor([u_idx], dtype=torch.long).to(device)\n",
    "        i_t = torch.tensor([i_idx], dtype=torch.long).to(device)\n",
    "        ccb_rating = ccb_model.predict_rating(u_t, i_t, ccb_edge_index, ccb_edge_weight).item()\n",
    "    \n",
    "    cca_norm = normalize_cca_score(cca_score)\n",
    "    ccb_norm = normalize_ccb_rating(ccb_rating)\n",
    "    final_score = ALPHA * cca_norm + BETA * ccb_norm\n",
    "    neg_scores.append(final_score)\n",
    "\n",
    "# AUC-ROC\n",
    "all_scores = np.concatenate([pos_scores, neg_scores])\n",
    "all_labels = np.concatenate([np.ones(len(pos_scores)), np.zeros(len(neg_scores))])\n",
    "\n",
    "val_auc = roc_auc_score(all_labels, all_scores)\n",
    "\n",
    "print(f\"\\nValidation AUC-ROC: {val_auc:.4f}\")\n",
    "print(f\"  Positive scores: mean={np.mean(pos_scores):.4f}, std={np.std(pos_scores):.4f}\")\n",
    "print(f\"  Negative scores: mean={np.mean(neg_scores):.4f}, std={np.std(neg_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Weight Experiments\n",
    "\n",
    "Try different α, β combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Experiment Results:\n",
      "================================================================================\n",
      "Config          α      β      AUC      Prec     Rec      F1      \n",
      "================================================================================\n",
      "CCB focus       0.3    0.7    0.9363   0.8763   0.8570   0.8665  \n",
      "Balanced        0.5    0.5    0.9536   0.9238   0.8360   0.8777  \n",
      "CCA focus       0.7    0.3    0.9633   0.9538   0.7840   0.8606  \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different weights\n",
    "weight_configs = [\n",
    "    (0.3, 0.7, \"CCB focus\"),\n",
    "    (0.5, 0.5, \"Balanced\"),\n",
    "    (0.7, 0.3, \"CCA focus\")\n",
    "]\n",
    "\n",
    "print(\"Weight Experiment Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Config':<15} {'α':<6} {'β':<6} {'AUC':<8} {'Prec':<8} {'Rec':<8} {'F1':<8}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for alpha, beta, name in weight_configs:\n",
    "    # Recalculate scores with new weights\n",
    "    pos_scores_exp = []\n",
    "    for u_idx, i_idx in zip(val_pos_users[:1000], val_pos_items[:1000]):  # Sample for speed\n",
    "        with torch.no_grad():\n",
    "            cca_score = (cca_u_emb[u_idx] * cca_i_emb[i_idx]).sum().item()\n",
    "            u_t = torch.tensor([u_idx], dtype=torch.long).to(device)\n",
    "            i_t = torch.tensor([i_idx], dtype=torch.long).to(device)\n",
    "            ccb_rating = ccb_model.predict_rating(u_t, i_t, ccb_edge_index, ccb_edge_weight).item()\n",
    "        \n",
    "        cca_norm = normalize_cca_score(cca_score)\n",
    "        ccb_norm = normalize_ccb_rating(ccb_rating)\n",
    "        final_score = alpha * cca_norm + beta * ccb_norm\n",
    "        pos_scores_exp.append(final_score)\n",
    "    \n",
    "    neg_scores_exp = []\n",
    "    for u_idx, i_idx in zip(neg_users[:1000], neg_items[:1000]):\n",
    "        with torch.no_grad():\n",
    "            cca_score = (cca_u_emb[u_idx] * cca_i_emb[i_idx]).sum().item()\n",
    "            u_t = torch.tensor([u_idx], dtype=torch.long).to(device)\n",
    "            i_t = torch.tensor([i_idx], dtype=torch.long).to(device)\n",
    "            ccb_rating = ccb_model.predict_rating(u_t, i_t, ccb_edge_index, ccb_edge_weight).item()\n",
    "        \n",
    "        cca_norm = normalize_cca_score(cca_score)\n",
    "        ccb_norm = normalize_ccb_rating(ccb_rating)\n",
    "        final_score = alpha * cca_norm + beta * ccb_norm\n",
    "        neg_scores_exp.append(final_score)\n",
    "    \n",
    "    all_scores_exp = np.concatenate([pos_scores_exp, neg_scores_exp])\n",
    "    all_labels_exp = np.concatenate([np.ones(len(pos_scores_exp)), np.zeros(len(neg_scores_exp))])\n",
    "    \n",
    "    auc_exp = roc_auc_score(all_labels_exp, all_scores_exp)\n",
    "    \n",
    "    # Use threshold = 0.5 for classification\n",
    "    preds_exp = (all_scores_exp > 0.5).astype(int)\n",
    "    prec_exp = precision_score(all_labels_exp, preds_exp, zero_division=0)\n",
    "    rec_exp = recall_score(all_labels_exp, preds_exp, zero_division=0)\n",
    "    f1_exp = f1_score(all_labels_exp, preds_exp, zero_division=0)\n",
    "    \n",
    "    print(f\"{name:<15} {alpha:<6.1f} {beta:<6.1f} {auc_exp:<8.4f} {prec_exp:<8.4f} {rec_exp:<8.4f} {f1_exp:<8.4f}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CCC2 - Combined Collaborative Approach (Ensemble)\n",
      "======================================================================\n",
      "\n",
      "Strategy:\n",
      "  Ensemble: final_score = α * CCA_norm + β * CCB_norm\n",
      "  Current weights: α=0.5, β=0.5\n",
      "  Threshold: 0.5\n",
      "\n",
      "Models:\n",
      "  CCA2: 351,648 parameters\n",
      "  CCB2: 352,737 parameters\n",
      "\n",
      "Validation Performance:\n",
      "  AUC-ROC: 0.9366\n",
      "  Accuracy: 0.7876\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.7876\n",
      "  F1 Score: 0.8812\n",
      "\n",
      "Test Performance:\n",
      "  Accuracy: 0.7811\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.7811\n",
      "  F1 Score: 0.8771\n",
      "\n",
      "Ready for comparison with CCA2, CCB2, CCC1, and CCC3!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CCC2 - Combined Collaborative Approach (Ensemble)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nStrategy:\")\n",
    "print(f\"  Ensemble: final_score = α * CCA_norm + β * CCB_norm\")\n",
    "print(f\"  Current weights: α={ALPHA}, β={BETA}\")\n",
    "print(f\"  Threshold: {ENSEMBLE_THRESHOLD}\")\n",
    "\n",
    "print(f\"\\nModels:\")\n",
    "print(f\"  CCA2: {sum(p.numel() for p in cca_model.parameters()):,} parameters\")\n",
    "print(f\"  CCB2: {sum(p.numel() for p in ccb_model.parameters()):,} parameters\")\n",
    "\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  AUC-ROC: {val_auc:.4f}\")\n",
    "print(f\"  Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  Precision: {val_prec:.4f}\")\n",
    "print(f\"  Recall: {val_rec:.4f}\")\n",
    "print(f\"  F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Precision: {test_prec:.4f}\")\n",
    "print(f\"  Recall: {test_rec:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nReady for comparison with CCA2, CCB2, CCC1, and CCC3!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
