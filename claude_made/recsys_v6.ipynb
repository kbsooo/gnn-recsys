{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCN V6 - Low-Rank Regularized GNN\n",
    "\n",
    "## 핵심 아이디어: \"High-Dimensional Data Analysis with Low-Dimensional Models\" 적용\n",
    "\n",
    "### 책의 Chapter 4 (Low-Rank Matrix Recovery) 개념 적용:\n",
    "1. **SVD-based Initialization**: Rating matrix의 저랭크 구조를 반영한 초기화\n",
    "2. **Nuclear Norm Regularization**: ||U||_F^2 + ||V||_F^2 정규화\n",
    "3. **Incoherence Penalty**: 임베딩 벡터 간 직교성 유도\n",
    "4. **Confidence Weighting**: User/Item activity 기반 신뢰도 가중치\n",
    "\n",
    "### V5 대비 변경사항:\n",
    "- 3-way split (Train/Valid/Test = 70/15/15)\n",
    "- SVD spectrum 분석으로 최적 rank 탐색\n",
    "- 저랭크 이론 기반 정규화 강화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Device: {device} ({torch.cuda.get_device_name()})')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f'Device: {device}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로딩 및 3-Way Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# User/Item 인덱싱\n",
    "user2idx = {u: i for i, u in enumerate(sorted(df['user'].unique()))}\n",
    "item2idx = {it: i for i, it in enumerate(sorted(df['item'].unique()))}\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "n_users, n_items = len(user2idx), len(item2idx)\n",
    "\n",
    "df['user_idx'] = df['user'].map(user2idx)\n",
    "df['item_idx'] = df['item'].map(item2idx)\n",
    "df['label'] = (df['rating'] >= 4.0).astype(int)  # 4.0 이상만 positive\n",
    "\n",
    "positive_df = df[df['label'] == 1].copy()\n",
    "\n",
    "print(f\"Total ratings: {len(df):,}\")\n",
    "print(f\"Positive ratings (>= 4.0): {len(positive_df):,}\")\n",
    "print(f\"Users: {n_users}, Items: {n_items}\")\n",
    "print(f\"Sparsity: {100 * (1 - len(df) / (n_users * n_items)):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-Way Split: Train 70%, Valid 15%, Test 15%\n",
    "# User별 stratified split (데이터 누수 방지)\n",
    "\n",
    "train_data, val_data, test_data = [], [], []\n",
    "\n",
    "for user_idx in range(n_users):\n",
    "    user_pos = positive_df[positive_df['user_idx'] == user_idx]\n",
    "    n_pos = len(user_pos)\n",
    "    \n",
    "    if n_pos >= 3:  # 최소 3개 이상이어야 3-way split 가능\n",
    "        user_pos = user_pos.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        \n",
    "        # 70 / 15 / 15 split\n",
    "        train_end = int(0.7 * n_pos)\n",
    "        val_end = int(0.85 * n_pos)\n",
    "        \n",
    "        # 최소 1개씩은 보장\n",
    "        train_end = max(1, train_end)\n",
    "        val_end = max(train_end + 1, val_end)\n",
    "        \n",
    "        train_data.append(user_pos.iloc[:train_end])\n",
    "        val_data.append(user_pos.iloc[train_end:val_end])\n",
    "        test_data.append(user_pos.iloc[val_end:])\n",
    "        \n",
    "    elif n_pos == 2:\n",
    "        # 2개면 train 1개, val 1개\n",
    "        user_pos = user_pos.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        train_data.append(user_pos.iloc[:1])\n",
    "        val_data.append(user_pos.iloc[1:])\n",
    "        \n",
    "    elif n_pos == 1:\n",
    "        # 1개면 train에만\n",
    "        train_data.append(user_pos)\n",
    "\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "val_df = pd.concat(val_data, ignore_index=True)\n",
    "test_df = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "print(f\"3-Way Split:\")\n",
    "print(f\"  Train: {len(train_df):,} ({100*len(train_df)/len(positive_df):.1f}%)\")\n",
    "print(f\"  Valid: {len(val_df):,} ({100*len(val_df)/len(positive_df):.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df):,} ({100*len(test_df)/len(positive_df):.1f}%)\")\n",
    "print(f\"  Total: {len(train_df) + len(val_df) + len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor 변환\n",
    "train_users = torch.LongTensor(train_df['user_idx'].values)\n",
    "train_items = torch.LongTensor(train_df['item_idx'].values)\n",
    "val_users = torch.LongTensor(val_df['user_idx'].values)\n",
    "val_items = torch.LongTensor(val_df['item_idx'].values)\n",
    "test_users = torch.LongTensor(test_df['user_idx'].values)\n",
    "test_items = torch.LongTensor(test_df['item_idx'].values)\n",
    "\n",
    "# User별 positive items (train set 기준)\n",
    "user_pos_items_train = defaultdict(set)\n",
    "for u, i in zip(train_df['user_idx'].values, train_df['item_idx'].values):\n",
    "    user_pos_items_train[int(u)].add(int(i))\n",
    "\n",
    "# Negative candidates (train set에 없는 items)\n",
    "user_neg_candidates = {}\n",
    "for u in range(n_users):\n",
    "    pos = user_pos_items_train[u]\n",
    "    user_neg_candidates[u] = np.array(list(set(range(n_items)) - pos))\n",
    "\n",
    "print(f\"Pre-computed negative candidates for {n_users} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVD Spectrum 분석 (책 Chapter 4.2)\n",
    "\n",
    "Rating Matrix의 singular value 분포를 분석하여 최적의 embedding dimension (rank) 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Rating Matrix 구성 (Train data 기준)\n",
    "# Binary matrix: positive interaction만 1\n",
    "row = train_df['user_idx'].values\n",
    "col = train_df['item_idx'].values\n",
    "data = np.ones(len(train_df))\n",
    "\n",
    "R_sparse = csr_matrix((data, (row, col)), shape=(n_users, n_items))\n",
    "print(f\"Rating Matrix: {R_sparse.shape}\")\n",
    "print(f\"Non-zeros: {R_sparse.nnz:,} ({100*R_sparse.nnz/(n_users*n_items):.4f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated SVD로 singular value spectrum 분석\n",
    "# R ≈ U @ Σ @ V^T\n",
    "k = min(100, min(n_users, n_items) - 1)  # Top k singular values\n",
    "\n",
    "print(f\"Computing top {k} singular values...\")\n",
    "U_svd, sigma, Vt_svd = svds(R_sparse.astype(np.float32), k=k)\n",
    "\n",
    "# Sort by descending order (svds returns in ascending order)\n",
    "idx = np.argsort(sigma)[::-1]\n",
    "sigma = sigma[idx]\n",
    "U_svd = U_svd[:, idx]\n",
    "Vt_svd = Vt_svd[idx, :]\n",
    "\n",
    "print(f\"Top 10 singular values: {sigma[:10]}\")\n",
    "print(f\"Largest: {sigma[0]:.2f}, Smallest: {sigma[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance ratio\n",
    "total_variance = np.sum(sigma**2)\n",
    "cumulative_variance = np.cumsum(sigma**2) / total_variance\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Singular Value Decay\n",
    "axes[0].plot(sigma, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Singular Value')\n",
    "axes[0].set_title('Singular Value Spectrum (Decay)')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# 2. Cumulative Variance\n",
    "axes[1].plot(cumulative_variance, 'g-', linewidth=2)\n",
    "axes[1].axhline(y=0.8, color='r', linestyle='--', label='80%')\n",
    "axes[1].axhline(y=0.9, color='orange', linestyle='--', label='90%')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Variance Ratio')\n",
    "axes[1].set_title('Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Optimal Rank Selection\n",
    "rank_80 = np.argmax(cumulative_variance >= 0.8) + 1\n",
    "rank_90 = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "rank_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "axes[2].bar(['80%', '90%', '95%'], [rank_80, rank_90, rank_95], color=['green', 'orange', 'red'])\n",
    "axes[2].set_ylabel('Required Rank')\n",
    "axes[2].set_title('Rank for Variance Threshold')\n",
    "for i, v in enumerate([rank_80, rank_90, rank_95]):\n",
    "    axes[2].text(i, v + 1, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.suptitle('Low-Rank Structure Analysis (Chapter 4.2)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal Rank Analysis:\")\n",
    "print(f\"  80% variance: rank {rank_80}\")\n",
    "print(f\"  90% variance: rank {rank_90}\")\n",
    "print(f\"  95% variance: rank {rank_95}\")\n",
    "print(f\"\\nV5 used: emb_dim=32\")\n",
    "print(f\"Recommendation: Use emb_dim={min(64, rank_90)} for balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(train_df):\n",
    "    \"\"\"Bipartite graph with normalized adjacency\"\"\"\n",
    "    users = train_df['user_idx'].values\n",
    "    items = train_df['item_idx'].values\n",
    "    \n",
    "    # User -> Item edges\n",
    "    edge_u2i = np.array([users, items + n_users])\n",
    "    # Item -> User edges\n",
    "    edge_i2u = np.array([items + n_users, users])\n",
    "    \n",
    "    edge_index = torch.LongTensor(np.concatenate([edge_u2i, edge_i2u], axis=1))\n",
    "    \n",
    "    # Symmetric normalization (D^{-1/2} A D^{-1/2})\n",
    "    num_nodes = n_users + n_items\n",
    "    deg = torch.zeros(num_nodes).scatter_add(0, edge_index[0], torch.ones(edge_index.shape[1]))\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    edge_weight = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]\n",
    "    \n",
    "    return edge_index.to(device), edge_weight.to(device)\n",
    "\n",
    "edge_index, edge_weight = build_graph(train_df)\n",
    "print(f\"Graph: {edge_index.shape[1]:,} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LightGCN with Low-Rank Regularization (책 Chapter 4.3, 4.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankLightGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    LightGCN with Low-Rank Theory Enhancements:\n",
    "    1. SVD-based initialization (Chapter 4.2)\n",
    "    2. Nuclear Norm regularization via Frobenius norm (Chapter 4.3)\n",
    "    3. Incoherence penalty (Chapter 4.4.3)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_items, emb_dim=32, n_layers=1, \n",
    "                 U_init=None, V_init=None):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        \n",
    "        # SVD-based initialization\n",
    "        if U_init is not None and V_init is not None:\n",
    "            with torch.no_grad():\n",
    "                self.user_emb.weight.copy_(torch.FloatTensor(U_init))\n",
    "                self.item_emb.weight.copy_(torch.FloatTensor(V_init))\n",
    "            print(f\"Initialized with SVD (rank={emb_dim})\")\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "            nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "            print(f\"Initialized with Xavier uniform\")\n",
    "    \n",
    "    def forward(self, edge_index, edge_weight):\n",
    "        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for _ in range(self.n_layers):\n",
    "            row, col = edge_index\n",
    "            messages = all_emb[col] * edge_weight.unsqueeze(1)\n",
    "            all_emb = torch.zeros_like(all_emb).scatter_add(\n",
    "                0, row.unsqueeze(1).expand(-1, self.emb_dim), messages\n",
    "            )\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.mean(torch.stack(embs), dim=0)\n",
    "        return final_emb[:self.n_users], final_emb[self.n_users:]\n",
    "    \n",
    "    def nuclear_norm_reg(self):\n",
    "        \"\"\"\n",
    "        Nuclear Norm Regularization (Chapter 4.3)\n",
    "        ||W||_* ≈ ||U||_F^2 + ||V||_F^2 for low-rank factorization W ≈ UV^T\n",
    "        \"\"\"\n",
    "        u_norm = torch.norm(self.user_emb.weight, p='fro') ** 2\n",
    "        v_norm = torch.norm(self.item_emb.weight, p='fro') ** 2\n",
    "        return (u_norm + v_norm) / 2\n",
    "    \n",
    "    def incoherence_penalty(self):\n",
    "        \"\"\"\n",
    "        Incoherence Condition (Chapter 4.4.3)\n",
    "        Encourage embedding vectors to be incoherent (spread out)\n",
    "        Penalize when embeddings are too aligned (high dot product)\n",
    "        \"\"\"\n",
    "        # Sample-based computation (for efficiency)\n",
    "        n_sample = min(100, self.n_users)\n",
    "        idx = torch.randperm(self.n_users)[:n_sample]\n",
    "        U_sample = self.user_emb.weight[idx]  # [n_sample, emb_dim]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        U_norm = F.normalize(U_sample, p=2, dim=1)\n",
    "        \n",
    "        # Gram matrix (similarity between all pairs)\n",
    "        gram = torch.mm(U_norm, U_norm.t())  # [n_sample, n_sample]\n",
    "        \n",
    "        # Penalize off-diagonal elements (we want identity-like structure)\n",
    "        # Remove diagonal\n",
    "        mask = 1 - torch.eye(n_sample, device=gram.device)\n",
    "        off_diag = gram * mask\n",
    "        \n",
    "        # Penalize high correlation\n",
    "        penalty = torch.sum(off_diag ** 2) / (n_sample * (n_sample - 1))\n",
    "        return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD-based Initialization\n",
    "EMB_DIM = 32  # V5와 동일하게 시작, 나중에 튜닝 가능\n",
    "N_LAYERS = 1  # V4에서 최적으로 확인됨\n",
    "\n",
    "# Truncated SVD for initialization\n",
    "U_init_svd, sigma_init, Vt_init_svd = svds(R_sparse.astype(np.float32), k=EMB_DIM)\n",
    "\n",
    "# Sort by descending singular values\n",
    "idx = np.argsort(sigma_init)[::-1]\n",
    "sigma_init = sigma_init[idx]\n",
    "U_init_svd = U_init_svd[:, idx]\n",
    "Vt_init_svd = Vt_init_svd[idx, :]\n",
    "\n",
    "# Scale by sqrt of singular values: U*sqrt(Σ), V*sqrt(Σ)\n",
    "# This way U @ V^T ≈ U*Σ*V^T\n",
    "sqrt_sigma = np.sqrt(sigma_init)\n",
    "U_init = U_init_svd * sqrt_sigma  # [n_users, emb_dim]\n",
    "V_init = Vt_init_svd.T * sqrt_sigma  # [n_items, emb_dim]\n",
    "\n",
    "print(f\"SVD Initialization:\")\n",
    "print(f\"  U: {U_init.shape}, V: {V_init.shape}\")\n",
    "print(f\"  Top singular values: {sigma_init[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 생성\n",
    "model = LowRankLightGCN(\n",
    "    n_users, n_items, \n",
    "    emb_dim=EMB_DIM, \n",
    "    n_layers=N_LAYERS,\n",
    "    U_init=U_init,\n",
    "    V_init=V_init\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  LowRankLightGCN: emb={EMB_DIM}, layers={N_LAYERS}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confidence-Weighted BPR Loss (책 Chapter 4.4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User/Item confidence 계산\n",
    "# 많이 활동한 user/인기 item에 더 높은 신뢰도\n",
    "\n",
    "user_activity = train_df.groupby('user_idx').size().values\n",
    "item_popularity = train_df.groupby('item_idx').size().reindex(range(n_items), fill_value=0).values\n",
    "\n",
    "# Log-scale confidence (급격한 차이 완화)\n",
    "user_confidence = np.log1p(user_activity)\n",
    "item_confidence = np.log1p(item_popularity)\n",
    "\n",
    "# Normalize to [0.5, 1.5] range\n",
    "user_confidence = 0.5 + (user_confidence - user_confidence.min()) / (user_confidence.max() - user_confidence.min())\n",
    "item_confidence = 0.5 + (item_confidence - item_confidence.min()) / (item_confidence.max() - item_confidence.min() + 1e-8)\n",
    "\n",
    "user_conf_tensor = torch.FloatTensor(user_confidence).to(device)\n",
    "item_conf_tensor = torch.FloatTensor(item_confidence).to(device)\n",
    "\n",
    "print(f\"User confidence: min={user_confidence.min():.3f}, max={user_confidence.max():.3f}\")\n",
    "print(f\"Item confidence: min={item_confidence.min():.3f}, max={item_confidence.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_weighted_bpr_loss(pos_scores, neg_scores, user_ids, pos_item_ids):\n",
    "    \"\"\"\n",
    "    Confidence-Weighted BPR Loss (Chapter 4.4.5 - Stable Matrix Completion)\n",
    "    \n",
    "    Weight each sample by confidence based on user activity and item popularity.\n",
    "    \"\"\"\n",
    "    # Confidence = user_conf * item_conf\n",
    "    confidence = user_conf_tensor[user_ids] * item_conf_tensor[pos_item_ids]\n",
    "    \n",
    "    # BPR loss with confidence weighting\n",
    "    diff = pos_scores.unsqueeze(1) - neg_scores  # [batch, num_neg]\n",
    "    bpr = -torch.log(torch.sigmoid(diff) + 1e-8).mean(dim=1)  # [batch]\n",
    "    \n",
    "    # Weighted mean\n",
    "    weighted_loss = (bpr * confidence).sum() / confidence.sum()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, eval_users, eval_items, k=10, n_neg=99, sample_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate on given user-item pairs.\n",
    "    Sample-based ranking: 1 positive + n_neg negatives\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    u_emb, i_emb = model(edge_index, edge_weight)\n",
    "    \n",
    "    if sample_size and len(eval_users) > sample_size:\n",
    "        sample_idx = np.random.choice(len(eval_users), sample_size, replace=False)\n",
    "    else:\n",
    "        sample_idx = np.arange(len(eval_users))\n",
    "    \n",
    "    hits, ndcgs = [], []\n",
    "    \n",
    "    for idx in sample_idx:\n",
    "        user_idx = int(eval_users[idx])\n",
    "        pos_item = int(eval_items[idx])\n",
    "        \n",
    "        cands = user_neg_candidates[user_idx]\n",
    "        if len(cands) < n_neg:\n",
    "            continue\n",
    "        \n",
    "        neg_items = np.random.choice(cands, size=n_neg, replace=False)\n",
    "        candidates = np.concatenate([[pos_item], neg_items])\n",
    "        \n",
    "        u_t = torch.full((len(candidates),), user_idx, dtype=torch.long, device=device)\n",
    "        i_t = torch.LongTensor(candidates).to(device)\n",
    "        scores = (u_emb[u_t] * i_emb[i_t]).sum(dim=1).cpu().numpy()\n",
    "        \n",
    "        rank = (scores > scores[0]).sum() + 1\n",
    "        hits.append(1.0 if rank <= k else 0.0)\n",
    "        ndcgs.append(1.0 / np.log2(rank + 1) if rank <= k else 0.0)\n",
    "    \n",
    "    return np.mean(hits), np.mean(ndcgs)\n",
    "\n",
    "print(\"Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LR = 5e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "NUM_NEG = 4\n",
    "\n",
    "# Low-Rank Regularization Weights\n",
    "LAMBDA_NUCLEAR = 1e-4  # Nuclear norm regularization\n",
    "LAMBDA_INCOH = 1e-3    # Incoherence penalty\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "history = {\n",
    "    'loss': [], 'bpr_loss': [], 'nuclear_loss': [], 'incoh_loss': [],\n",
    "    'val_hit@10': [], 'val_ndcg@10': []\n",
    "}\n",
    "\n",
    "best_val_hit = 0\n",
    "best_val_ndcg = 0\n",
    "n_train = len(train_users)\n",
    "\n",
    "print(f\"Training LowRankLightGCN V6\")\n",
    "print(f\"Config: BS={BATCH_SIZE}, LR={LR}, NEG={NUM_NEG}\")\n",
    "print(f\"Low-Rank Reg: Nuclear={LAMBDA_NUCLEAR}, Incoh={LAMBDA_INCOH}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    perm = torch.randperm(n_train)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_bpr = 0\n",
    "    epoch_nuclear = 0\n",
    "    epoch_incoh = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, n_train, BATCH_SIZE):\n",
    "        batch_idx = perm[i:i+BATCH_SIZE]\n",
    "        pos_u = train_users[batch_idx].to(device)\n",
    "        pos_i = train_items[batch_idx].to(device)\n",
    "        \n",
    "        # Negative sampling\n",
    "        neg_i = torch.zeros((len(batch_idx), NUM_NEG), dtype=torch.long)\n",
    "        for j, u_idx in enumerate(pos_u.cpu().numpy()):\n",
    "            cands = user_neg_candidates[int(u_idx)]\n",
    "            neg_i[j] = torch.LongTensor(np.random.choice(cands, size=NUM_NEG, replace=False))\n",
    "        neg_i = neg_i.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        u_emb, i_emb = model(edge_index, edge_weight)\n",
    "        \n",
    "        # Scores\n",
    "        pos_scores = (u_emb[pos_u] * i_emb[pos_i]).sum(dim=1)\n",
    "        neg_u_expanded = pos_u.unsqueeze(1).expand(-1, NUM_NEG)\n",
    "        neg_scores = (u_emb[neg_u_expanded] * i_emb[neg_i]).sum(dim=2)\n",
    "        \n",
    "        # Losses\n",
    "        bpr_loss = confidence_weighted_bpr_loss(pos_scores, neg_scores, pos_u, pos_i)\n",
    "        nuclear_loss = LAMBDA_NUCLEAR * model.nuclear_norm_reg()\n",
    "        incoh_loss = LAMBDA_INCOH * model.incoherence_penalty()\n",
    "        \n",
    "        total_loss = bpr_loss + nuclear_loss + incoh_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_bpr += bpr_loss.item()\n",
    "        epoch_nuclear += nuclear_loss.item()\n",
    "        epoch_incoh += incoh_loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Record losses\n",
    "    history['loss'].append(epoch_loss / n_batches)\n",
    "    history['bpr_loss'].append(epoch_bpr / n_batches)\n",
    "    history['nuclear_loss'].append(epoch_nuclear / n_batches)\n",
    "    history['incoh_loss'].append(epoch_incoh / n_batches)\n",
    "    \n",
    "    # Validate every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        val_hit, val_ndcg = evaluate(model, val_users, val_items, sample_size=3000)\n",
    "        history['val_hit@10'].append(val_hit)\n",
    "        history['val_ndcg@10'].append(val_ndcg)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {epoch_loss/n_batches:.4f} \"\n",
    "              f\"(BPR: {epoch_bpr/n_batches:.4f}, Nuc: {epoch_nuclear/n_batches:.6f}, Inc: {epoch_incoh/n_batches:.6f}) \"\n",
    "              f\"| Val Hit@10: {val_hit:.4f} | Val NDCG@10: {val_ndcg:.4f}\")\n",
    "        \n",
    "        if val_hit > best_val_hit:\n",
    "            best_val_hit = val_hit\n",
    "            best_val_ndcg = val_ndcg\n",
    "            torch.save(model.state_dict(), 'best_lightgcn_v6.pt')\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {elapsed/60:.1f} minutes\")\n",
    "print(f\"Best Validation: Hit@10={best_val_hit:.4f}, NDCG@10={best_val_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Total Loss\n",
    "axes[0, 0].plot(history['loss'], 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# BPR Loss\n",
    "axes[0, 1].plot(history['bpr_loss'], 'g-', linewidth=2)\n",
    "axes[0, 1].set_title('BPR Loss (Confidence-Weighted)')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Nuclear Norm Loss\n",
    "axes[0, 2].plot(history['nuclear_loss'], 'r-', linewidth=2)\n",
    "axes[0, 2].set_title('Nuclear Norm Regularization')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "# Incoherence Loss\n",
    "axes[1, 0].plot(history['incoh_loss'], 'purple', linewidth=2)\n",
    "axes[1, 0].set_title('Incoherence Penalty')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Validation Hit@10\n",
    "epochs_val = np.arange(5, EPOCHS+1, 5)[:len(history['val_hit@10'])]\n",
    "axes[1, 1].plot(epochs_val, history['val_hit@10'], 'go-', linewidth=2)\n",
    "axes[1, 1].set_title('Validation Hit@10')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# Validation NDCG@10\n",
    "axes[1, 2].plot(epochs_val, history['val_ndcg@10'], 'ro-', linewidth=2)\n",
    "axes[1, 2].set_title('Validation NDCG@10')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('LowRankLightGCN V6 Training Progress', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Set 최종 평가 (1회만!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_lightgcn_v6.pt'))\n",
    "model.eval()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL TEST SET EVALUATION (Single Run - No Peeking!)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test set evaluation\n",
    "test_hit, test_ndcg = evaluate(model, test_users, test_items, sample_size=None)\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Hit@10:  {test_hit:.4f}\")\n",
    "print(f\"  NDCG@10: {test_ndcg:.4f}\")\n",
    "\n",
    "print(f\"\\nValidation Set Results (for reference):\")\n",
    "print(f\"  Hit@10:  {best_val_hit:.4f}\")\n",
    "print(f\"  NDCG@10: {best_val_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Threshold & O/X 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold tuning on validation set\n",
    "with torch.no_grad():\n",
    "    u_emb, i_emb = model(edge_index, edge_weight)\n",
    "\n",
    "# Positive scores (validation)\n",
    "val_pos_scores = (u_emb[val_users.to(device)] * i_emb[val_items.to(device)]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "# Negative scores (validation)\n",
    "val_neg_scores = []\n",
    "for user_idx in val_df['user_idx'].unique():\n",
    "    n_pos = (val_df['user_idx'] == user_idx).sum()\n",
    "    cands = user_neg_candidates[int(user_idx)]\n",
    "    neg_items = np.random.choice(cands, size=min(n_pos, len(cands)), replace=False)\n",
    "    \n",
    "    u_t = torch.full((len(neg_items),), user_idx, dtype=torch.long, device=device)\n",
    "    i_t = torch.LongTensor(neg_items).to(device)\n",
    "    scores = (u_emb[u_t] * i_emb[i_t]).sum(dim=1).cpu().numpy()\n",
    "    val_neg_scores.extend(scores)\n",
    "\n",
    "all_scores = np.concatenate([val_pos_scores, np.array(val_neg_scores)])\n",
    "all_labels = np.concatenate([np.ones(len(val_pos_scores)), np.zeros(len(val_neg_scores))])\n",
    "\n",
    "print(f\"Threshold tuning on {len(all_scores)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold\n",
    "thresholds = np.percentile(all_scores, [30, 40, 50, 60, 70, 80, 90])\n",
    "\n",
    "print(\"Threshold Tuning:\")\n",
    "print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_prec, best_th, best_f1 = 0, 0, 0\n",
    "for th in thresholds:\n",
    "    preds = (all_scores >= th).astype(int)\n",
    "    tp = ((preds == 1) & (all_labels == 1)).sum()\n",
    "    fp = ((preds == 1) & (all_labels == 0)).sum()\n",
    "    fn = ((preds == 0) & (all_labels == 1)).sum()\n",
    "    \n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "    \n",
    "    print(f\"{th:<12.4f} {prec:<12.4f} {rec:<12.4f} {f1:<12.4f}\")\n",
    "    \n",
    "    if prec >= 0.6 and prec > best_prec:\n",
    "        best_prec, best_th, best_f1 = prec, th, f1\n",
    "    elif best_prec < 0.6 and f1 > best_f1:\n",
    "        best_f1, best_th, best_prec = f1, th, prec\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Selected Threshold: {best_th:.4f} (Precision: {best_prec:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ox(test_pairs_df):\n",
    "    results = []\n",
    "    \n",
    "    known_mask = test_pairs_df['user'].isin(user2idx) & test_pairs_df['item'].isin(item2idx)\n",
    "    known_df = test_pairs_df[known_mask]\n",
    "    unknown_df = test_pairs_df[~known_mask]\n",
    "    \n",
    "    if len(known_df) > 0:\n",
    "        u_idx = torch.LongTensor([user2idx[u] for u in known_df['user']]).to(device)\n",
    "        i_idx = torch.LongTensor([item2idx[i] for i in known_df['item']]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = (u_emb[u_idx] * i_emb[i_idx]).sum(dim=1).cpu().numpy()\n",
    "        \n",
    "        for (_, row), score in zip(known_df.iterrows(), scores):\n",
    "            results.append({\n",
    "                'user': row['user'], 'item': row['item'],\n",
    "                'recommend': 'O' if score >= best_th else 'X'\n",
    "            })\n",
    "    \n",
    "    for _, row in unknown_df.iterrows():\n",
    "        results.append({'user': row['user'], 'item': row['item'], 'recommend': 'X'})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test on validation set\n",
    "preds = predict_ox(val_df[['user', 'item']])\n",
    "o_ratio = (preds['recommend'] == 'O').mean()\n",
    "print(f\"\\nO ratio on validation: {100*o_ratio:.1f}%\")\n",
    "print(preds.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 최종 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LightGCN V6 - Low-Rank Regularized GNN Final Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nLow-Rank Theory Enhancements (from textbook):\")\n",
    "print(f\"  1. SVD-based Initialization (Chapter 4.2)\")\n",
    "print(f\"  2. Nuclear Norm Regularization (Chapter 4.3): λ={LAMBDA_NUCLEAR}\")\n",
    "print(f\"  3. Incoherence Penalty (Chapter 4.4.3): λ={LAMBDA_INCOH}\")\n",
    "print(f\"  4. Confidence-Weighted BPR (Chapter 4.4.5)\")\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  emb_dim: {EMB_DIM}\")\n",
    "print(f\"  n_layers: {N_LAYERS}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nData Split:\")\n",
    "print(f\"  Train: {len(train_df):,} ({100*len(train_df)/len(positive_df):.1f}%)\")\n",
    "print(f\"  Valid: {len(val_df):,} ({100*len(val_df)/len(positive_df):.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df):,} ({100*len(test_df)/len(positive_df):.1f}%)\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Validation: Hit@10={best_val_hit:.4f}, NDCG@10={best_val_ndcg:.4f}\")\n",
    "print(f\"  Test:       Hit@10={test_hit:.4f}, NDCG@10={test_ndcg:.4f}\")\n",
    "print(f\"  Precision:  {best_prec:.4f}\")\n",
    "print(f\"  O ratio:    {100*o_ratio:.1f}%\")\n",
    "\n",
    "print(f\"\\nV5 (In-batch Neg) → V6 (Low-Rank Reg) Comparison:\")\n",
    "print(f\"  Note: V5 used 80/20 split, V6 uses 70/15/15 split\")\n",
    "print(f\"  V5 Val Hit@10:  ~0.78 (on 80/20 split)\")\n",
    "print(f\"  V6 Val Hit@10:  {best_val_hit:.4f}\")\n",
    "print(f\"  V6 Test Hit@10: {test_hit:.4f} (NEW - unbiased estimate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready for final test inference\n",
    "# final_test_df = pd.read_csv('../data/test.csv')\n",
    "# final_preds = predict_ox(final_test_df)\n",
    "# final_preds.to_csv('predictions_v6.csv', index=False)\n",
    "\n",
    "print(\"Final test inference ready.\")\n",
    "print(\"Uncomment above code when test.csv is provided.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
