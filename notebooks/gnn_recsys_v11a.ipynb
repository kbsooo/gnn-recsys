{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GNN ê¸°ë°˜ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ\n# V11A: Hard Negative Mining\n# Step 2 - Breakthrough experiment"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "í™˜ê²½ ì„¤ì •\n",
      "============================================================\n",
      "PyTorch ë²„ì „: 2.9.0\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: False\n",
      "MPS ì‚¬ìš© ê°€ëŠ¥: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"í™˜ê²½ ì„¤ì •\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (V11A - Hard Negative Mining)\nCONFIG = {\n    # Filtering ê¸°ì¤€ (V6ì™€ ë™ì¼)\n    'min_user_interactions': 30,\n    'min_item_interactions': 10,\n    'rating_threshold': None,\n    \n    # Split ë¹„ìœ¨ (V6ì™€ ë™ì¼)\n    'train_ratio': 0.70,\n    'valid_ratio': 0.15,\n    'test_ratio': 0.15,\n    \n    # ëª¨ë¸ íŒŒë¼ë¯¸í„° (V9cì™€ ë™ì¼)\n    'embedding_dim': 64,\n    'n_layers': 2,\n    \n    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n    'learning_rate': 0.001,\n    'weight_decay': 1e-4,\n    'batch_size': 512,\n    'epochs': 100,\n    'patience': 20,\n    'neg_ratio': 6,\n    \n    # InfoNCE íŒŒë¼ë¯¸í„°\n    'temperature': 0.2,\n    \n    # â­ Hard Negative Mining íŒŒë¼ë¯¸í„°\n    'hard_neg_ratio': 0.5,  # 50% hard negatives, 50% random\n    \n    # í‰ê°€\n    'top_k': 10,\n    \n    # ì‹œìŠ¤í…œ\n    'device': 'mps' if torch.backends.mps.is_available() else 'cpu',\n    'seed': 42,\n    \n    # ê²½ë¡œ\n    'data_dir': '../data',\n    'processed_dir': '../data/processed',\n    'model_dir': '../models',\n    'result_dir': '../results',\n}\n\n# í´ë” ìƒì„±\nfor dir_path in [CONFIG['processed_dir'], CONFIG['model_dir'], CONFIG['result_dir']]:\n    os.makedirs(dir_path, exist_ok=True)\n\n# Random seed ê³ ì •\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    if torch.backends.mps.is_available():\n        torch.mps.manual_seed(seed)\n\nset_seed(CONFIG['seed'])\n\nprint(\"=\" * 60)\nprint(\"V11A ì„¤ì • ì™„ë£Œ! (Hard Negative Mining)\")\nprint(\"=\" * 60)\nprint(f\"Device: {CONFIG['device']}\")\nprint(\"\\nğŸ”¥ í•µì‹¬ ë³€ê²½: Random Negative â†’ Hard Negative Mining\")\nprint(f\"  V9cê¹Œì§€: ì™„ì „ ëœë¤ ìƒ˜í”Œë§ (ë„ˆë¬´ ì‰¬ìš´ negative)\")\nprint(f\"  V11a: ëª¨ë¸ ê¸°ë°˜ hard negative ì„ íƒ\")\nprint(f\"\\nì „ëµ:\")\nprint(f\"  1. Semi-hard negatives (50%): ëª¨ë¸ ì ìˆ˜ ì¤‘ê°„ ë²”ìœ„ [0.3-0.7]\")\nprint(f\"     â†’ ë„ˆë¬´ ì‰½ì§€ë„, ë„ˆë¬´ ì–´ë µì§€ë„ ì•Šì€ negatives\")\nprint(f\"  2. In-batch negatives (50%): ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ì˜ positive items\")\nprint(f\"     â†’ ë‹¤ë¥¸ ìœ ì €ëŠ” ì¢‹ì•„í•˜ì§€ë§Œ ì´ ìœ ì €ëŠ” ì‹«ì–´í•˜ëŠ” items\")\nprint(f\"\\nê¸°ëŒ€ íš¨ê³¼:\")\nprint(f\"  - ëœë¤ë³´ë‹¤ í•™ìŠµì´ ì–´ë ¤ì›Œì ¸ì„œ ë” ê°•í•œ discriminative power\")\nprint(f\"  - Embedding spaceê°€ ë” êµ¬ì¡°í™”ë¨\")\nprint(f\"  - ì˜ˆìƒ ê°œì„ : +4~8% Recall@10\")\nprint(f\"\\nê¸°íƒ€ ì„¤ì • (V9cì™€ ë™ì¼):\")\nprint(f\"  Loss: InfoNCE (temperature={CONFIG['temperature']})\")\nprint(f\"  Layers: {CONFIG['n_layers']}\")\nprint(f\"  Embedding dim: {CONFIG['embedding_dim']}\")\nprint(f\"  Negative ratio: {CONFIG['neg_ratio']}\")\nprint(\"=\" * 60)\nprint(\"\\nğŸ¯ ëª©í‘œ: Recall@10 > 18% (V9cì˜ 15.78% ëŒ€ë¹„ breakthrough!)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ë¡œë“œ (V6 ì „ì²˜ë¦¬ íŒŒì¼ ì‚¬ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "V6 ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ\n",
      "============================================================\n",
      "\n",
      "ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\n",
      "  Users: 529\n",
      "  Items: 2283\n",
      "  Train: 56,072\n",
      "  Valid: 11,826\n",
      "  Test:  12,572\n",
      "\n",
      "âœ… V6ì™€ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš© â†’ ê³µì •í•œ ë¹„êµ ê°€ëŠ¥\n"
     ]
    }
   ],
   "source": [
    "# V6ì—ì„œ ì „ì²˜ë¦¬í•œ ë°ì´í„° ë¡œë“œ\n",
    "print(\"=\" * 60)\n",
    "print(\"V6 ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'train_split_v6.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'valid_split_v6.csv'))\n",
    "test_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'test_split_v6.csv'))\n",
    "\n",
    "with open(os.path.join(CONFIG['processed_dir'], 'id_mappings_v6.pkl'), 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "n_users = len(mappings['user_id_map'])\n",
    "n_items = len(mappings['item_id_map'])\n",
    "\n",
    "print(f\"\\në°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"  Users: {n_users}\")\n",
    "print(f\"  Items: {n_items}\")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Valid: {len(valid_df):,}\")\n",
    "print(f\"  Test:  {len(test_df):,}\")\n",
    "print(\"\\nâœ… V6ì™€ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš© â†’ ê³µì •í•œ ë¹„êµ ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Graph êµ¬ì„±\n",
      "============================================================\n",
      "\n",
      "[1] Train Graph (í•™ìŠµìš©)\n",
      "Node ìˆ˜: 2812 (User: 529, Item: 2283)\n",
      "Edge ìˆ˜: 112,144 (ì–‘ë°©í–¥)\n",
      "\n",
      "[2] Train+Valid Graph (Test í‰ê°€ìš©)\n",
      "Node ìˆ˜: 2812 (User: 529, Item: 2283)\n",
      "Edge ìˆ˜: 135,796 (ì–‘ë°©í–¥)\n",
      "\n",
      "âœ… Graph ìƒì„± ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/1w84fr7s5kxcwc2l24qrjjwc0000gn/T/ipykernel_86377/3903611053.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  edge_index = torch.tensor([\n"
     ]
    }
   ],
   "source": [
    "# Graph êµ¬ì„±\n",
    "print(\"=\" * 60)\n",
    "print(\"Graph êµ¬ì„±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_graph(df, n_users, n_items):\n",
    "    \"\"\"User-Item Bipartite Graph ìƒì„±\"\"\"\n",
    "    user_ids = df['user_id'].values\n",
    "    item_ids = df['item_id'].values + n_users\n",
    "    \n",
    "    edge_index = torch.tensor([\n",
    "        np.concatenate([user_ids, item_ids]),\n",
    "        np.concatenate([item_ids, user_ids])\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    print(f\"Node ìˆ˜: {n_users + n_items} (User: {n_users}, Item: {n_items})\")\n",
    "    print(f\"Edge ìˆ˜: {edge_index.shape[1]:,} (ì–‘ë°©í–¥)\")\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "print(\"\\n[1] Train Graph (í•™ìŠµìš©)\")\n",
    "train_edge_index = create_graph(train_df, n_users, n_items)\n",
    "\n",
    "print(\"\\n[2] Train+Valid Graph (Test í‰ê°€ìš©)\")\n",
    "train_valid_df = pd.concat([train_df, valid_df])\n",
    "train_valid_edge_index = create_graph(train_valid_df, n_users, n_items)\n",
    "\n",
    "print(\"\\nâœ… Graph ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â­ Hard Negative Mining (í•µì‹¬ ë³€ê²½!)\ndef create_user_item_dict(df):\n    \"\"\"Userë³„ë¡œ ìƒí˜¸ì‘ìš©í•œ item ì§‘í•© ìƒì„±\"\"\"\n    user_items = defaultdict(set)\n    for _, row in df.iterrows():\n        user_items[row['user_id']].add(row['item_id'])\n    return user_items\n\ndef hard_negative_sampling(model, edge_index, batch_df, user_items_dict, n_items, device, neg_ratio=6):\n    \"\"\"\n    Hard Negative Mining\n    \n    ì „ëµ:\n    1. Semi-hard negatives (50%): ëª¨ë¸ ì ìˆ˜ ì¤‘ê°„ ë²”ìœ„ [30-70 percentile]\n       â†’ ë„ˆë¬´ ì‰½ì§€ë„ (ì ìˆ˜ ë‚®ìŒ), ë„ˆë¬´ ì–´ë µì§€ë„ (ì ìˆ˜ ë†’ìŒ) ì•Šì€ negatives\n       â†’ í•™ìŠµ íš¨ìœ¨ ìµœëŒ€í™”\n    \n    2. In-batch negatives (50%): ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ë“¤ì˜ positive items\n       â†’ ë‹¤ë¥¸ ìœ ì €ëŠ” ì¢‹ì•„í•˜ì§€ë§Œ ì´ ìœ ì €ì—ê²ŒëŠ” negative\n       â†’ Collaborative filteringì˜ ë³¸ì§ˆ í•™ìŠµ\n    \n    Random sampling ëŒ€ë¹„ ì¥ì :\n    - Randomì€ ëŒ€ë¶€ë¶„ ë„ˆë¬´ ì‰¬ìš´ negatives (userì™€ ê´€ë ¨ ì—†ëŠ” items)\n    - Hard negativesëŠ” meaningfulí•œ ì°¨ì´ í•™ìŠµ\n    - Embedding spaceê°€ ë” êµ¬ì¡°í™”ë¨\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        user_emb, item_emb = model(edge_index.to(device))\n    model.train()\n    \n    pos_users = batch_df['user_id'].values\n    neg_users = []\n    neg_items = []\n    \n    # ê° ìœ ì €ë³„ë¡œ hard negatives ìƒì„±\n    for idx, user_id in enumerate(pos_users):\n        user_pos_items = user_items_dict[user_id]\n        \n        # 1. Semi-hard negatives (ì ˆë°˜)\n        n_semi_hard = neg_ratio // 2\n        \n        # í˜„ì¬ ìœ ì €ì˜ ëª¨ë“  itemì— ëŒ€í•œ ì ìˆ˜ ê³„ì‚°\n        user_emb_single = user_emb[user_id].unsqueeze(0)  # (1, emb_dim)\n        scores = torch.matmul(user_emb_single, item_emb.t()).squeeze()  # (n_items,)\n        scores_np = scores.cpu().numpy()\n        \n        # Positive itemsëŠ” ì œì™¸\n        for item in user_pos_items:\n            scores_np[int(item)] = -np.inf\n        \n        # ìœ íš¨í•œ itemë“¤ (positiveê°€ ì•„ë‹Œ ê²ƒë“¤)\n        valid_items = np.where(scores_np > -np.inf)[0]\n        \n        if len(valid_items) > 0:\n            # 30-70 percentile ë²”ìœ„ì˜ items ì„ íƒ\n            score_30 = np.percentile(scores_np[valid_items], 30)\n            score_70 = np.percentile(scores_np[valid_items], 70)\n            semi_hard_mask = (scores_np >= score_30) & (scores_np <= score_70)\n            semi_hard_candidates = np.where(semi_hard_mask)[0]\n            \n            if len(semi_hard_candidates) >= n_semi_hard:\n                semi_hard = np.random.choice(semi_hard_candidates, n_semi_hard, replace=False)\n            else:\n                # ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ valid itemsì—ì„œ ëœë¤ ìƒ˜í”Œë§\n                semi_hard = np.random.choice(valid_items, min(n_semi_hard, len(valid_items)), replace=False)\n        else:\n            semi_hard = []\n        \n        # 2. In-batch negatives (ë‚˜ë¨¸ì§€ ì ˆë°˜)\n        n_in_batch = neg_ratio - len(semi_hard)\n        \n        # ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ë“¤ì˜ positive items ìˆ˜ì§‘\n        other_users_items = []\n        for other_idx, other_user in enumerate(pos_users):\n            if other_idx != idx:  # ìê¸° ìì‹  ì œì™¸\n                other_items = user_items_dict[other_user]\n                # í˜„ì¬ ìœ ì €ì˜ positiveê°€ ì•„ë‹Œ ê²ƒë§Œ\n                other_users_items.extend([i for i in other_items if i not in user_pos_items])\n        \n        if len(other_users_items) >= n_in_batch:\n            in_batch = np.random.choice(other_users_items, n_in_batch, replace=False)\n        else:\n            # ë¶€ì¡±í•˜ë©´ ëœë¤ìœ¼ë¡œ ì±„ì›€\n            in_batch = []\n            if len(other_users_items) > 0:\n                in_batch = list(np.random.choice(other_users_items, min(n_in_batch, len(other_users_items)), replace=False))\n        \n        # 3. í•©ì¹˜ê¸°\n        hard_negs = list(semi_hard) + list(in_batch)\n        \n        # 4. ë¶€ì¡±í•˜ë©´ ì™„ì „ ëœë¤ìœ¼ë¡œ ì±„ì›€ (fallback)\n        while len(hard_negs) < neg_ratio:\n            rand_item = random.randint(0, n_items - 1)\n            if rand_item not in user_pos_items and rand_item not in hard_negs:\n                hard_negs.append(rand_item)\n        \n        hard_negs = hard_negs[:neg_ratio]\n        \n        for neg_item in hard_negs:\n            neg_users.append(user_id)\n            neg_items.append(neg_item)\n    \n    return np.array(neg_users), np.array(neg_items)\n\ntrain_user_items = create_user_item_dict(train_df)\ntrain_valid_user_items = create_user_item_dict(train_valid_df)\n\nprint(\"=\" * 60)\nprint(\"Hard Negative Mining í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\nprint(\"=\" * 60)\nprint(\"Random sampling vs Hard negative mining:\")\nprint(\"\\n[Random Sampling (V9cê¹Œì§€)]\")\nprint(\"  - ëª¨ë“  negativeë¥¼ ë™ì¼í•œ í™•ë¥ ë¡œ ìƒ˜í”Œë§\")\nprint(\"  - ëŒ€ë¶€ë¶„ì´ ë„ˆë¬´ ì‰¬ìš´ negatives (userì™€ ë¬´ê´€í•œ items)\")\nprint(\"  - í•™ìŠµ íš¨ìœ¨ ë‚®ìŒ\")\nprint(\"\\n[Hard Negative Mining (V11a)]\")\nprint(\"  1ï¸âƒ£ Semi-hard (50%): ëª¨ë¸ ì ìˆ˜ 30-70 percentile\")\nprint(\"     â†’ ì ë‹¹íˆ ì–´ë ¤ìš´ negativesë¡œ í•™ìŠµ íš¨ìœ¨ â†‘\")\nprint(\"  2ï¸âƒ£ In-batch (50%): ë‹¤ë¥¸ ìœ ì €ì˜ positive items\")\nprint(\"     â†’ Collaborative filtering ë³¸ì§ˆ í•™ìŠµ\")\nprint(\"\\nì˜ˆìƒ íš¨ê³¼: +4~8% Recall@10\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGCN ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# LightGCN ëª¨ë¸ (V6ì™€ ë™ì¼)\n",
    "class LightGCNConv(MessagePassing):\n",
    "    \"\"\"LightGCN Convolution Layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "    \n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    \"\"\"LightGCN for Recommendation\"\"\"\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.1)\n",
    "        \n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, edge_index):\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        all_emb = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            all_emb = conv(all_emb, edge_index)\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.stack(embs, dim=0).mean(dim=0)\n",
    "        \n",
    "        user_final = final_emb[:self.n_users]\n",
    "        item_final = final_emb[self.n_users:]\n",
    "        \n",
    "        return user_final, item_final\n",
    "    \n",
    "    def predict(self, users, items, edge_index):\n",
    "        user_emb, item_emb = self.forward(edge_index)\n",
    "        user_emb = user_emb[users]\n",
    "        item_emb = item_emb[items]\n",
    "        scores = (user_emb * item_emb).sum(dim=1)\n",
    "        return scores\n",
    "\n",
    "print(\"LightGCN ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InfoNCE Loss í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n",
      "\n",
      "ìˆ˜ì‹:\n",
      "  BPR:     loss = -log(Ïƒ(s_pos - s_neg)) for each pair\n",
      "  InfoNCE: loss = -log(exp(s_pos/Ï„) / Î£exp(s_all/Ï„))\n",
      "\n",
      "ì°¨ì´: BPRì€ ë…ë¦½ì  pair, InfoNCEëŠ” ëª¨ë“  negative ë™ì‹œ ê³ ë ¤\n"
     ]
    }
   ],
   "source": [
    "# â­ InfoNCE Loss (í•µì‹¬ ë³€ê²½!)\n",
    "def infonce_loss(pos_scores, neg_scores, neg_ratio=4, temperature=0.2):\n",
    "    \"\"\"\n",
    "    InfoNCE (Contrastive Learning) Loss\n",
    "    \n",
    "    loss = -log( exp(pos/Ï„) / (exp(pos/Ï„) + sum(exp(neg_i/Ï„))) )\n",
    "         = -log( exp(pos/Ï„) / sum(exp(all/Ï„)) )\n",
    "         = -pos/Ï„ + log(sum(exp(all/Ï„)))\n",
    "    \n",
    "    BPRê³¼ì˜ ì°¨ì´:\n",
    "    - BPR: pairwise comparison (pos vs neg_1, pos vs neg_2, ...)\n",
    "    - InfoNCE: multi-class classification (posë¥¼ [pos, neg_1, ..., neg_k] ì¤‘ì—ì„œ êµ¬ë¶„)\n",
    "    \n",
    "    ì¥ì :\n",
    "    1. ëª¨ë“  negativeë¥¼ ë™ì‹œì— í™œìš© â†’ ë” ê°•í•œ gradient\n",
    "    2. Temperatureë¡œ hard negativeì— ì§‘ì¤‘ ê°€ëŠ¥\n",
    "    3. Score distributionì´ ë” diverseí•´ì§ (collapse ë°©ì§€)\n",
    "    \"\"\"\n",
    "    batch_size = pos_scores.size(0)\n",
    "    \n",
    "    # Reshape: neg_scoresë¥¼ (batch_size, neg_ratio)ë¡œ\n",
    "    neg_scores = neg_scores.view(batch_size, neg_ratio)\n",
    "    \n",
    "    # pos_scoresë¥¼ (batch_size, 1)ë¡œ í™•ì¥\n",
    "    pos_scores = pos_scores.unsqueeze(1)\n",
    "    \n",
    "    # ëª¨ë“  scoresë¥¼ concatenate: (batch_size, 1 + neg_ratio)\n",
    "    all_scores = torch.cat([pos_scores, neg_scores], dim=1)\n",
    "    \n",
    "    # Temperature scaling\n",
    "    all_scores = all_scores / temperature\n",
    "    \n",
    "    # InfoNCE: -log(exp(pos) / sum(exp(all)))\n",
    "    # = -log_softmax(all_scores)[:, 0]\n",
    "    # positiveëŠ” í•­ìƒ index 0\n",
    "    log_prob = F.log_softmax(all_scores, dim=1)\n",
    "    loss = -log_prob[:, 0].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(\"InfoNCE Loss í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"\\nìˆ˜ì‹:\")\n",
    "print(\"  BPR:     loss = -log(Ïƒ(s_pos - s_neg)) for each pair\")\n",
    "print(\"  InfoNCE: loss = -log(exp(s_pos/Ï„) / Î£exp(s_all/Ï„))\")\n",
    "print(\"\\nì°¨ì´: BPRì€ ë…ë¦½ì  pair, InfoNCEëŠ” ëª¨ë“  negative ë™ì‹œ ê³ ë ¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€ í•¨ìˆ˜ (V6ì™€ ë™ì¼)\n",
    "def evaluate_model(model, edge_index, eval_df, user_items_dict, n_items, k=10, device='cpu'):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€: Precision@K, Recall@K, NDCG@K\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        user_emb, item_emb = model(edge_index.to(device))\n",
    "        \n",
    "        precisions, recalls, ndcgs = [], [], []\n",
    "        \n",
    "        for user_id, group in eval_df.groupby('user_id'):\n",
    "            true_items = set(group['item_id'].values)\n",
    "            exclude_items = user_items_dict[user_id]\n",
    "            \n",
    "            user_emb_single = user_emb[user_id].unsqueeze(0)\n",
    "            scores = torch.matmul(user_emb_single, item_emb.t()).squeeze()\n",
    "            \n",
    "            scores_np = scores.cpu().numpy()\n",
    "            for item_id in exclude_items:\n",
    "                scores_np[int(item_id)] = -np.inf\n",
    "            \n",
    "            top_k_items = np.argsort(scores_np)[-k:][::-1]\n",
    "            \n",
    "            hits = len(set(top_k_items) & true_items)\n",
    "            \n",
    "            precision = hits / k\n",
    "            recall = hits / len(true_items) if len(true_items) > 0 else 0\n",
    "            \n",
    "            dcg = sum([1 / np.log2(i + 2) for i, item in enumerate(top_k_items) if item in true_items])\n",
    "            idcg = sum([1 / np.log2(i + 2) for i in range(min(len(true_items), k))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            ndcgs.append(ndcg)\n",
    "    \n",
    "    return {\n",
    "        f'precision@{k}': np.mean(precisions),\n",
    "        f'recall@{k}': np.mean(recalls),\n",
    "        f'ndcg@{k}': np.mean(ndcgs),\n",
    "    }\n",
    "\n",
    "print(\"í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training í•¨ìˆ˜ (Hard Negative Mining ì ìš©)\ndef train_one_epoch(model, edge_index, train_df, user_items_dict, n_items, \n                    optimizer, batch_size, neg_ratio, temperature, device):\n    \"\"\"1 epoch training with InfoNCE + Hard Negative Mining\"\"\"\n    model.train()\n    \n    train_df_shuffled = train_df.sample(frac=1).reset_index(drop=True)\n    \n    total_loss = 0\n    n_batches = 0\n    \n    for start_idx in range(0, len(train_df_shuffled), batch_size):\n        end_idx = min(start_idx + batch_size, len(train_df_shuffled))\n        batch_df = train_df_shuffled.iloc[start_idx:end_idx]\n        \n        pos_users = torch.tensor(batch_df['user_id'].values, dtype=torch.long).to(device)\n        pos_items = torch.tensor(batch_df['item_id'].values, dtype=torch.long).to(device)\n        \n        # â­ Hard Negative Mining ì‚¬ìš©!\n        neg_users_np, neg_items_np = hard_negative_sampling(\n            model, edge_index, batch_df, user_items_dict, n_items, device, neg_ratio\n        )\n        neg_users = torch.tensor(neg_users_np, dtype=torch.long).to(device)\n        neg_items = torch.tensor(neg_items_np, dtype=torch.long).to(device)\n        \n        pos_scores = model.predict(pos_users, pos_items, edge_index.to(device))\n        neg_scores = model.predict(neg_users, neg_items, edge_index.to(device))\n        \n        # InfoNCE Loss ì‚¬ìš©\n        loss = infonce_loss(pos_scores, neg_scores, neg_ratio, temperature)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        n_batches += 1\n    \n    return total_loss / n_batches\n\nprint(\"Training í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ! (InfoNCE + Hard Negative Mining)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë©”ì¸ Training Loop (V11A - Hard Negative Mining)\nmodel = LightGCN(\n    n_users=n_users,\n    n_items=n_items,\n    embedding_dim=CONFIG['embedding_dim'],\n    n_layers=CONFIG['n_layers']\n).to(CONFIG['device'])\n\noptimizer = torch.optim.Adam(\n    model.parameters(), \n    lr=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay']\n)\n\nhistory = {\n    'train_loss': [],\n    'valid_precision': [],\n    'valid_recall': [],\n    'valid_ndcg': [],\n}\n\nbest_recall = 0\npatience_counter = 0\n\nprint(\"=\" * 60)\nprint(\"Training ì‹œì‘ (V11A - Hard Negative Mining)\")\nprint(\"=\" * 60)\nprint(f\"Model: LightGCN\")\nprint(f\"  - Users: {n_users}, Items: {n_items}\")\nprint(f\"  - Embedding dim: {CONFIG['embedding_dim']}\")\nprint(f\"  - Layers: {CONFIG['n_layers']}\")\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"  - Total params: {total_params:,}\")\nprint(f\"\\nğŸ”¥ í•µì‹¬ ë³€ê²½: Random â†’ Hard Negative Mining\")\nprint(f\"  V9c: Random sampling (ì‰¬ìš´ negatives)\")\nprint(f\"  V11a: Semi-hard (50%) + In-batch (50%)\")\nprint(f\"  â†’ 4~8% Recall ê°œì„  ì˜ˆìƒ\")\nprint(f\"\\nLoss Function: InfoNCE (temperature={CONFIG['temperature']})\")\nprint(f\"Device: {CONFIG['device']}\")\nprint(f\"Learning rate: {CONFIG['learning_rate']}\")\nprint(f\"Negative ratio: {CONFIG['neg_ratio']}\")\nprint(\"=\" * 60)\n\nfor epoch in range(CONFIG['epochs']):\n    train_loss = train_one_epoch(\n        model, train_edge_index, train_df, train_user_items,\n        n_items, optimizer, CONFIG['batch_size'], \n        CONFIG['neg_ratio'], CONFIG['temperature'], CONFIG['device']\n    )\n    \n    val_metrics = evaluate_model(\n        model, train_edge_index, valid_df, train_user_items,\n        n_items, k=CONFIG['top_k'], device=CONFIG['device']\n    )\n    \n    history['train_loss'].append(train_loss)\n    history['valid_precision'].append(val_metrics[f'precision@{CONFIG[\"top_k\"]}'])\n    history['valid_recall'].append(val_metrics[f'recall@{CONFIG[\"top_k\"]}'])\n    history['valid_ndcg'].append(val_metrics[f'ndcg@{CONFIG[\"top_k\"]}'])\n    \n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        print(f\"Epoch {epoch+1:3d}/{CONFIG['epochs']} | \"\n              f\"Loss: {train_loss:.4f} | \"\n              f\"P@{CONFIG['top_k']}: {val_metrics[f'precision@{CONFIG[\"top_k\"]}']:.4f} | \"\n              f\"R@{CONFIG['top_k']}: {val_metrics[f'recall@{CONFIG[\"top_k\"]}']:.4f} | \"\n              f\"NDCG@{CONFIG['top_k']}: {val_metrics[f'ndcg@{CONFIG[\"top_k\"]}']:.4f}\")\n    \n    current_recall = val_metrics[f'recall@{CONFIG[\"top_k\"]}']\n    if current_recall > best_recall:\n        best_recall = current_recall\n        patience_counter = 0\n        torch.save(model.state_dict(), \n                   os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_best.pth'))\n    else:\n        patience_counter += 1\n    \n    if patience_counter >= CONFIG['patience']:\n        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n        break\n\nprint(\"=\" * 60)\nprint(f\"Training ì™„ë£Œ! (V11A - Hard Negative Mining)\")\nprint(f\"Best Recall@{CONFIG['top_k']}: {best_recall:.4f}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training ê²°ê³¼ ì‹œê°í™”\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\naxes[0].plot(history['train_loss'], label='Train Loss (InfoNCE + Hard Neg)', linewidth=2, color='#e74c3c')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss Curve (V11A - Hard Negative Mining)')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(history['valid_precision'], label=f'Precision@{CONFIG[\"top_k\"]}', linewidth=2)\naxes[1].plot(history['valid_recall'], label=f'Recall@{CONFIG[\"top_k\"]}', linewidth=2)\naxes[1].plot(history['valid_ndcg'], label=f'NDCG@{CONFIG[\"top_k\"]}', linewidth=2)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Score')\naxes[1].set_title('Validation Metrics (V11A)')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(CONFIG['result_dir'], 'training_curves_v11a.png'), dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Set í‰ê°€\nmodel.load_state_dict(torch.load(os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_best.pth')))\n\nprint(\"=\" * 60)\nprint(\"Test Set í‰ê°€ (V11A - Hard Negative Mining)\")\nprint(\"=\" * 60)\n\nv11a_results = {}\n\nfor k in [5, 10, 20]:\n    test_metrics = evaluate_model(\n        model, \n        train_valid_edge_index,\n        test_df, \n        train_valid_user_items,\n        n_items, \n        k=k, \n        device=CONFIG['device']\n    )\n    \n    v11a_results[k] = test_metrics\n    \n    print(f\"\\nTop-{k} ì¶”ì²œ:\")\n    print(f\"  Precision@{k}: {test_metrics[f'precision@{k}']:.4f}\")\n    print(f\"  Recall@{k}:    {test_metrics[f'recall@{k}']:.4f}\")\n    print(f\"  NDCG@{k}:      {test_metrics[f'ndcg@{k}']:.4f}\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8a vs V6 ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"V11A vs V9C ê²°ê³¼ ë¹„êµ\")\nprint(\"=\" * 60)\n\n# V9c ê²°ê³¼ (baseline)\nv9c_results = {\n    10: {'precision@10': 0.2756, 'recall@10': 0.1578, 'ndcg@10': 0.3129}\n}\n\nprint(\"\\ní•µì‹¬ ë³€ê²½ì‚¬í•­:\")\nprint(f\"  V9c: Random negative sampling\")\nprint(f\"  V11a: Hard negative mining (Semi-hard 50% + In-batch 50%)\")\n\nprint(\"\\nTest Set ì„±ëŠ¥ (Recall@10):\")\nprint(f\"  V9c:  {v9c_results[10]['recall@10']:.4f} (15.78%)\")\nprint(f\"  V11a: {v11a_results[10]['recall@10']:.4f} ({v11a_results[10]['recall@10']*100:.2f}%)\")\n\nimprovement = (v11a_results[10]['recall@10'] - v9c_results[10]['recall@10']) / v9c_results[10]['recall@10'] * 100\nprint(f\"  ë³€í™”: {improvement:+.1f}%\")\n\nprint(\"\\nì „ì²´ ì§€í‘œ ë¹„êµ (Top-10):\")\nfor metric in ['precision@10', 'recall@10', 'ndcg@10']:\n    v9c_val = v9c_results[10][metric]\n    v11a_val = v11a_results[10][metric]\n    change = (v11a_val - v9c_val) / v9c_val * 100\n    symbol = 'âœ…' if change > 0 else 'âŒ'\n    print(f\"  {metric:15s}: V9c={v9c_val:.4f}, V11a={v11a_val:.4f} ({change:+.1f}%) {symbol}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ì‹¤í—˜ ê²°ê³¼ í•´ì„:\")\nif improvement > 5:\n    print(\"ğŸ‰ BREAKTHROUGH ì„±ê³µ!\")\n    print(\"   Hard negative miningì´ í•µì‹¬ bottleneckì´ì—ˆìŒ\")\n    print(\"   â†’ Random samplingì´ ë„ˆë¬´ ì‰¬ìš´ negativesë§Œ ì œê³µ\")\n    print(\"   â†’ Semi-hard + In-batchë¡œ í•™ìŠµ íš¨ìœ¨ ëŒ€í­ í–¥ìƒ\")\n    print(\"\\në‹¤ìŒ ë‹¨ê³„:\")\n    print(\"   - V10a (embedding capacity)ì™€ ë¹„êµ\")\n    print(\"   - ìµœì  ëª¨ë¸ ì„ íƒ í›„ ìµœì¢… í‰ê°€\")\nelif improvement > 2:\n    print(\"âœ… ì†Œí­ ê°œì„ . Hard negative mining íš¨ê³¼ ìˆìŒ\")\n    print(\"   â†’ V10aì™€ ë¹„êµ í›„ ìµœì¢… ê²°ì •\")\nelif improvement > -2:\n    print(\"â– ë¹„ìŠ·í•œ ìˆ˜ì¤€. Hard negative íš¨ê³¼ ë¯¸ë¯¸\")\n    print(\"   â†’ V10a (embedding capacity) ì‹œë„\")\nelse:\n    print(\"âŒ ì„±ëŠ¥ í•˜ë½. ì´ ë°ì´í„°ì…‹ì—ì„œëŠ” randomì´ ë” ë‚˜ìŒ\")\n    print(\"   â†’ V10aë¡œ ë‹¤ë¥¸ ë°©í–¥ íƒìƒ‰\")\nprint(\"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}