{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN ê¸°ë°˜ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ\n",
    "# V11A: Hard Negative Mining\n",
    "# Step 2 - Breakthrough experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport pickle\nimport random\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import MessagePassing\nfrom torch_geometric.utils import degree\n\nfrom tqdm.notebook import tqdm\n\n# ì‹œê°í™” ì„¤ì •\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\nprint(\"=\" * 60)\nprint(\"í™˜ê²½ ì„¤ì •\")\nprint(\"=\" * 60)\nprint(f\"PyTorch ë²„ì „: {torch.__version__}\")\nprint(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\nprint(f\"MPS ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Google Colab ê°ì§€ ë° ê²½ë¡œ ì„¤ì •\ntry:\n    import google.colab\n    IN_COLAB = True\n    print(\"ğŸŒ Google Colab í™˜ê²½ ê°ì§€ë¨\")\nexcept ImportError:\n    IN_COLAB = False\n    print(\"ğŸ’» ë¡œì»¬ í™˜ê²½\")\n\n# Google Drive ë§ˆìš´íŠ¸ (Colab only)\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    BASE_PATH = '/content/drive/MyDrive/gnn-recsys'\n    print(f\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")\n    print(f\"   Base path: {BASE_PATH}\")\nelse:\n    # ë¡œì»¬ í™˜ê²½: notebooks í´ë”ì—ì„œ ì‹¤í–‰ëœë‹¤ê³  ê°€ì •\n    BASE_PATH = '..'\n    print(f\"   Base path: {BASE_PATH}\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (V11A - Hard Negative Mining)\nCONFIG = {\n    # Filtering ê¸°ì¤€ (V6ì™€ ë™ì¼)\n    'min_user_interactions': 30,\n    'min_item_interactions': 10,\n    'rating_threshold': None,\n    \n    # Split ë¹„ìœ¨ (V6ì™€ ë™ì¼)\n    'train_ratio': 0.70,\n    'valid_ratio': 0.15,\n    'test_ratio': 0.15,\n    \n    # ëª¨ë¸ íŒŒë¼ë¯¸í„° (V9cì™€ ë™ì¼)\n    'embedding_dim': 64,\n    'n_layers': 2,\n    \n    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n    'learning_rate': 0.001,\n    'weight_decay': 1e-4,\n    'batch_size': 512,\n    'epochs': 100,\n    'patience': 20,\n    'neg_ratio': 6,\n    \n    # InfoNCE íŒŒë¼ë¯¸í„°\n    'temperature': 0.2,\n    \n    # â­ Hard Negative Mining íŒŒë¼ë¯¸í„°\n    'hard_neg_ratio': 0.5,  # 50% hard negatives, 50% random\n    \n    # í‰ê°€\n    'top_k': 10,\n    \n    # ì‹œìŠ¤í…œ\n    'device': 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'),\n    'seed': 42,\n    \n    # ê²½ë¡œ (BASE_PATH ê¸°ë°˜)\n    'data_dir': os.path.join(BASE_PATH, 'data'),\n    'processed_dir': os.path.join(BASE_PATH, 'data', 'processed'),\n    'model_dir': os.path.join(BASE_PATH, 'models'),\n    'result_dir': os.path.join(BASE_PATH, 'results'),\n}\n\n# í´ë” ìƒì„±\nfor dir_path in [CONFIG['processed_dir'], CONFIG['model_dir'], CONFIG['result_dir']]:\n    os.makedirs(dir_path, exist_ok=True)\n\n# Random seed ê³ ì •\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    if torch.backends.mps.is_available():\n        torch.mps.manual_seed(seed)\n\nset_seed(CONFIG['seed'])\n\nprint(\"=\" * 60)\nprint(\"V11A ì„¤ì • ì™„ë£Œ! (Hard Negative Mining)\")\nprint(\"=\" * 60)\nprint(f\"Device: {CONFIG['device']}\")\nprint(f\"ë°ì´í„° ê²½ë¡œ: {CONFIG['processed_dir']}\")\nprint(\"\\nğŸ”¥ í•µì‹¬ ë³€ê²½: Random Negative â†’ Hard Negative Mining\")\nprint(f\"  V9cê¹Œì§€: ì™„ì „ ëœë¤ ìƒ˜í”Œë§ (ë„ˆë¬´ ì‰¬ìš´ negative)\")\nprint(f\"  V11a: ëª¨ë¸ ê¸°ë°˜ hard negative ì„ íƒ\")\nprint(f\"\\nì „ëµ:\")\nprint(f\"  1. Semi-hard negatives (50%): ëª¨ë¸ ì ìˆ˜ ì¤‘ê°„ ë²”ìœ„ [0.3-0.7]\")\nprint(f\"     â†’ ë„ˆë¬´ ì‰½ì§€ë„, ë„ˆë¬´ ì–´ë µì§€ë„ ì•Šì€ negatives\")\nprint(f\"  2. In-batch negatives (50%): ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ì˜ positive items\")\nprint(f\"     â†’ ë‹¤ë¥¸ ìœ ì €ëŠ” ì¢‹ì•„í•˜ì§€ë§Œ ì´ ìœ ì €ëŠ” ì‹«ì–´í•˜ëŠ” items\")\nprint(f\"\\nê¸°ëŒ€ íš¨ê³¼:\")\nprint(f\"  - ëœë¤ë³´ë‹¤ í•™ìŠµì´ ì–´ë ¤ì›Œì ¸ì„œ ë” ê°•í•œ discriminative power\")\nprint(f\"  - Embedding spaceê°€ ë” êµ¬ì¡°í™”ë¨\")\nprint(f\"  - ì˜ˆìƒ ê°œì„ : +4~8% Recall@10\")\nprint(f\"\\nê¸°íƒ€ ì„¤ì • (V9cì™€ ë™ì¼):\")\nprint(f\"  Loss: InfoNCE (temperature={CONFIG['temperature']})\")\nprint(f\"  Layers: {CONFIG['n_layers']}\")\nprint(f\"  Embedding dim: {CONFIG['embedding_dim']}\")\nprint(f\"  Negative ratio: {CONFIG['neg_ratio']}\")\nprint(\"=\" * 60)\nprint(\"\\nğŸ¯ ëª©í‘œ: Recall@10 > 18% (V9cì˜ 15.78% ëŒ€ë¹„ breakthrough!)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (V11A - Hard Negative Mining)\n",
    "CONFIG = {\n",
    "    # Filtering ê¸°ì¤€ (V6ì™€ ë™ì¼)\n",
    "    'min_user_interactions': 30,\n",
    "    'min_item_interactions': 10,\n",
    "    'rating_threshold': None,\n",
    "    \n",
    "    # Split ë¹„ìœ¨ (V6ì™€ ë™ì¼)\n",
    "    'train_ratio': 0.70,\n",
    "    'valid_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    \n",
    "    # ëª¨ë¸ íŒŒë¼ë¯¸í„° (V9cì™€ ë™ì¼)\n",
    "    'embedding_dim': 64,\n",
    "    'n_layers': 2,\n",
    "    \n",
    "    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'batch_size': 512,\n",
    "    'epochs': 100,\n",
    "    'patience': 20,\n",
    "    'neg_ratio': 6,\n",
    "    \n",
    "    # InfoNCE íŒŒë¼ë¯¸í„°\n",
    "    'temperature': 0.2,\n",
    "    \n",
    "    # â­ Hard Negative Mining íŒŒë¼ë¯¸í„°\n",
    "    'hard_neg_ratio': 0.5,  # 50% hard negatives, 50% random\n",
    "    \n",
    "    # í‰ê°€\n",
    "    'top_k': 10,\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ\n",
    "    'device': 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'),\n",
    "    'seed': 42,\n",
    "    \n",
    "    # ê²½ë¡œ\n",
    "    'data_dir': str(DATA_ROOT),\n",
    "    'processed_dir': str(PROCESSED_ROOT),\n",
    "    'model_dir': str(MODEL_ROOT),\n",
    "    'result_dir': str(RESULT_ROOT),\n",
    "}\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "for dir_path in [CONFIG['processed_dir'], CONFIG['model_dir'], CONFIG['result_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Random seed ê³ ì •\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V11A ì„¤ì • ì™„ë£Œ! (Hard Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(\"\\nğŸ”¥ í•µì‹¬ ë³€ê²½: Random Negative â†’ Hard Negative Mining\")\n",
    "print(f\"  V9cê¹Œì§€: ì™„ì „ ëœë¤ ìƒ˜í”Œë§ (ë„ˆë¬´ ì‰¬ìš´ negative)\")\n",
    "print(f\"  V11a: ëª¨ë¸ ê¸°ë°˜ hard negative ì„ íƒ\")\n",
    "print(f\"\\nì „ëµ:\")\n",
    "print(f\"  1. Semi-hard negatives (50%): ëª¨ë¸ ì ìˆ˜ ì¤‘ê°„ ë²”ìœ„ [0.3-0.7]\")\n",
    "print(f\"     â†’ ë„ˆë¬´ ì‰½ì§€ë„, ë„ˆë¬´ ì–´ë µì§€ë„ ì•Šì€ negatives\")\n",
    "print(f\"  2. In-batch negatives (50%): ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ì˜ positive items\")\n",
    "print(f\"     â†’ ë‹¤ë¥¸ ìœ ì €ëŠ” ì¢‹ì•„í•˜ì§€ë§Œ ì´ ìœ ì €ëŠ” ì‹«ì–´í•˜ëŠ” items\")\n",
    "print(f\"\\nê¸°ëŒ€ íš¨ê³¼:\")\n",
    "print(f\"  - ëœë¤ë³´ë‹¤ í•™ìŠµì´ ì–´ë ¤ì›Œì ¸ì„œ ë” ê°•í•œ discriminative power\")\n",
    "print(f\"  - Embedding spaceê°€ ë” êµ¬ì¡°í™”ë¨\")\n",
    "print(f\"  - ì˜ˆìƒ ê°œì„ : +4~8% Recall@10\")\n",
    "print(f\"\\nê¸°íƒ€ ì„¤ì • (V9cì™€ ë™ì¼):\")\n",
    "print(f\"  Loss: InfoNCE (temperature={CONFIG['temperature']})\")\n",
    "print(f\"  Layers: {CONFIG['n_layers']}\")\n",
    "print(f\"  Embedding dim: {CONFIG['embedding_dim']}\")\n",
    "print(f\"  Negative ratio: {CONFIG['neg_ratio']}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ¯ ëª©í‘œ: Recall@10 > 18% (V9cì˜ 15.78% ëŒ€ë¹„ breakthrough!)\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# V6ì—ì„œ ì „ì²˜ë¦¬í•œ ë°ì´í„° ë¡œë“œ\nprint(\"=\" * 60)\nprint(\"V6 ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ\")\nprint(\"=\" * 60)\n\ntrain_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'train_split_v6.csv'))\nvalid_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'valid_split_v6.csv'))\ntest_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'test_split_v6.csv'))\n\nwith open(os.path.join(CONFIG['processed_dir'], 'id_mappings_v6.pkl'), 'rb') as f:\n    mappings = pickle.load(f)\n\nn_users = len(mappings['user_id_map'])\nn_items = len(mappings['item_id_map'])\n\nprint(f\"\\në°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\nprint(f\"  Users: {n_users}\")\nprint(f\"  Items: {n_items}\")\nprint(f\"  Train: {len(train_df):,}\")\nprint(f\"  Valid: {len(valid_df):,}\")\nprint(f\"  Test:  {len(test_df):,}\")\nprint(\"\\nâœ… V6ì™€ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš© â†’ ê³µì •í•œ ë¹„êµ ê°€ëŠ¥\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Mount (Colab only)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print('âœ… Google Drive mounted')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('âš ï¸  Local mode') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "V6 ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/gnn-recsys/data/processed/train_split_v6.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3708602960.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# test_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'test_split_v6.csv'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/gnn-recsys/data/processed/train_split_v6.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/gnn-recsys/data/processed/valid_split_v6.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/gnn-recsys/data/processed/test_split_v6.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/gnn-recsys/data/processed/train_split_v6.csv'"
     ]
    }
   ],
   "source": [
    "# V6ì—ì„œ ì „ì²˜ë¦¬í•œ ë°ì´í„° ë¡œë“œ\n",
    "print(\"=\" * 60)\n",
    "print(\"V6 ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# train_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'train_split_v6.csv'))\n",
    "# valid_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'valid_split_v6.csv'))\n",
    "# test_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'test_split_v6.csv'))\n",
    "\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/gnn-recsys/data/processed/train_split_v6.csv')\n",
    "valid_df = pd.read_csv('/content/drive/MyDrive/gnn-recsys/data/processed/valid_split_v6.csv')\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/gnn-recsys/data/processed/test_split_v6.csv')\n",
    "\n",
    "with open(os.path.join(CONFIG['processed_dir'], 'id_mappings_v6.pkl'), 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "n_users = len(mappings['user_id_map'])\n",
    "n_items = len(mappings['item_id_map'])\n",
    "\n",
    "print(f\"\\në°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"  Users: {n_users}\")\n",
    "print(f\"  Items: {n_items}\")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Valid: {len(valid_df):,}\")\n",
    "print(f\"  Test:  {len(test_df):,}\")\n",
    "print(\"\\nâœ… V6ì™€ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš© â†’ ê³µì •í•œ ë¹„êµ ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph êµ¬ì„±\n",
    "print(\"=\" * 60)\n",
    "print(\"Graph êµ¬ì„±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_graph(df, n_users, n_items):\n",
    "    \"\"\"User-Item Bipartite Graph ìƒì„±\"\"\"\n",
    "    user_ids = df['user_id'].values\n",
    "    item_ids = df['item_id'].values + n_users\n",
    "    \n",
    "    edge_index = torch.tensor([\n",
    "        np.concatenate([user_ids, item_ids]),\n",
    "        np.concatenate([item_ids, user_ids])\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    print(f\"Node ìˆ˜: {n_users + n_items} (User: {n_users}, Item: {n_items})\")\n",
    "    print(f\"Edge ìˆ˜: {edge_index.shape[1]:,} (ì–‘ë°©í–¥)\")\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "print(\"\\n[1] Train Graph (í•™ìŠµìš©)\")\n",
    "train_edge_index = create_graph(train_df, n_users, n_items)\n",
    "\n",
    "print(\"\\n[2] Train+Valid Graph (Test í‰ê°€ìš©)\")\n",
    "train_valid_df = pd.concat([train_df, valid_df])\n",
    "train_valid_edge_index = create_graph(train_valid_df, n_users, n_items)\n",
    "\n",
    "print(\"\\nâœ… Graph ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â­ Hard Negative Mining (í•µì‹¬ ë³€ê²½!)\n",
    "def create_user_item_dict(df):\n",
    "    \"\"\"Userë³„ë¡œ ìƒí˜¸ì‘ìš©í•œ item ì§‘í•© ìƒì„±\"\"\"\n",
    "    user_items = defaultdict(set)\n",
    "    for _, row in df.iterrows():\n",
    "        user_items[row['user_id']].add(row['item_id'])\n",
    "    return user_items\n",
    "\n",
    "def hard_negative_sampling(model, edge_index, batch_df, user_items_dict, n_items, device, neg_ratio=6):\n",
    "    \"\"\"\n",
    "    Hard Negative Mining\n",
    "    \n",
    "    ì „ëµ:\n",
    "    1. Semi-hard negatives (50%): ëª¨ë¸ ì ìˆ˜ ì¤‘ê°„ ë²”ìœ„ [30-70 percentile]\n",
    "       â†’ ë„ˆë¬´ ì‰½ì§€ë„ (ì ìˆ˜ ë‚®ìŒ), ë„ˆë¬´ ì–´ë µì§€ë„ (ì ìˆ˜ ë†’ìŒ) ì•Šì€ negatives\n",
    "       â†’ í•™ìŠµ íš¨ìœ¨ ìµœëŒ€í™”\n",
    "    \n",
    "    2. In-batch negatives (50%): ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ë“¤ì˜ positive items\n",
    "       â†’ ë‹¤ë¥¸ ìœ ì €ëŠ” ì¢‹ì•„í•˜ì§€ë§Œ ì´ ìœ ì €ì—ê²ŒëŠ” negative\n",
    "       â†’ Collaborative filteringì˜ ë³¸ì§ˆ í•™ìŠµ\n",
    "    \n",
    "    Random sampling ëŒ€ë¹„ ì¥ì :\n",
    "    - Randomì€ ëŒ€ë¶€ë¶„ ë„ˆë¬´ ì‰¬ìš´ negatives (userì™€ ê´€ë ¨ ì—†ëŠ” items)\n",
    "    - Hard negativesëŠ” meaningfulí•œ ì°¨ì´ í•™ìŠµ\n",
    "    - Embedding spaceê°€ ë” êµ¬ì¡°í™”ë¨\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_emb, item_emb = model(edge_index.to(device))\n",
    "    model.train()\n",
    "    \n",
    "    pos_users = batch_df['user_id'].values\n",
    "    neg_users = []\n",
    "    neg_items = []\n",
    "    \n",
    "    # ê° ìœ ì €ë³„ë¡œ hard negatives ìƒì„±\n",
    "    for idx, user_id in enumerate(pos_users):\n",
    "        user_pos_items = user_items_dict[user_id]\n",
    "        \n",
    "        # 1. Semi-hard negatives (ì ˆë°˜)\n",
    "        n_semi_hard = neg_ratio // 2\n",
    "        \n",
    "        # í˜„ì¬ ìœ ì €ì˜ ëª¨ë“  itemì— ëŒ€í•œ ì ìˆ˜ ê³„ì‚°\n",
    "        user_emb_single = user_emb[user_id].unsqueeze(0)  # (1, emb_dim)\n",
    "        scores = torch.matmul(user_emb_single, item_emb.t()).squeeze()  # (n_items,)\n",
    "        scores_np = scores.cpu().numpy()\n",
    "        \n",
    "        # Positive itemsëŠ” ì œì™¸\n",
    "        for item in user_pos_items:\n",
    "            scores_np[int(item)] = -np.inf\n",
    "        \n",
    "        # ìœ íš¨í•œ itemë“¤ (positiveê°€ ì•„ë‹Œ ê²ƒë“¤)\n",
    "        valid_items = np.where(scores_np > -np.inf)[0]\n",
    "        \n",
    "        if len(valid_items) > 0:\n",
    "            # 30-70 percentile ë²”ìœ„ì˜ items ì„ íƒ\n",
    "            score_30 = np.percentile(scores_np[valid_items], 30)\n",
    "            score_70 = np.percentile(scores_np[valid_items], 70)\n",
    "            semi_hard_mask = (scores_np >= score_30) & (scores_np <= score_70)\n",
    "            semi_hard_candidates = np.where(semi_hard_mask)[0]\n",
    "            \n",
    "            if len(semi_hard_candidates) >= n_semi_hard:\n",
    "                semi_hard = np.random.choice(semi_hard_candidates, n_semi_hard, replace=False)\n",
    "            else:\n",
    "                # ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ valid itemsì—ì„œ ëœë¤ ìƒ˜í”Œë§\n",
    "                semi_hard = np.random.choice(valid_items, min(n_semi_hard, len(valid_items)), replace=False)\n",
    "        else:\n",
    "            semi_hard = []\n",
    "        \n",
    "        # 2. In-batch negatives (ë‚˜ë¨¸ì§€ ì ˆë°˜)\n",
    "        n_in_batch = neg_ratio - len(semi_hard)\n",
    "        \n",
    "        # ê°™ì€ ë°°ì¹˜ì˜ ë‹¤ë¥¸ ìœ ì €ë“¤ì˜ positive items ìˆ˜ì§‘\n",
    "        other_users_items = []\n",
    "        for other_idx, other_user in enumerate(pos_users):\n",
    "            if other_idx != idx:  # ìê¸° ìì‹  ì œì™¸\n",
    "                other_items = user_items_dict[other_user]\n",
    "                # í˜„ì¬ ìœ ì €ì˜ positiveê°€ ì•„ë‹Œ ê²ƒë§Œ\n",
    "                other_users_items.extend([i for i in other_items if i not in user_pos_items])\n",
    "        \n",
    "        if len(other_users_items) >= n_in_batch:\n",
    "            in_batch = np.random.choice(other_users_items, n_in_batch, replace=False)\n",
    "        else:\n",
    "            # ë¶€ì¡±í•˜ë©´ ëœë¤ìœ¼ë¡œ ì±„ì›€\n",
    "            in_batch = []\n",
    "            if len(other_users_items) > 0:\n",
    "                in_batch = list(np.random.choice(other_users_items, min(n_in_batch, len(other_users_items)), replace=False))\n",
    "        \n",
    "        # 3. í•©ì¹˜ê¸°\n",
    "        hard_negs = list(semi_hard) + list(in_batch)\n",
    "        \n",
    "        # 4. ë¶€ì¡±í•˜ë©´ ì™„ì „ ëœë¤ìœ¼ë¡œ ì±„ì›€ (fallback)\n",
    "        while len(hard_negs) < neg_ratio:\n",
    "            rand_item = random.randint(0, n_items - 1)\n",
    "            if rand_item not in user_pos_items and rand_item not in hard_negs:\n",
    "                hard_negs.append(rand_item)\n",
    "        \n",
    "        hard_negs = hard_negs[:neg_ratio]\n",
    "        \n",
    "        for neg_item in hard_negs:\n",
    "            neg_users.append(user_id)\n",
    "            neg_items.append(neg_item)\n",
    "    \n",
    "    return np.array(neg_users), np.array(neg_items)\n",
    "\n",
    "train_user_items = create_user_item_dict(train_df)\n",
    "train_valid_user_items = create_user_item_dict(train_valid_df)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Hard Negative Mining í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Random sampling vs Hard negative mining:\")\n",
    "print(\"\\n[Random Sampling (V9cê¹Œì§€)]\")\n",
    "print(\"  - ëª¨ë“  negativeë¥¼ ë™ì¼í•œ í™•ë¥ ë¡œ ìƒ˜í”Œë§\")\n",
    "print(\"  - ëŒ€ë¶€ë¶„ì´ ë„ˆë¬´ ì‰¬ìš´ negatives (userì™€ ë¬´ê´€í•œ items)\")\n",
    "print(\"  - í•™ìŠµ íš¨ìœ¨ ë‚®ìŒ\")\n",
    "print(\"\\n[Hard Negative Mining (V11a)]\")\n",
    "print(\"  1ï¸âƒ£ Semi-hard (50%): ëª¨ë¸ ì ìˆ˜ 30-70 percentile\")\n",
    "print(\"     â†’ ì ë‹¹íˆ ì–´ë ¤ìš´ negativesë¡œ í•™ìŠµ íš¨ìœ¨ â†‘\")\n",
    "print(\"  2ï¸âƒ£ In-batch (50%): ë‹¤ë¥¸ ìœ ì €ì˜ positive items\")\n",
    "print(\"     â†’ Collaborative filtering ë³¸ì§ˆ í•™ìŠµ\")\n",
    "print(\"\\nì˜ˆìƒ íš¨ê³¼: +4~8% Recall@10\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGCN ëª¨ë¸ (V6ì™€ ë™ì¼)\n",
    "class LightGCNConv(MessagePassing):\n",
    "    \"\"\"LightGCN Convolution Layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "    \n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    \"\"\"LightGCN for Recommendation\"\"\"\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.1)\n",
    "        \n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, edge_index):\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        all_emb = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            all_emb = conv(all_emb, edge_index)\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.stack(embs, dim=0).mean(dim=0)\n",
    "        \n",
    "        user_final = final_emb[:self.n_users]\n",
    "        item_final = final_emb[self.n_users:]\n",
    "        \n",
    "        return user_final, item_final\n",
    "    \n",
    "    def predict(self, users, items, edge_index):\n",
    "        user_emb, item_emb = self.forward(edge_index)\n",
    "        user_emb = user_emb[users]\n",
    "        item_emb = item_emb[items]\n",
    "        scores = (user_emb * item_emb).sum(dim=1)\n",
    "        return scores\n",
    "\n",
    "print(\"LightGCN ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â­ InfoNCE Loss (í•µì‹¬ ë³€ê²½!)\n",
    "def infonce_loss(pos_scores, neg_scores, neg_ratio=4, temperature=0.2):\n",
    "    \"\"\"\n",
    "    InfoNCE (Contrastive Learning) Loss\n",
    "    \n",
    "    loss = -log( exp(pos/Ï„) / (exp(pos/Ï„) + sum(exp(neg_i/Ï„))) )\n",
    "         = -log( exp(pos/Ï„) / sum(exp(all/Ï„)) )\n",
    "         = -pos/Ï„ + log(sum(exp(all/Ï„)))\n",
    "    \n",
    "    BPRê³¼ì˜ ì°¨ì´:\n",
    "    - BPR: pairwise comparison (pos vs neg_1, pos vs neg_2, ...)\n",
    "    - InfoNCE: multi-class classification (posë¥¼ [pos, neg_1, ..., neg_k] ì¤‘ì—ì„œ êµ¬ë¶„)\n",
    "    \n",
    "    ì¥ì :\n",
    "    1. ëª¨ë“  negativeë¥¼ ë™ì‹œì— í™œìš© â†’ ë” ê°•í•œ gradient\n",
    "    2. Temperatureë¡œ hard negativeì— ì§‘ì¤‘ ê°€ëŠ¥\n",
    "    3. Score distributionì´ ë” diverseí•´ì§ (collapse ë°©ì§€)\n",
    "    \"\"\"\n",
    "    batch_size = pos_scores.size(0)\n",
    "    \n",
    "    # Reshape: neg_scoresë¥¼ (batch_size, neg_ratio)ë¡œ\n",
    "    neg_scores = neg_scores.view(batch_size, neg_ratio)\n",
    "    \n",
    "    # pos_scoresë¥¼ (batch_size, 1)ë¡œ í™•ì¥\n",
    "    pos_scores = pos_scores.unsqueeze(1)\n",
    "    \n",
    "    # ëª¨ë“  scoresë¥¼ concatenate: (batch_size, 1 + neg_ratio)\n",
    "    all_scores = torch.cat([pos_scores, neg_scores], dim=1)\n",
    "    \n",
    "    # Temperature scaling\n",
    "    all_scores = all_scores / temperature\n",
    "    \n",
    "    # InfoNCE: -log(exp(pos) / sum(exp(all)))\n",
    "    # = -log_softmax(all_scores)[:, 0]\n",
    "    # positiveëŠ” í•­ìƒ index 0\n",
    "    log_prob = F.log_softmax(all_scores, dim=1)\n",
    "    loss = -log_prob[:, 0].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(\"InfoNCE Loss í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"\\nìˆ˜ì‹:\")\n",
    "print(\"  BPR:     loss = -log(Ïƒ(s_pos - s_neg)) for each pair\")\n",
    "print(\"  InfoNCE: loss = -log(exp(s_pos/Ï„) / Î£exp(s_all/Ï„))\")\n",
    "print(\"\\nì°¨ì´: BPRì€ ë…ë¦½ì  pair, InfoNCEëŠ” ëª¨ë“  negative ë™ì‹œ ê³ ë ¤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ í•¨ìˆ˜ (V6ì™€ ë™ì¼)\n",
    "def evaluate_model(model, edge_index, eval_df, user_items_dict, n_items, k=10, device='cpu'):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€: Precision@K, Recall@K, NDCG@K\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        user_emb, item_emb = model(edge_index.to(device))\n",
    "        \n",
    "        precisions, recalls, ndcgs = [], [], []\n",
    "        \n",
    "        for user_id, group in eval_df.groupby('user_id'):\n",
    "            true_items = set(group['item_id'].values)\n",
    "            exclude_items = user_items_dict[user_id]\n",
    "            \n",
    "            user_emb_single = user_emb[user_id].unsqueeze(0)\n",
    "            scores = torch.matmul(user_emb_single, item_emb.t()).squeeze()\n",
    "            \n",
    "            scores_np = scores.cpu().numpy()\n",
    "            for item_id in exclude_items:\n",
    "                scores_np[int(item_id)] = -np.inf\n",
    "            \n",
    "            top_k_items = np.argsort(scores_np)[-k:][::-1]\n",
    "            \n",
    "            hits = len(set(top_k_items) & true_items)\n",
    "            \n",
    "            precision = hits / k\n",
    "            recall = hits / len(true_items) if len(true_items) > 0 else 0\n",
    "            \n",
    "            dcg = sum([1 / np.log2(i + 2) for i, item in enumerate(top_k_items) if item in true_items])\n",
    "            idcg = sum([1 / np.log2(i + 2) for i in range(min(len(true_items), k))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            ndcgs.append(ndcg)\n",
    "    \n",
    "    return {\n",
    "        f'precision@{k}': np.mean(precisions),\n",
    "        f'recall@{k}': np.mean(recalls),\n",
    "        f'ndcg@{k}': np.mean(ndcgs),\n",
    "    }\n",
    "\n",
    "print(\"í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training í•¨ìˆ˜ (Hard Negative Mining ì ìš©)\n",
    "def train_one_epoch(model, edge_index, train_df, user_items_dict, n_items, \n",
    "                    optimizer, batch_size, neg_ratio, temperature, device):\n",
    "    \"\"\"1 epoch training with InfoNCE + Hard Negative Mining\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_df_shuffled = train_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for start_idx in range(0, len(train_df_shuffled), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(train_df_shuffled))\n",
    "        batch_df = train_df_shuffled.iloc[start_idx:end_idx]\n",
    "        \n",
    "        pos_users = torch.tensor(batch_df['user_id'].values, dtype=torch.long).to(device)\n",
    "        pos_items = torch.tensor(batch_df['item_id'].values, dtype=torch.long).to(device)\n",
    "        \n",
    "        # â­ Hard Negative Mining ì‚¬ìš©!\n",
    "        neg_users_np, neg_items_np = hard_negative_sampling(\n",
    "            model, edge_index, batch_df, user_items_dict, n_items, device, neg_ratio\n",
    "        )\n",
    "        neg_users = torch.tensor(neg_users_np, dtype=torch.long).to(device)\n",
    "        neg_items = torch.tensor(neg_items_np, dtype=torch.long).to(device)\n",
    "        \n",
    "        pos_scores = model.predict(pos_users, pos_items, edge_index.to(device))\n",
    "        neg_scores = model.predict(neg_users, neg_items, edge_index.to(device))\n",
    "        \n",
    "        # InfoNCE Loss ì‚¬ìš©\n",
    "        loss = infonce_loss(pos_scores, neg_scores, neg_ratio, temperature)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "print(\"Training í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ! (InfoNCE + Hard Negative Mining)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì¸ Training Loop (V11A - Hard Negative Mining)\n",
    "model = LightGCN(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    embedding_dim=CONFIG['embedding_dim'],\n",
    "    n_layers=CONFIG['n_layers']\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'valid_precision': [],\n",
    "    'valid_recall': [],\n",
    "    'valid_ndcg': [],\n",
    "}\n",
    "\n",
    "best_recall = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training ì‹œì‘ (V11A - Hard Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: LightGCN\")\n",
    "print(f\"  - Users: {n_users}, Items: {n_items}\")\n",
    "print(f\"  - Embedding dim: {CONFIG['embedding_dim']}\")\n",
    "print(f\"  - Layers: {CONFIG['n_layers']}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  - Total params: {total_params:,}\")\n",
    "print(f\"\\nğŸ”¥ í•µì‹¬ ë³€ê²½: Random â†’ Hard Negative Mining\")\n",
    "print(f\"  V9c: Random sampling (ì‰¬ìš´ negatives)\")\n",
    "print(f\"  V11a: Semi-hard (50%) + In-batch (50%)\")\n",
    "print(f\"  â†’ 4~8% Recall ê°œì„  ì˜ˆìƒ\")\n",
    "print(f\"\\nLoss Function: InfoNCE (temperature={CONFIG['temperature']})\")\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Negative ratio: {CONFIG['neg_ratio']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_edge_index, train_df, train_user_items,\n",
    "        n_items, optimizer, CONFIG['batch_size'], \n",
    "        CONFIG['neg_ratio'], CONFIG['temperature'], CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    val_metrics = evaluate_model(\n",
    "        model, train_edge_index, valid_df, train_user_items,\n",
    "        n_items, k=CONFIG['top_k'], device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['valid_precision'].append(val_metrics[f'precision@{CONFIG[\"top_k\"]}'])\n",
    "    history['valid_recall'].append(val_metrics[f'recall@{CONFIG[\"top_k\"]}'])\n",
    "    history['valid_ndcg'].append(val_metrics[f'ndcg@{CONFIG[\"top_k\"]}'])\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{CONFIG['epochs']} | \"\n",
    "              f\"Loss: {train_loss:.4f} | \"\n",
    "              f\"P@{CONFIG['top_k']}: {val_metrics[f'precision@{CONFIG[\"top_k\"]}']:.4f} | \"\n",
    "              f\"R@{CONFIG['top_k']}: {val_metrics[f'recall@{CONFIG[\"top_k\"]}']:.4f} | \"\n",
    "              f\"NDCG@{CONFIG['top_k']}: {val_metrics[f'ndcg@{CONFIG[\"top_k\"]}']:.4f}\")\n",
    "    \n",
    "    current_recall = val_metrics[f'recall@{CONFIG[\"top_k\"]}']\n",
    "    if current_recall > best_recall:\n",
    "        best_recall = current_recall\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_best.pth'))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training ì™„ë£Œ! (V11A - Hard Negative Mining)\")\n",
    "print(f\"Best Recall@{CONFIG['top_k']}: {best_recall:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train Loss (InfoNCE + Hard Neg)', linewidth=2, color='#e74c3c')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Curve (V11A - Hard Negative Mining)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['valid_precision'], label=f'Precision@{CONFIG[\"top_k\"]}', linewidth=2)\n",
    "axes[1].plot(history['valid_recall'], label=f'Recall@{CONFIG[\"top_k\"]}', linewidth=2)\n",
    "axes[1].plot(history['valid_ndcg'], label=f'NDCG@{CONFIG[\"top_k\"]}', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Validation Metrics (V11A)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['result_dir'], 'training_curves_v11a.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\" * 60)\nprint(\"V11A vs V9C ê²°ê³¼ ë¹„êµ\")\nprint(\"=\" * 60)\n\n# V9c ê²°ê³¼ (baseline)\nv9c_results = {\n    10: {'precision@10': 0.2756, 'recall@10': 0.1578, 'ndcg@10': 0.3129}\n}\n\nprint(\"\\ní•µì‹¬ ë³€ê²½ì‚¬í•­:\")\nprint(f\"  V9c: Random negative sampling\")\nprint(f\"  V11a: Hard negative mining (Semi-hard 50% + In-batch 50%)\")\n\nprint(\"\\nTest Set ì„±ëŠ¥ (Recall@10):\")\nprint(f\"  V9c:  {v9c_results[10]['recall@10']:.4f} (15.78%)\")\nprint(f\"  V11a: {v11a_results[10]['recall@10']:.4f} ({v11a_results[10]['recall@10']*100:.2f}%)\")\n\nimprovement = (v11a_results[10]['recall@10'] - v9c_results[10]['recall@10']) / v9c_results[10]['recall@10'] * 100\nprint(f\"  ë³€í™”: {improvement:+.1f}%\")\n\nprint(\"\\nì „ì²´ ì§€í‘œ ë¹„êµ (Top-10):\")\nfor metric in ['precision@10', 'recall@10', 'ndcg@10']:\n    v9c_val = v9c_results[10][metric]\n    v11a_val = v11a_results[10][metric]\n    change = (v11a_val - v9c_val) / v9c_val * 100\n    symbol = 'âœ…' if change > 0 else 'âŒ'\n    print(f\"  {metric:15s}: V9c={v9c_val:.4f}, V11a={v11a_val:.4f} ({change:+.1f}%) {symbol}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ì‹¤í—˜ ê²°ê³¼ í•´ì„:\")\nif improvement > 5:\n    print(\"ğŸ‰ BREAKTHROUGH ì„±ê³µ!\")\n    print(\"   Hard negative miningì´ í•µì‹¬ bottleneckì´ì—ˆìŒ\")\n    print(\"   â†’ Random samplingì´ ë„ˆë¬´ ì‰¬ìš´ negativesë§Œ ì œê³µ\")\n    print(\"   â†’ Semi-hard + In-batchë¡œ í•™ìŠµ íš¨ìœ¨ ëŒ€í­ í–¥ìƒ\")\n    print(\"\\në‹¤ìŒ ë‹¨ê³„:\")\n    print(\"   - V10a (embedding capacity)ì™€ ë¹„êµ\")\n    print(\"   - ìµœì  ëª¨ë¸ ì„ íƒ í›„ ìµœì¢… í‰ê°€\")\nelif improvement > 2:\n    print(\"âœ… ì†Œí­ ê°œì„ . Hard negative mining íš¨ê³¼ ìˆìŒ\")\n    print(\"   â†’ V10aì™€ ë¹„êµ í›„ ìµœì¢… ê²°ì •\")\nelif improvement > -2:\n    print(\"â– ë¹„ìŠ·í•œ ìˆ˜ì¤€. Hard negative íš¨ê³¼ ë¯¸ë¯¸\")\n    print(\"   â†’ V10a (embedding capacity) ì‹œë„\")\nelse:\n    print(\"âŒ ì„±ëŠ¥ í•˜ë½. ì´ ë°ì´í„°ì…‹ì—ì„œëŠ” randomì´ ë” ë‚˜ìŒ\")\n    print(\"   â†’ V10aë¡œ ë‹¤ë¥¸ ë°©í–¥ íƒìƒ‰\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set í‰ê°€\n",
    "model.load_state_dict(torch.load(os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_best.pth')))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Test Set í‰ê°€ (V11A - Hard Negative Mining)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "v11a_results = {}\n",
    "\n",
    "for k in [5, 10, 20]:\n",
    "    test_metrics = evaluate_model(\n",
    "        model, \n",
    "        train_valid_edge_index,\n",
    "        test_df, \n",
    "        train_valid_user_items,\n",
    "        n_items, \n",
    "        k=k, \n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    v11a_results[k] = test_metrics\n",
    "    \n",
    "    print(f\"\\nTop-{k} ì¶”ì²œ:\")\n",
    "    print(f\"  Precision@{k}: {test_metrics[f'precision@{k}']:.4f}\")\n",
    "    print(f\"  Recall@{k}:    {test_metrics[f'recall@{k}']:.4f}\")\n",
    "    print(f\"  NDCG@{k}:      {test_metrics[f'ndcg@{k}']:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8a vs V6 ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"V11A vs V9C ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# V9c ê²°ê³¼ (baseline)\n",
    "v9c_results = {\n",
    "    10: {'precision@10': 0.2756, 'recall@10': 0.1578, 'ndcg@10': 0.3129}\n",
    "}\n",
    "\n",
    "print(\"\\ní•µì‹¬ ë³€ê²½ì‚¬í•­:\")\n",
    "print(f\"  V9c: Random negative sampling\")\n",
    "print(f\"  V11a: Hard negative mining (Semi-hard 50% + In-batch 50%)\")\n",
    "\n",
    "print(\"\\nTest Set ì„±ëŠ¥ (Recall@10):\")\n",
    "print(f\"  V9c:  {v9c_results[10]['recall@10']:.4f} (15.78%)\")\n",
    "print(f\"  V11a: {v11a_results[10]['recall@10']:.4f} ({v11a_results[10]['recall@10']*100:.2f}%)\")\n",
    "\n",
    "improvement = (v11a_results[10]['recall@10'] - v9c_results[10]['recall@10']) / v9c_results[10]['recall@10'] * 100\n",
    "print(f\"  ë³€í™”: {improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\nì „ì²´ ì§€í‘œ ë¹„êµ (Top-10):\")\n",
    "for metric in ['precision@10', 'recall@10', 'ndcg@10']:\n",
    "    v9c_val = v9c_results[10][metric]\n",
    "    v11a_val = v11a_results[10][metric]\n",
    "    change = (v11a_val - v9c_val) / v9c_val * 100\n",
    "    symbol = 'âœ…' if change > 0 else 'âŒ'\n",
    "    print(f\"  {metric:15s}: V9c={v9c_val:.4f}, V11a={v11a_val:.4f} ({change:+.1f}%) {symbol}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ì‹¤í—˜ ê²°ê³¼ í•´ì„:\")\n",
    "if improvement > 5:\n",
    "    print(\"ğŸ‰ BREAKTHROUGH ì„±ê³µ!\")\n",
    "    print(\"   Hard negative miningì´ í•µì‹¬ bottleneckì´ì—ˆìŒ\")\n",
    "    print(\"   â†’ Random samplingì´ ë„ˆë¬´ ì‰¬ìš´ negativesë§Œ ì œê³µ\")\n",
    "    print(\"   â†’ Semi-hard + In-batchë¡œ í•™ìŠµ íš¨ìœ¨ ëŒ€í­ í–¥ìƒ\")\n",
    "    print(\"\\në‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(\"   - V10a (embedding capacity)ì™€ ë¹„êµ\")\n",
    "    print(\"   - ìµœì  ëª¨ë¸ ì„ íƒ í›„ ìµœì¢… í‰ê°€\")\n",
    "elif improvement > 2:\n",
    "    print(\"âœ… ì†Œí­ ê°œì„ . Hard negative mining íš¨ê³¼ ìˆìŒ\")\n",
    "    print(\"   â†’ V10aì™€ ë¹„êµ í›„ ìµœì¢… ê²°ì •\")\n",
    "elif improvement > -2:\n",
    "    print(\"â– ë¹„ìŠ·í•œ ìˆ˜ì¤€. Hard negative íš¨ê³¼ ë¯¸ë¯¸\")\n",
    "    print(\"   â†’ V10a (embedding capacity) ì‹œë„\")\n",
    "else:\n",
    "    print(\"âŒ ì„±ëŠ¥ í•˜ë½. ì´ ë°ì´í„°ì…‹ì—ì„œëŠ” randomì´ ë” ë‚˜ìŒ\")\n",
    "    print(\"   â†’ V10aë¡œ ë‹¤ë¥¸ ë°©í–¥ íƒìƒ‰\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}