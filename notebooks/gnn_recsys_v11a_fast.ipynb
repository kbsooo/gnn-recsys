{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GNN 기반 영화 추천 시스템\n# V11A_FAST: In-batch Negative Mining (MacBook M4 최적화)\n# Step 2 - 빠른 breakthrough 실험"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "환경 설정\n",
      "============================================================\n",
      "PyTorch 버전: 2.9.0\n",
      "CUDA 사용 가능: False\n",
      "MPS 사용 가능: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"환경 설정\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS 사용 가능: {torch.backends.mps.is_available()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 하이퍼파라미터 설정 (V11A_FAST - In-batch Negative Mining)\nCONFIG = {\n    # Filtering 기준\n    'min_user_interactions': 30,\n    'min_item_interactions': 10,\n    'rating_threshold': None,\n    \n    # Split 비율\n    'train_ratio': 0.70,\n    'valid_ratio': 0.15,\n    'test_ratio': 0.15,\n    \n    # 모델 파라미터\n    'embedding_dim': 64,\n    'n_layers': 2,\n    \n    # 학습 파라미터\n    'learning_rate': 0.001,\n    'weight_decay': 1e-4,\n    'batch_size': 512,\n    'epochs': 100,\n    'patience': 20,\n    'neg_ratio': 6,\n    \n    # InfoNCE 파라미터\n    'temperature': 0.2,\n    \n    # 평가\n    'top_k': 10,\n    \n    # 시스템\n    'device': 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'),\n    'seed': 42,\n    \n    # 경로\n    'data_dir': '../data',\n    'processed_dir': '../data/processed',\n    'model_dir': '../models',\n    'result_dir': '../results',\n}\n\n# 폴더 생성\nfor dir_path in [CONFIG['processed_dir'], CONFIG['model_dir'], CONFIG['result_dir']]:\n    os.makedirs(dir_path, exist_ok=True)\n\n# Random seed 고정\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    if torch.backends.mps.is_available():\n        torch.mps.manual_seed(seed)\n\nset_seed(CONFIG['seed'])\n\nprint(\"=\" * 60)\nprint(\"V11A_FAST 설정 완료! (In-batch Negative Mining)\")\nprint(\"=\" * 60)\nprint(f\"Device: {CONFIG['device']}\")\nprint(\"\\n⚡ 핵심: Random → In-batch Negative (MacBook M4 최적화)\")\nprint(f\"  V9c: Random sampling\")\nprint(f\"  V11a_fast: In-batch only → V9c와 동일한 속도!\")\nprint(f\"\\n전략:\")\nprint(f\"  - 같은 배치 내 다른 유저의 positive items 사용\")\nprint(f\"  - 모델 forward 불필요 → 매우 빠름\")\nprint(f\"\\n예상: Recall@10 > 17% (V9c 속도로!)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드 (V6 전처리 파일 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "V6 전처리 데이터 로드\n",
      "============================================================\n",
      "\n",
      "데이터 로드 완료!\n",
      "  Users: 529\n",
      "  Items: 2283\n",
      "  Train: 56,072\n",
      "  Valid: 11,826\n",
      "  Test:  12,572\n",
      "\n",
      "✅ V6와 동일한 데이터 사용 → 공정한 비교 가능\n"
     ]
    }
   ],
   "source": [
    "# V6에서 전처리한 데이터 로드\n",
    "print(\"=\" * 60)\n",
    "print(\"V6 전처리 데이터 로드\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'train_split_v6.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'valid_split_v6.csv'))\n",
    "test_df = pd.read_csv(os.path.join(CONFIG['processed_dir'], 'test_split_v6.csv'))\n",
    "\n",
    "with open(os.path.join(CONFIG['processed_dir'], 'id_mappings_v6.pkl'), 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "n_users = len(mappings['user_id_map'])\n",
    "n_items = len(mappings['item_id_map'])\n",
    "\n",
    "print(f\"\\n데이터 로드 완료!\")\n",
    "print(f\"  Users: {n_users}\")\n",
    "print(f\"  Items: {n_items}\")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Valid: {len(valid_df):,}\")\n",
    "print(f\"  Test:  {len(test_df):,}\")\n",
    "print(\"\\n✅ V6와 동일한 데이터 사용 → 공정한 비교 가능\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Graph 구성\n",
      "============================================================\n",
      "\n",
      "[1] Train Graph (학습용)\n",
      "Node 수: 2812 (User: 529, Item: 2283)\n",
      "Edge 수: 112,144 (양방향)\n",
      "\n",
      "[2] Train+Valid Graph (Test 평가용)\n",
      "Node 수: 2812 (User: 529, Item: 2283)\n",
      "Edge 수: 135,796 (양방향)\n",
      "\n",
      "✅ Graph 생성 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/1w84fr7s5kxcwc2l24qrjjwc0000gn/T/ipykernel_86377/3903611053.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  edge_index = torch.tensor([\n"
     ]
    }
   ],
   "source": [
    "# Graph 구성\n",
    "print(\"=\" * 60)\n",
    "print(\"Graph 구성\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_graph(df, n_users, n_items):\n",
    "    \"\"\"User-Item Bipartite Graph 생성\"\"\"\n",
    "    user_ids = df['user_id'].values\n",
    "    item_ids = df['item_id'].values + n_users\n",
    "    \n",
    "    edge_index = torch.tensor([\n",
    "        np.concatenate([user_ids, item_ids]),\n",
    "        np.concatenate([item_ids, user_ids])\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    print(f\"Node 수: {n_users + n_items} (User: {n_users}, Item: {n_items})\")\n",
    "    print(f\"Edge 수: {edge_index.shape[1]:,} (양방향)\")\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "print(\"\\n[1] Train Graph (학습용)\")\n",
    "train_edge_index = create_graph(train_df, n_users, n_items)\n",
    "\n",
    "print(\"\\n[2] Train+Valid Graph (Test 평가용)\")\n",
    "train_valid_df = pd.concat([train_df, valid_df])\n",
    "train_valid_edge_index = create_graph(train_valid_df, n_users, n_items)\n",
    "\n",
    "print(\"\\n✅ Graph 생성 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ⭐ In-batch Negative Mining (FAST!)\ndef create_user_item_dict(df):\n    \"\"\"User별로 상호작용한 item 집합 생성\"\"\"\n    user_items = defaultdict(set)\n    for _, row in df.iterrows():\n        user_items[row['user_id']].add(row['item_id'])\n    return user_items\n\ndef inbatch_negative_sampling(batch_df, user_items_dict, n_items, neg_ratio=6):\n    \"\"\"In-batch negative mining - 모델 forward 없이 빠름!\"\"\"\n    pos_users = batch_df['user_id'].values\n    neg_users = []\n    neg_items = []\n    \n    # 배치 내 모든 유저의 positive items 수집\n    batch_user_items = defaultdict(list)\n    for user_id in pos_users:\n        batch_user_items[user_id] = list(user_items_dict[user_id])\n    \n    # 각 유저별로 in-batch negatives 생성\n    for idx, user_id in enumerate(pos_users):\n        user_pos_items = user_items_dict[user_id]\n        \n        # 같은 배치의 다른 유저들의 positive items 수집\n        candidates = []\n        for other_idx, other_user in enumerate(pos_users):\n            if other_idx != idx:\n                other_items = batch_user_items[other_user]\n                candidates.extend([i for i in other_items if i not in user_pos_items])\n        \n        # neg_ratio개 샘플링\n        if len(candidates) >= neg_ratio:\n            sampled = np.random.choice(candidates, neg_ratio, replace=False)\n        else:\n            # 부족하면 random으로 채움\n            sampled = list(candidates) if candidates else []\n            while len(sampled) < neg_ratio:\n                rand_item = random.randint(0, n_items - 1)\n                if rand_item not in user_pos_items and rand_item not in sampled:\n                    sampled.append(rand_item)\n        \n        for neg_item in sampled:\n            neg_users.append(user_id)\n            neg_items.append(neg_item)\n    \n    return np.array(neg_users), np.array(neg_items)\n\ntrain_user_items = create_user_item_dict(train_df)\ntrain_valid_user_items = create_user_item_dict(train_valid_df)\n\nprint(\"In-batch Negative Mining 함수 정의 완료!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGCN 모델 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "# LightGCN 모델 (V6와 동일)\n",
    "class LightGCNConv(MessagePassing):\n",
    "    \"\"\"LightGCN Convolution Layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "    \n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    \"\"\"LightGCN for Recommendation\"\"\"\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.1)\n",
    "        \n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, edge_index):\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        all_emb = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            all_emb = conv(all_emb, edge_index)\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.stack(embs, dim=0).mean(dim=0)\n",
    "        \n",
    "        user_final = final_emb[:self.n_users]\n",
    "        item_final = final_emb[self.n_users:]\n",
    "        \n",
    "        return user_final, item_final\n",
    "    \n",
    "    def predict(self, users, items, edge_index):\n",
    "        user_emb, item_emb = self.forward(edge_index)\n",
    "        user_emb = user_emb[users]\n",
    "        item_emb = item_emb[items]\n",
    "        scores = (user_emb * item_emb).sum(dim=1)\n",
    "        return scores\n",
    "\n",
    "print(\"LightGCN 모델 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InfoNCE Loss 함수 정의 완료!\n",
      "\n",
      "수식:\n",
      "  BPR:     loss = -log(σ(s_pos - s_neg)) for each pair\n",
      "  InfoNCE: loss = -log(exp(s_pos/τ) / Σexp(s_all/τ))\n",
      "\n",
      "차이: BPR은 독립적 pair, InfoNCE는 모든 negative 동시 고려\n"
     ]
    }
   ],
   "source": [
    "# ⭐ InfoNCE Loss (핵심 변경!)\n",
    "def infonce_loss(pos_scores, neg_scores, neg_ratio=4, temperature=0.2):\n",
    "    \"\"\"\n",
    "    InfoNCE (Contrastive Learning) Loss\n",
    "    \n",
    "    loss = -log( exp(pos/τ) / (exp(pos/τ) + sum(exp(neg_i/τ))) )\n",
    "         = -log( exp(pos/τ) / sum(exp(all/τ)) )\n",
    "         = -pos/τ + log(sum(exp(all/τ)))\n",
    "    \n",
    "    BPR과의 차이:\n",
    "    - BPR: pairwise comparison (pos vs neg_1, pos vs neg_2, ...)\n",
    "    - InfoNCE: multi-class classification (pos를 [pos, neg_1, ..., neg_k] 중에서 구분)\n",
    "    \n",
    "    장점:\n",
    "    1. 모든 negative를 동시에 활용 → 더 강한 gradient\n",
    "    2. Temperature로 hard negative에 집중 가능\n",
    "    3. Score distribution이 더 diverse해짐 (collapse 방지)\n",
    "    \"\"\"\n",
    "    batch_size = pos_scores.size(0)\n",
    "    \n",
    "    # Reshape: neg_scores를 (batch_size, neg_ratio)로\n",
    "    neg_scores = neg_scores.view(batch_size, neg_ratio)\n",
    "    \n",
    "    # pos_scores를 (batch_size, 1)로 확장\n",
    "    pos_scores = pos_scores.unsqueeze(1)\n",
    "    \n",
    "    # 모든 scores를 concatenate: (batch_size, 1 + neg_ratio)\n",
    "    all_scores = torch.cat([pos_scores, neg_scores], dim=1)\n",
    "    \n",
    "    # Temperature scaling\n",
    "    all_scores = all_scores / temperature\n",
    "    \n",
    "    # InfoNCE: -log(exp(pos) / sum(exp(all)))\n",
    "    # = -log_softmax(all_scores)[:, 0]\n",
    "    # positive는 항상 index 0\n",
    "    log_prob = F.log_softmax(all_scores, dim=1)\n",
    "    loss = -log_prob[:, 0].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(\"InfoNCE Loss 함수 정의 완료!\")\n",
    "print(\"\\n수식:\")\n",
    "print(\"  BPR:     loss = -log(σ(s_pos - s_neg)) for each pair\")\n",
    "print(\"  InfoNCE: loss = -log(exp(s_pos/τ) / Σexp(s_all/τ))\")\n",
    "print(\"\\n차이: BPR은 독립적 pair, InfoNCE는 모든 negative 동시 고려\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 함수 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "# 평가 함수 (V6와 동일)\n",
    "def evaluate_model(model, edge_index, eval_df, user_items_dict, n_items, k=10, device='cpu'):\n",
    "    \"\"\"모델 평가: Precision@K, Recall@K, NDCG@K\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        user_emb, item_emb = model(edge_index.to(device))\n",
    "        \n",
    "        precisions, recalls, ndcgs = [], [], []\n",
    "        \n",
    "        for user_id, group in eval_df.groupby('user_id'):\n",
    "            true_items = set(group['item_id'].values)\n",
    "            exclude_items = user_items_dict[user_id]\n",
    "            \n",
    "            user_emb_single = user_emb[user_id].unsqueeze(0)\n",
    "            scores = torch.matmul(user_emb_single, item_emb.t()).squeeze()\n",
    "            \n",
    "            scores_np = scores.cpu().numpy()\n",
    "            for item_id in exclude_items:\n",
    "                scores_np[int(item_id)] = -np.inf\n",
    "            \n",
    "            top_k_items = np.argsort(scores_np)[-k:][::-1]\n",
    "            \n",
    "            hits = len(set(top_k_items) & true_items)\n",
    "            \n",
    "            precision = hits / k\n",
    "            recall = hits / len(true_items) if len(true_items) > 0 else 0\n",
    "            \n",
    "            dcg = sum([1 / np.log2(i + 2) for i, item in enumerate(top_k_items) if item in true_items])\n",
    "            idcg = sum([1 / np.log2(i + 2) for i in range(min(len(true_items), k))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            ndcgs.append(ndcg)\n",
    "    \n",
    "    return {\n",
    "        f'precision@{k}': np.mean(precisions),\n",
    "        f'recall@{k}': np.mean(recalls),\n",
    "        f'ndcg@{k}': np.mean(ndcgs),\n",
    "    }\n",
    "\n",
    "print(\"평가 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training 함수 (In-batch Negative 적용)\ndef train_one_epoch(model, edge_index, train_df, user_items_dict, n_items, \n                    optimizer, batch_size, neg_ratio, temperature, device):\n    \"\"\"1 epoch training with InfoNCE + In-batch Negative\"\"\"\n    model.train()\n    \n    train_df_shuffled = train_df.sample(frac=1).reset_index(drop=True)\n    \n    total_loss = 0\n    n_batches = 0\n    \n    for start_idx in range(0, len(train_df_shuffled), batch_size):\n        end_idx = min(start_idx + batch_size, len(train_df_shuffled))\n        batch_df = train_df_shuffled.iloc[start_idx:end_idx]\n        \n        pos_users = torch.tensor(batch_df['user_id'].values, dtype=torch.long).to(device)\n        pos_items = torch.tensor(batch_df['item_id'].values, dtype=torch.long).to(device)\n        \n        # ⭐ In-batch Negative 사용!\n        neg_users_np, neg_items_np = inbatch_negative_sampling(\n            batch_df, user_items_dict, n_items, neg_ratio\n        )\n        neg_users = torch.tensor(neg_users_np, dtype=torch.long).to(device)\n        neg_items = torch.tensor(neg_items_np, dtype=torch.long).to(device)\n        \n        pos_scores = model.predict(pos_users, pos_items, edge_index.to(device))\n        neg_scores = model.predict(neg_users, neg_items, edge_index.to(device))\n        \n        loss = infonce_loss(pos_scores, neg_scores, neg_ratio, temperature)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        n_batches += 1\n    \n    return total_loss / n_batches\n\nprint(\"Training 함수 정의 완료! (InfoNCE + In-batch Negative)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 메인 Training Loop (V11A_FAST)\nmodel = LightGCN(\n    n_users=n_users,\n    n_items=n_items,\n    embedding_dim=CONFIG['embedding_dim'],\n    n_layers=CONFIG['n_layers']\n).to(CONFIG['device'])\n\noptimizer = torch.optim.Adam(\n    model.parameters(), \n    lr=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay']\n)\n\nhistory = {\n    'train_loss': [],\n    'valid_precision': [],\n    'valid_recall': [],\n    'valid_ndcg': [],\n}\n\nbest_recall = 0\npatience_counter = 0\n\nprint(\"=\" * 60)\nprint(\"Training 시작 (V11A_FAST - In-batch Negative)\")\nprint(\"=\" * 60)\nprint(f\"Model: LightGCN\")\nprint(f\"  - Users: {n_users}, Items: {n_items}\")\nprint(f\"  - Embedding: {CONFIG['embedding_dim']}, Layers: {CONFIG['n_layers']}\")\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"  - Total params: {total_params:,}\")\nprint(f\"\\n⚡ In-batch Negative (V9c 속도!)\")\nprint(f\"Device: {CONFIG['device']}\")\nprint(\"=\" * 60)\n\nfor epoch in range(CONFIG['epochs']):\n    train_loss = train_one_epoch(\n        model, train_edge_index, train_df, train_user_items,\n        n_items, optimizer, CONFIG['batch_size'], \n        CONFIG['neg_ratio'], CONFIG['temperature'], CONFIG['device']\n    )\n    \n    val_metrics = evaluate_model(\n        model, train_edge_index, valid_df, train_user_items,\n        n_items, k=CONFIG['top_k'], device=CONFIG['device']\n    )\n    \n    history['train_loss'].append(train_loss)\n    history['valid_precision'].append(val_metrics[f'precision@{CONFIG[\"top_k\"]}'])\n    history['valid_recall'].append(val_metrics[f'recall@{CONFIG[\"top_k\"]}'])\n    history['valid_ndcg'].append(val_metrics[f'ndcg@{CONFIG[\"top_k\"]}'])\n    \n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        print(f\"Epoch {epoch+1:3d}/{CONFIG['epochs']} | \"\n              f\"Loss: {train_loss:.4f} | \"\n              f\"R@{CONFIG['top_k']}: {val_metrics[f'recall@{CONFIG[\"top_k\"]}']:.4f}\")\n    \n    current_recall = val_metrics[f'recall@{CONFIG[\"top_k\"]}']\n    if current_recall > best_recall:\n        best_recall = current_recall\n        patience_counter = 0\n        torch.save(model.state_dict(), \n                   os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_fast_best.pth'))\n    else:\n        patience_counter += 1\n    \n    if patience_counter >= CONFIG['patience']:\n        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n        break\n\nprint(\"=\" * 60)\nprint(f\"Training 완료! Best Recall@{CONFIG['top_k']}: {best_recall:.4f}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training 결과 시각화\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\naxes[0].plot(history['train_loss'], label='Train Loss', linewidth=2, color='#e74c3c')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('V11A_FAST Training Loss')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(history['valid_recall'], label=f'Recall@{CONFIG[\"top_k\"]}', linewidth=2)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Score')\naxes[1].set_title('V11A_FAST Validation Recall')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(CONFIG['result_dir'], 'training_curves_v11a_fast.png'), dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Set 평가\nmodel.load_state_dict(torch.load(os.path.join(CONFIG['model_dir'], 'lightgcn_v11a_fast_best.pth')))\n\nprint(\"=\" * 60)\nprint(\"Test Set 평가 (V11A_FAST)\")\nprint(\"=\" * 60)\n\nv11a_fast_results = {}\n\nfor k in [5, 10, 20]:\n    test_metrics = evaluate_model(\n        model, \n        train_valid_edge_index,\n        test_df, \n        train_valid_user_items,\n        n_items, \n        k=k, \n        device=CONFIG['device']\n    )\n    \n    v11a_fast_results[k] = test_metrics\n    \n    print(f\"\\nTop-{k} 추천:\")\n    print(f\"  Precision@{k}: {test_metrics[f'precision@{k}']:.4f}\")\n    print(f\"  Recall@{k}:    {test_metrics[f'recall@{k}']:.4f}\")\n    print(f\"  NDCG@{k}:      {test_metrics[f'ndcg@{k}']:.4f}\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8a vs V6 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"V11A_FAST vs V9C 결과 비교\")\nprint(\"=\" * 60)\n\n# V9c 결과\nv9c_baseline = {\n    10: {'precision@10': 0.2756, 'recall@10': 0.1578, 'ndcg@10': 0.3129}\n}\n\nprint(\"\\n핵심 변경:\")\nprint(f\"  V9c: Random sampling\")\nprint(f\"  V11a_fast: In-batch negative\")\n\nprint(\"\\nTest Recall@10:\")\nprint(f\"  V9c:        {v9c_baseline[10]['recall@10']:.4f} (15.78%)\")\nprint(f\"  V11a_fast:  {v11a_fast_results[10]['recall@10']:.4f} ({v11a_fast_results[10]['recall@10']*100:.2f}%)\")\n\nimprovement = (v11a_fast_results[10]['recall@10'] - v9c_baseline[10]['recall@10']) / v9c_baseline[10]['recall@10'] * 100\nprint(f\"  변화: {improvement:+.1f}%\")\n\nprint(\"\\n전체 지표:\")\nfor metric in ['precision@10', 'recall@10', 'ndcg@10']:\n    v9c_val = v9c_baseline[10][metric]\n    v11a_val = v11a_fast_results[10][metric]\n    change = (v11a_val - v9c_val) / v9c_val * 100\n    symbol = '✅' if change > 0 else '❌'\n    print(f\"  {metric:15s}: {v9c_val:.4f} → {v11a_val:.4f} ({change:+.1f}%) {symbol}\")\n\nprint(\"=\" * 60)\nif improvement > 3:\n    print(\"✅ 성공! In-batch가 효과적!\")\nelif improvement > 0:\n    print(\"✅ 소폭 개선\")\nelse:\n    print(\"➖ 비슷한 수준\")\nprint(\"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}