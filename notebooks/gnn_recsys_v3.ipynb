{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b972aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN ê¸°ë°˜ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ\n",
    "# V3: ëª¨ë¸ ì¶•ì†Œ + Rating threshold ì™„í™”\n",
    "# MacBook Pro M4 16GB ìµœì í™” ë²„ì „\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"í™˜ê²½ ì„¤ì •\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (V3 - Reduced Model Size)\n",
    "CONFIG = {\n",
    "    # Split ë¹„ìœ¨\n",
    "    'train_ratio': 0.7,\n",
    "    'valid_ratio': 0.1,\n",
    "    'test_ratio': 0.2,\n",
    "    \n",
    "    # ëª¨ë¸ íŒŒë¼ë¯¸í„° (V3: ëŒ€í­ ì¶•ì†Œ!)\n",
    "    'embedding_dim': 32,       # 128 â†’ 32 (1/4ë¡œ ì¶•ì†Œ)\n",
    "    'n_layers': 1,             # 2 â†’ 1 (ë” ê°„ë‹¨í•˜ê²Œ)\n",
    "    \n",
    "    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "    'learning_rate': 0.001,    # 0.0005 â†’ 0.001 (ì‘ì€ ëª¨ë¸ì€ ë” í° LR)\n",
    "    'weight_decay': 1e-4,      # 1e-5 â†’ 1e-4 (ë” ê°•í•œ ì •ê·œí™”)\n",
    "    'batch_size': 512,         # 1024 â†’ 512\n",
    "    'epochs': 100,\n",
    "    'patience': 15,\n",
    "    'neg_ratio': 1,            # 2 â†’ 1 (ê°„ë‹¨í•˜ê²Œ)\n",
    "    \n",
    "    # í‰ê°€\n",
    "    'top_k': 10,\n",
    "    \n",
    "    # Rating threshold (V3: ì™„í™”!)\n",
    "    'rating_threshold': 3.5,   # 4.0 â†’ 3.5 (ë” ë§ì€ ë°ì´í„°)\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ\n",
    "    'device': 'mps' if torch.backends.mps.is_available() else 'cpu',\n",
    "    'seed': 42,\n",
    "    \n",
    "    # ê²½ë¡œ\n",
    "    'data_dir': '../data',\n",
    "    'processed_dir': '../data/processed',\n",
    "    'model_dir': '../models',\n",
    "    'result_dir': '../results',\n",
    "}\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "for dir_path in [CONFIG['processed_dir'], CONFIG['model_dir'], CONFIG['result_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Random seed ê³ ì •\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V3 ì„¤ì • ì™„ë£Œ! (ëª¨ë¸ ì¶•ì†Œ + Threshold ì™„í™”)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Embedding dim: {CONFIG['embedding_dim']} (128 â†’ 32)\")\n",
    "print(f\"Layers: {CONFIG['n_layers']} (2 â†’ 1)\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']} (0.0005 â†’ 0.001)\")\n",
    "print(f\"Weight decay: {CONFIG['weight_decay']} (1e-5 â†’ 1e-4)\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']} (1024 â†’ 512)\")\n",
    "print(f\"Rating threshold: {CONFIG['rating_threshold']} (4.0 â†’ 3.5)\")\n",
    "print(f\"Negative ratio: {CONFIG['neg_ratio']} (2 â†’ 1)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ¯ ëª©í‘œ: íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œë¡œ ì¼ë°˜í™” ì„±ëŠ¥ ê°œì„ !\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2795e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv(os.path.join(CONFIG['data_dir'], 'train.csv'))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ë°ì´í„° ê¸°ë³¸ ì •ë³´\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì´ ìƒí˜¸ì‘ìš© ìˆ˜: {len(df):,}\")\n",
    "print(f\"\\nì»¬ëŸ¼: {list(df.columns)}\")\n",
    "print(f\"\\në°ì´í„° íƒ€ì…:\\n{df.dtypes}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"í†µê³„ ì •ë³´\")\n",
    "print(\"=\" * 60)\n",
    "n_users = df['user'].nunique()\n",
    "n_items = df['item'].nunique()\n",
    "print(f\"ê³ ìœ  ì‚¬ìš©ì ìˆ˜: {n_users:,}\")\n",
    "print(f\"ê³ ìœ  ì•„ì´í…œ ìˆ˜: {n_items:,}\")\n",
    "print(f\"ì‚¬ìš©ì ë‹¹ í‰ê·  ìƒí˜¸ì‘ìš©: {len(df) / n_users:.2f}\")\n",
    "print(f\"ì•„ì´í…œ ë‹¹ í‰ê·  ìƒí˜¸ì‘ìš©: {len(df) / n_items:.2f}\")\n",
    "\n",
    "sparsity = 100 * (1 - len(df) / (n_users * n_items))\n",
    "print(f\"í¬ì†Œì„±(Sparsity): {sparsity:.2f}%\")\n",
    "\n",
    "print(f\"\\nUser ID ë²”ìœ„: {df['user'].min()} ~ {df['user'].max()}\")\n",
    "print(f\"Item ID ë²”ìœ„: {df['item'].min()} ~ {df['item'].max()}\")\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„°\n",
    "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907973e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User/Item ID Re-indexing\n",
    "print(\"=\" * 60)\n",
    "print(\"ID Re-indexing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ê³ ìœ  ID ì¶”ì¶œ\n",
    "unique_users = sorted(df['user'].unique())\n",
    "unique_items = sorted(df['item'].unique())\n",
    "\n",
    "print(f\"ê³ ìœ  ì‚¬ìš©ì: {len(unique_users)}\")\n",
    "print(f\"ê³ ìœ  ì•„ì´í…œ: {len(unique_items)}\")\n",
    "\n",
    "# Mapping ìƒì„±\n",
    "user_id_map = {old_id: new_id for new_id, old_id in enumerate(unique_users)}\n",
    "item_id_map = {old_id: new_id for new_id, old_id in enumerate(unique_items)}\n",
    "user_id_reverse = {v: k for k, v in user_id_map.items()}\n",
    "item_id_reverse = {v: k for k, v in item_id_map.items()}\n",
    "\n",
    "# DataFrameì— ìƒˆë¡œìš´ ID ì ìš©\n",
    "df['user_id'] = df['user'].map(user_id_map)\n",
    "df['item_id'] = df['item'].map(item_id_map)\n",
    "\n",
    "# Mapping ì €ì¥\n",
    "mappings = {\n",
    "    'user_id_map': user_id_map,\n",
    "    'item_id_map': item_id_map,\n",
    "    'user_id_reverse': user_id_reverse,\n",
    "    'item_id_reverse': item_id_reverse,\n",
    "}\n",
    "\n",
    "with open(os.path.join(CONFIG['processed_dir'], 'id_mappings.pkl'), 'wb') as f:\n",
    "    pickle.dump(mappings, f)\n",
    "\n",
    "print(f\"\\në³€í™˜ ì™„ë£Œ!\")\n",
    "print(f\"User ID: 0 ~ {len(unique_users)-1}\")\n",
    "print(f\"Item ID: 0 ~ {len(unique_items)-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f00111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Valid/Test Split with Rating Threshold (V3: 3.5)\n",
    "print(\"=\" * 60)\n",
    "print(\"Train/Valid/Test Split (V3)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Rating threshold ì ìš©\n",
    "print(f\"\\nRating Threshold: {CONFIG['rating_threshold']}\")\n",
    "df_positive = df[df['rating'] >= CONFIG['rating_threshold']].copy()\n",
    "df_negative = df[df['rating'] < CONFIG['rating_threshold']].copy()\n",
    "\n",
    "print(f\"\\nì›ë³¸ ë°ì´í„°: {len(df):,}\")\n",
    "print(f\"Positive (â‰¥{CONFIG['rating_threshold']}): {len(df_positive):,} ({len(df_positive)/len(df)*100:.1f}%)\")\n",
    "print(f\"Negative (<{CONFIG['rating_threshold']}): {len(df_negative):,} ({len(df_negative)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nâœ… V2 ëŒ€ë¹„ Positive ë°ì´í„° ì¦ê°€: {len(df_positive) - 51830:,}ê°œ\")\n",
    "\n",
    "df_work = df_positive.copy()\n",
    "\n",
    "def split_user_interactions(group, train_r=0.7, valid_r=0.1, test_r=0.2, seed=42):\n",
    "    \"\"\"ê° userì˜ ìƒí˜¸ì‘ìš©ì„ train/valid/testë¡œ ë¶„í• \"\"\"\n",
    "    n = len(group)\n",
    "    indices = np.arange(n)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_end = int(n * train_r)\n",
    "    valid_end = train_end + int(n * valid_r)\n",
    "    \n",
    "    train_idx = indices[:train_end]\n",
    "    valid_idx = indices[train_end:valid_end]\n",
    "    test_idx = indices[valid_end:]\n",
    "    \n",
    "    return train_idx, valid_idx, test_idx\n",
    "\n",
    "# Userë³„ë¡œ split\n",
    "train_list, valid_list, test_list = [], [], []\n",
    "\n",
    "for user_id, group in df_work.groupby('user_id'):\n",
    "    train_idx, valid_idx, test_idx = split_user_interactions(\n",
    "        group, \n",
    "        CONFIG['train_ratio'], \n",
    "        CONFIG['valid_ratio'], \n",
    "        CONFIG['test_ratio'],\n",
    "        CONFIG['seed']\n",
    "    )\n",
    "    \n",
    "    train_list.append(group.iloc[train_idx])\n",
    "    valid_list.append(group.iloc[valid_idx])\n",
    "    test_list.append(group.iloc[test_idx])\n",
    "\n",
    "train_df = pd.concat(train_list, ignore_index=True)\n",
    "valid_df = pd.concat(valid_list, ignore_index=True)\n",
    "test_df = pd.concat(test_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\nPositive ë°ì´í„° split:\")\n",
    "print(f\"Train set: {len(train_df):,} ({len(train_df)/len(df_work)*100:.1f}%)\")\n",
    "print(f\"Valid set: {len(valid_df):,} ({len(valid_df)/len(df_work)*100:.1f}%)\")\n",
    "print(f\"Test set:  {len(test_df):,} ({len(test_df)/len(df_work)*100:.1f}%)\")\n",
    "\n",
    "# ì €ì¥\n",
    "train_df.to_csv(os.path.join(CONFIG['processed_dir'], 'train_split_v3.csv'), index=False)\n",
    "valid_df.to_csv(os.path.join(CONFIG['processed_dir'], 'valid_split_v3.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(CONFIG['processed_dir'], 'test_split_v3.csv'), index=False)\n",
    "\n",
    "print(\"\\nSplit ë°ì´í„° ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "n_users = len(unique_users)\n",
    "n_items = len(unique_items)\n",
    "print(f\"\\nì´ ì‚¬ìš©ì: {n_users}, ì´ ì•„ì´í…œ: {n_items}\")\n",
    "\n",
    "negative_pool = df_negative.copy()\n",
    "print(f\"Negative pool size: {len(negative_pool):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8678be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph êµ¬ì„±\n",
    "print(\"=\" * 60)\n",
    "print(\"Graph êµ¬ì„±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_graph(df, n_users, n_items):\n",
    "    \"\"\"User-Item Bipartite Graph ìƒì„±\"\"\"\n",
    "    user_ids = df['user_id'].values\n",
    "    item_ids = df['item_id'].values + n_users\n",
    "    \n",
    "    edge_index = torch.tensor([\n",
    "        np.concatenate([user_ids, item_ids]),\n",
    "        np.concatenate([item_ids, user_ids])\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    print(f\"Node ìˆ˜: {n_users + n_items} (User: {n_users}, Item: {n_items})\")\n",
    "    print(f\"Edge ìˆ˜: {edge_index.shape[1]:,} (ì–‘ë°©í–¥)\")\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "train_edge_index = create_graph(train_df, n_users, n_items)\n",
    "full_edge_index = create_graph(df, n_users, n_items)\n",
    "\n",
    "print(\"\\nGraph ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec67265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Sampling\n",
    "def create_user_item_dict(df):\n",
    "    \"\"\"Userë³„ë¡œ ìƒí˜¸ì‘ìš©í•œ item ì§‘í•© ìƒì„±\"\"\"\n",
    "    user_items = defaultdict(set)\n",
    "    for _, row in df.iterrows():\n",
    "        user_items[row['user_id']].add(row['item_id'])\n",
    "    return user_items\n",
    "\n",
    "def negative_sampling(df, user_items_dict, n_items, neg_ratio=1):\n",
    "    \"\"\"Random negative sampling\"\"\"\n",
    "    pos_users = df['user_id'].values\n",
    "    pos_items = df['item_id'].values\n",
    "    \n",
    "    neg_users = []\n",
    "    neg_items = []\n",
    "    \n",
    "    for user_id, pos_item in zip(pos_users, pos_items):\n",
    "        user_pos_items = user_items_dict[user_id]\n",
    "        \n",
    "        for _ in range(neg_ratio):\n",
    "            while True:\n",
    "                neg_item = random.randint(0, n_items - 1)\n",
    "                if neg_item not in user_pos_items:\n",
    "                    break\n",
    "            \n",
    "            neg_users.append(user_id)\n",
    "            neg_items.append(neg_item)\n",
    "    \n",
    "    return np.array(neg_users), np.array(neg_items)\n",
    "\n",
    "train_user_items = create_user_item_dict(train_df)\n",
    "valid_user_items = create_user_item_dict(pd.concat([train_df, valid_df]))\n",
    "test_user_items = create_user_item_dict(df_positive)\n",
    "\n",
    "print(f\"User-item dictionary ìƒì„± ì™„ë£Œ! (V3)\")\n",
    "print(f\"ì˜ˆì‹œ - User 0ì˜ positive items: {len(train_user_items[0])}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ccdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGCN ëª¨ë¸\n",
    "class LightGCNConv(MessagePassing):\n",
    "    \"\"\"LightGCN Convolution Layer\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "    \n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    \"\"\"LightGCN for Recommendation\"\"\"\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.1)\n",
    "        \n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, edge_index):\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        all_emb = torch.cat([user_emb, item_emb], dim=0)\n",
    "        \n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            all_emb = conv(all_emb, edge_index)\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.stack(embs, dim=0).mean(dim=0)\n",
    "        \n",
    "        user_final = final_emb[:self.n_users]\n",
    "        item_final = final_emb[self.n_users:]\n",
    "        \n",
    "        return user_final, item_final\n",
    "    \n",
    "    def predict(self, users, items, edge_index):\n",
    "        user_emb, item_emb = self.forward(edge_index)\n",
    "        user_emb = user_emb[users]\n",
    "        item_emb = item_emb[items]\n",
    "        scores = (user_emb * item_emb).sum(dim=1)\n",
    "        return scores\n",
    "\n",
    "print(\"LightGCN ëª¨ë¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPR Loss\n",
    "def bpr_loss(pos_scores, neg_scores, neg_ratio=1):\n",
    "    \"\"\"BPR Loss\"\"\"\n",
    "    if neg_ratio > 1:\n",
    "        batch_size = pos_scores.size(0)\n",
    "        neg_scores = neg_scores.view(batch_size, neg_ratio)\n",
    "        pos_scores_expanded = pos_scores.unsqueeze(1).expand_as(neg_scores)\n",
    "        loss = -torch.log(torch.sigmoid(pos_scores_expanded - neg_scores) + 1e-10).mean()\n",
    "    else:\n",
    "        loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(\"Loss í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e147f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate_model(model, edge_index, eval_df, user_items_dict, n_items, k=10, device='cpu'):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€: Precision@K, Recall@K, NDCG@K\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        user_emb, item_emb = model(edge_index.to(device))\n",
    "        \n",
    "        precisions, recalls, ndcgs = [], [], []\n",
    "        \n",
    "        for user_id, group in eval_df.groupby('user_id'):\n",
    "            true_items = set(group['item_id'].values)\n",
    "            exclude_items = user_items_dict[user_id]\n",
    "            \n",
    "            user_emb_single = user_emb[user_id].unsqueeze(0)\n",
    "            scores = torch.matmul(user_emb_single, item_emb.t()).squeeze()\n",
    "            \n",
    "            scores_np = scores.cpu().numpy()\n",
    "            for item_id in exclude_items:\n",
    "                scores_np[int(item_id)] = -np.inf\n",
    "            \n",
    "            top_k_items = np.argsort(scores_np)[-k:][::-1]\n",
    "            \n",
    "            hits = len(set(top_k_items) & true_items)\n",
    "            \n",
    "            precision = hits / k\n",
    "            recall = hits / len(true_items) if len(true_items) > 0 else 0\n",
    "            \n",
    "            dcg = sum([1 / np.log2(i + 2) for i, item in enumerate(top_k_items) if item in true_items])\n",
    "            idcg = sum([1 / np.log2(i + 2) for i in range(min(len(true_items), k))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            ndcgs.append(ndcg)\n",
    "    \n",
    "    return {\n",
    "        f'precision@{k}': np.mean(precisions),\n",
    "        f'recall@{k}': np.mean(recalls),\n",
    "        f'ndcg@{k}': np.mean(ndcgs),\n",
    "    }\n",
    "\n",
    "print(\"í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bccecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training í•¨ìˆ˜\n",
    "def train_one_epoch(model, edge_index, train_df, user_items_dict, n_items, \n",
    "                    optimizer, batch_size, neg_ratio, device):\n",
    "    \"\"\"1 epoch training\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_df_shuffled = train_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for start_idx in range(0, len(train_df_shuffled), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(train_df_shuffled))\n",
    "        batch_df = train_df_shuffled.iloc[start_idx:end_idx]\n",
    "        \n",
    "        pos_users = torch.tensor(batch_df['user_id'].values, dtype=torch.long).to(device)\n",
    "        pos_items = torch.tensor(batch_df['item_id'].values, dtype=torch.long).to(device)\n",
    "        \n",
    "        neg_users_np, neg_items_np = negative_sampling(\n",
    "            batch_df, user_items_dict, n_items, neg_ratio\n",
    "        )\n",
    "        neg_users = torch.tensor(neg_users_np, dtype=torch.long).to(device)\n",
    "        neg_items = torch.tensor(neg_items_np, dtype=torch.long).to(device)\n",
    "        \n",
    "        pos_scores = model.predict(pos_users, pos_items, edge_index.to(device))\n",
    "        neg_scores = model.predict(neg_users, neg_items, edge_index.to(device))\n",
    "        \n",
    "        loss = bpr_loss(pos_scores, neg_scores, neg_ratio)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "print(\"Training í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì¸ Training Loop (V3)\n",
    "model = LightGCN(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    embedding_dim=CONFIG['embedding_dim'],\n",
    "    n_layers=CONFIG['n_layers']\n",
    ").to(CONFIG['device'])\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'valid_precision': [],\n",
    "    'valid_recall': [],\n",
    "    'valid_ndcg': [],\n",
    "}\n",
    "\n",
    "best_recall = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training ì‹œì‘ (V3 - Reduced Model)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: LightGCN\")\n",
    "print(f\"  - Users: {n_users}, Items: {n_items}\")\n",
    "print(f\"  - Embedding dim: {CONFIG['embedding_dim']}\")\n",
    "print(f\"  - Layers: {CONFIG['n_layers']}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  - Total params: {total_params:,}\")\n",
    "print(f\"  - Train samples: {len(train_df):,}\")\n",
    "print(f\"  - Params/Data ratio: {total_params/len(train_df):.2f}x\")\n",
    "print(f\"\\nDevice: {CONFIG['device']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Weight decay: {CONFIG['weight_decay']}\")\n",
    "print(f\"Negative ratio: {CONFIG['neg_ratio']}\")\n",
    "print(f\"Patience: {CONFIG['patience']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_edge_index, train_df, train_user_items,\n",
    "        n_items, optimizer, CONFIG['batch_size'], \n",
    "        CONFIG['neg_ratio'], CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    val_metrics = evaluate_model(\n",
    "        model, train_edge_index, valid_df, train_user_items,\n",
    "        n_items, k=CONFIG['top_k'], device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['valid_precision'].append(val_metrics[f'precision@{CONFIG[\"top_k\"]}'])\n",
    "    history['valid_recall'].append(val_metrics[f'recall@{CONFIG[\"top_k\"]}'])\n",
    "    history['valid_ndcg'].append(val_metrics[f'ndcg@{CONFIG[\"top_k\"]}'])\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{CONFIG['epochs']} | \"\n",
    "              f\"Loss: {train_loss:.4f} | \"\n",
    "              f\"P@{CONFIG['top_k']}: {val_metrics[f'precision@{CONFIG[\"top_k\"]}']:.4f} | \"\n",
    "              f\"R@{CONFIG['top_k']}: {val_metrics[f'recall@{CONFIG[\"top_k\"]}']:.4f} | \"\n",
    "              f\"NDCG@{CONFIG['top_k']}: {val_metrics[f'ndcg@{CONFIG[\"top_k\"]}']:.4f}\")\n",
    "    \n",
    "    current_recall = val_metrics[f'recall@{CONFIG[\"top_k\"]}']\n",
    "    if current_recall > best_recall:\n",
    "        best_recall = current_recall\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(CONFIG['model_dir'], 'lightgcn_v3_best.pth'))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training ì™„ë£Œ! (V3)\")\n",
    "print(f\"Best Recall@{CONFIG['top_k']}: {best_recall:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Curve (V3)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['valid_precision'], label=f'Precision@{CONFIG[\"top_k\"]}', linewidth=2)\n",
    "axes[1].plot(history['valid_recall'], label=f'Recall@{CONFIG[\"top_k\"]}', linewidth=2)\n",
    "axes[1].plot(history['valid_ndcg'], label=f'NDCG@{CONFIG[\"top_k\"]}', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Validation Metrics (V3)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['result_dir'], 'training_curves_v3.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set í‰ê°€\n",
    "model.load_state_dict(torch.load(os.path.join(CONFIG['model_dir'], 'lightgcn_v3_best.pth')))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Test Set í‰ê°€ (V3)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for k in [5, 10, 20]:\n",
    "    test_metrics = evaluate_model(\n",
    "        model, full_edge_index, test_df, test_user_items,\n",
    "        n_items, k=k, device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop-{k} ì¶”ì²œ:\")\n",
    "    print(f\"  Precision@{k}: {test_metrics[f'precision@{k}']:.4f}\")\n",
    "    print(f\"  Recall@{k}:    {test_metrics[f'recall@{k}']:.4f}\")\n",
    "    print(f\"  NDCG@{k}:      {test_metrics[f'ndcg@{k}']:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80555a",
   "metadata": {},
   "source": [
    "## V3 ê°œì„ ì‚¬í•­ ìš”ì•½\n",
    "\n",
    "### ğŸ¯ í•µì‹¬ ë³€ê²½ì‚¬í•­\n",
    "\n",
    "**1. ëª¨ë¸ í¬ê¸° ëŒ€í­ ì¶•ì†Œ** â­â­â­\n",
    "- `embedding_dim`: 128 â†’ 32 (1/4ë¡œ ì¶•ì†Œ)\n",
    "- `n_layers`: 2 â†’ 1 (ë” ê°„ë‹¨í•˜ê²Œ)\n",
    "- **ì´ íŒŒë¼ë¯¸í„°**: 1,406,592 â†’ ~350,000 (75% ê°ì†Œ!)\n",
    "- **íŒŒë¼ë¯¸í„°/ë°ì´í„° ë¹„ìœ¨**: 40:1 â†’ ~10:1\n",
    "\n",
    "**2. Hyperparameter ì¡°ì •**\n",
    "- `learning_rate`: 0.0005 â†’ 0.001 (ì‘ì€ ëª¨ë¸ì€ ë” í° LR)\n",
    "- `weight_decay`: 1e-5 â†’ 1e-4 (ë” ê°•í•œ ì •ê·œí™”)\n",
    "- `batch_size`: 1024 â†’ 512\n",
    "- `neg_ratio`: 2 â†’ 1 (ê°„ë‹¨í•˜ê²Œ)\n",
    "\n",
    "**3. Rating Threshold ì™„í™”**\n",
    "- 4.0 â†’ 3.5 (ë” ë§ì€ í•™ìŠµ ë°ì´í„°)\n",
    "- Positive ë°ì´í„° ì¦ê°€ ì˜ˆìƒ\n",
    "\n",
    "### ğŸ“Š ì„±ëŠ¥ ë¹„êµ (ì˜ˆìƒ)\n",
    "\n",
    "| ë²„ì „ | Params | Params/Data | Recall@10 |\n",
    "|------|--------|-------------|----------|\n",
    "| V2   | 1.4M   | ~40:1       | 7.6%     |\n",
    "| V3   | 350K   | ~10:1       | ???      |\n",
    "\n",
    "### ğŸ’¡ ê¸°ëŒ€ íš¨ê³¼\n",
    "\n",
    "1. **ê³¼ì í•© ê°ì†Œ** - ëª¨ë¸ capacity ì¶•ì†Œ\n",
    "2. **ë” ë‚˜ì€ ì¼ë°˜í™”** - íŒŒë¼ë¯¸í„°/ë°ì´í„° ë¹„ìœ¨ ê°œì„ \n",
    "3. **ë” ë§ì€ í•™ìŠµ ë°ì´í„°** - threshold ì™„í™”\n",
    "4. **ë¹ ë¥¸ í•™ìŠµ** - ì‘ì€ ëª¨ë¸, ì‘ì€ ë°°ì¹˜\n",
    "5. **ì•ˆì •ì ì¸ ìˆ˜ë ´** - ê°•í•œ ì •ê·œí™”\n",
    "\n",
    "### ğŸ”¬ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´\n",
    "\n",
    "ë§Œì•½ V3ë„ ê°œì„ ì´ ë¶€ì¡±í•˜ë‹¤ë©´:\n",
    "- `embedding_dim`: 16ê¹Œì§€ ì¤„ì—¬ë³´ê¸°\n",
    "- `rating_threshold`: 3.0 ë˜ëŠ” ì œê±°\n",
    "- Dropout ì¶”ê°€ (0.1~0.3)\n",
    "- Matrix Factorization baseline êµ¬í˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769725b8",
   "metadata": {},
   "source": [
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "1. âœ… V3 ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n",
    "2. V2 vs V3 ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n",
    "3. ì¶”ê°€ ê°œì„  ë°©í–¥ ê²°ì •\n",
    "4. ìµœì¢… ëª¨ë¸ ì„ íƒ ë° ì œì¶œ ì¤€ë¹„"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
