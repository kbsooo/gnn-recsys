# V8 BREAKTHROUGH PLAN

## í˜„ì¬ ìƒí™© ë¶„ì„

### ê¸°ì¡´ ì‹¤í—˜ ê²°ê³¼ (V1-V5)
| ëª¨ë¸ | Recall@10 | NDCG@10 | í•µì‹¬ ì•„ì´ë””ì–´ |
|------|-----------|---------|--------------|
| V1 | 0.0000 | 0.0000 | ê¸°ë³¸ LightGCN (data leakage) |
| V2 | N/A | N/A | Rating â‰¥4.0, í° ëª¨ë¸ (í‰ê°€ ì‹¤íŒ¨) |
| V3 | 0.0786 | 0.1303 | ì‘ì€ ëª¨ë¸, rating â‰¥3.5 |
| V4 | 0.0775 | 0.1331 | neg_ratio 4 |
| V5 | 0.0784 | 0.1340 | í‘œì¤€ ì„¤ì • (emb 64, layer 2) |
| **BPR-MF** | **0.0800** | **0.1389** | Graph ì—†ëŠ” MF (ìµœê³ ) |

### í•µì‹¬ ë¬¸ì œì 
1. **ì ˆëŒ€ì  ì„±ëŠ¥ ë‚®ìŒ**: Recall@10ì´ 8% ìˆ˜ì¤€ (10ê°œ ì¶”ì²œí•´ì„œ 0.8ê°œë§Œ ë§ì¶¤)
2. **Graphê°€ ë„ì›€ ì•ˆë¨**: LightGCNì´ BPR-MFë³´ë‹¤ ë‚®ìŒ
3. **Cold-start ì˜ˆìƒ**: Long-tail ì‹¬ê° (79% ì•„ì´í…œì´ â‰¤10 interactions)
4. **Extreme sparsity**: 98.48% â†’ Message passing ì œí•œì 

---

## V8 BREAKTHROUGH ì „ëµ

### í•µì‹¬ ì•„ì´ë””ì–´: "Triple Boost Strategy"

#### ğŸš€ Boost #1: User/Item Bias Modeling
**ë¬¸ì œ**: ì‚¬ìš©ìë§ˆë‹¤ í‰ì  ìŠ¤ì¼€ì¼ì´ ë‹¤ë¦„ (ì–´ë–¤ ì‚¬ëŒì€ í›„í•˜ê²Œ, ì–´ë–¤ ì‚¬ëŒì€ ë°•í•˜ê²Œ í‰ê°€)

**í•´ê²°ì±…**:
```python
score = user_emb Â· item_emb + user_bias + item_bias + global_bias
```

**ê¸°ëŒ€ íš¨ê³¼**:
- ê°œì¸ë³„ í‰ì  ê²½í–¥ ë³´ì •
- ì¸ê¸° ì•„ì´í…œ í¸í–¥ ëª…ì‹œì  ëª¨ë¸ë§
- Matrix Factorizationì˜ í•µì‹¬ ìš”ì†Œ ì¶”ê°€

---

#### ğŸš€ Boost #2: Multi-Task Learning (Rating Regression + Ranking)
**ë¬¸ì œ**: Rating ì •ë³´ë¥¼ thresholdë¡œë§Œ ì‚¬ìš© â†’ ì •ë³´ ì†ì‹¤

**í•´ê²°ì±…**:
```python
# Task 1: Rating Regression
rating_pred = model.predict_rating(user, item)
mse_loss = (rating_pred - true_rating)^2

# Task 2: BPR Ranking
bpr_loss = -log(sigmoid(pos_score - neg_score))

# Combined
total_loss = alpha * mse_loss + (1 - alpha) * bpr_loss
```

**ê¸°ëŒ€ íš¨ê³¼**:
- Ratingì˜ ì—°ì†ì  ì •ë³´ í™œìš©
- ë” í’ë¶€í•œ í•™ìŠµ ì‹ í˜¸
- Regressionì´ representation í•™ìŠµ ë„ì›€

---

#### ğŸš€ Boost #3: Hard Negative Sampling
**ë¬¸ì œ**: Random negativeëŠ” ë„ˆë¬´ ì‰¬ì›€ â†’ í•™ìŠµ ì‹ í˜¸ ì•½í•¨

**í•´ê²°ì±…**:
```python
# Strategy 1: Low-rating items as negative (rating < 3.0)
# Strategy 2: Popular but not interacted (hard to distinguish)
# Strategy 3: Mixed (50% hard, 50% random)
```

**ê¸°ëŒ€ íš¨ê³¼**:
- ë” ì–´ë ¤ìš´ negativeë¡œ ë” ê°•í•œ í•™ìŠµ
- ì‹¤ì œ dislike ì •ë³´ í™œìš©
- Better discrimination

---

### ì¶”ê°€ ê°œì„ ì‚¬í•­

#### 4. Graph Augmentation
```python
# Training timeì— edge dropout ì ìš©
edge_dropout_rate = 0.1  # 10% edgeë¥¼ ëœë¤í•˜ê²Œ ì œê±°
# Robustness í–¥ìƒ, overfitting ë°©ì§€
```

#### 5. Attention Mechanism (ì„ íƒì )
```python
# ì¤‘ìš”í•œ neighborì— ë” ì§‘ì¤‘
# GraphSAGE-style attention ë˜ëŠ” GAT
```

#### 6. Layer Normalization
```python
# ê° layer í›„ normalization
# Training stability í–¥ìƒ
```

---

## V8 Model Architecture

```python
class LightGCN_V8(nn.Module):
    def __init__(self, n_users, n_items, embedding_dim=64, n_layers=2):
        super().__init__()

        # Embeddings
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.item_embedding = nn.Embedding(n_items, embedding_dim)

        # â­ Bias terms (Boost #1)
        self.user_bias = nn.Embedding(n_users, 1)
        self.item_bias = nn.Embedding(n_items, 1)
        self.global_bias = nn.Parameter(torch.zeros(1))

        # Graph layers
        self.convs = nn.ModuleList([LightGCNConv() for _ in range(n_layers)])

        # â­ Rating regression head (Boost #2)
        self.rating_mlp = nn.Sequential(
            nn.Linear(embedding_dim * 2, embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(embedding_dim, 1)
        )

    def forward(self, edge_index, edge_dropout=0.0):
        # Edge dropout for augmentation
        if self.training and edge_dropout > 0:
            edge_index = apply_edge_dropout(edge_index, edge_dropout)

        # Graph convolution
        user_emb, item_emb = self.graph_forward(edge_index)

        return user_emb, item_emb

    def predict_ranking(self, users, items, edge_index):
        """For BPR ranking loss"""
        user_emb, item_emb = self.forward(edge_index)
        user_emb = user_emb[users]
        item_emb = item_emb[items]

        # Dot product + bias
        scores = (user_emb * item_emb).sum(dim=1)
        scores = scores + self.user_bias(users).squeeze()
        scores = scores + self.item_bias(items).squeeze()
        scores = scores + self.global_bias

        return scores

    def predict_rating(self, users, items, edge_index):
        """For rating regression"""
        user_emb, item_emb = self.forward(edge_index)
        user_emb = user_emb[users]
        item_emb = item_emb[items]

        # MLP for rating prediction
        concat = torch.cat([user_emb, item_emb], dim=1)
        rating = self.rating_mlp(concat)

        # Add bias
        rating = rating + self.user_bias(users)
        rating = rating + self.item_bias(items)
        rating = rating + self.global_bias

        return rating.squeeze()
```

---

## V8 Training Strategy

### Loss Function
```python
def v8_loss(model, pos_users, pos_items, pos_ratings,
            neg_users, neg_items, edge_index, alpha=0.3):
    """
    alpha: weight for rating regression (0.3)
    1-alpha: weight for BPR ranking (0.7)
    """

    # Boost #2: Rating Regression
    pred_ratings = model.predict_rating(pos_users, pos_items, edge_index)
    mse_loss = F.mse_loss(pred_ratings, pos_ratings)

    # BPR Ranking
    pos_scores = model.predict_ranking(pos_users, pos_items, edge_index)
    neg_scores = model.predict_ranking(neg_users, neg_items, edge_index)
    bpr_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10).mean()

    # Combined
    total_loss = alpha * mse_loss + (1 - alpha) * bpr_loss

    return total_loss, mse_loss.item(), bpr_loss.item()
```

### Hard Negative Sampling (Boost #3)
```python
def hard_negative_sampling(batch_df, user_items_dict, n_items,
                           low_rating_items_dict, hard_ratio=0.5):
    """
    hard_ratio: 50%ëŠ” hard negative, 50%ëŠ” random negative
    """
    neg_users, neg_items = [], []

    for user_id, pos_item in zip(batch_df['user_id'], batch_df['item_id']):
        user_pos_items = user_items_dict[user_id]
        user_low_rating = low_rating_items_dict.get(user_id, set())

        # Hard negative (from low ratings)
        if random.random() < hard_ratio and len(user_low_rating) > 0:
            neg_item = random.choice(list(user_low_rating))
        # Random negative
        else:
            while True:
                neg_item = random.randint(0, n_items - 1)
                if neg_item not in user_pos_items:
                    break

        neg_users.append(user_id)
        neg_items.append(neg_item)

    return np.array(neg_users), np.array(neg_items)
```

### Hyperparameters (V8)
```python
CONFIG = {
    # Model
    'embedding_dim': 64,
    'n_layers': 2,

    # Training
    'learning_rate': 0.0005,
    'weight_decay': 1e-4,
    'batch_size': 512,
    'epochs': 100,
    'patience': 20,  # ë” ì—¬ìœ ìˆê²Œ

    # Loss weights (â­ NEW)
    'alpha': 0.3,  # Rating regression weight

    # Negative sampling (â­ NEW)
    'neg_ratio': 4,
    'hard_neg_ratio': 0.5,  # 50% hard negative

    # Augmentation (â­ NEW)
    'edge_dropout': 0.1,  # 10% edge dropout

    # Rating threshold for low ratings
    'low_rating_threshold': 3.0,  # < 3.0ì€ hard negative
    'high_rating_threshold': 3.5,  # â‰¥ 3.5ëŠ” positive
}
```

---

## Expected Performance Gain

### Baseline
- BPR-MF: Recall@10 = 0.0800, NDCG@10 = 0.1389
- LightGCN V5: Recall@10 = 0.0784, NDCG@10 = 0.1340

### Target (V8)
- **Conservative**: Recall@10 > 0.09 (+12.5% improvement)
- **Optimistic**: Recall@10 > 0.10 (+25% improvement)
- **Ambitious**: Recall@10 > 0.12 (+50% improvement)

### ê°œì„  ìš”ì¸ë³„ ê¸°ëŒ€ íš¨ê³¼
1. User/Item Bias: +1~2% (ê°œì¸ë³„ ì„ í˜¸ë„ ë³´ì •)
2. Multi-task Learning: +1~2% (rating ì •ë³´ í™œìš©)
3. Hard Negative Sampling: +1~2% (ë” ê°•í•œ í•™ìŠµ ì‹ í˜¸)
4. Graph Augmentation: +0.5~1% (robustness)
5. **Combined Synergy**: +2~3% (ì‹œë„ˆì§€ íš¨ê³¼)

**Total Expected**: +5~10% absolute improvement

---

## Implementation Plan

### Phase 1: Core Implementation
1. âœ… Breakthrough ë¶„ì„
2. â¬œ Low-rating ë°ì´í„° ì¤€ë¹„ (rating < 3.0)
3. â¬œ LightGCN_V8 ëª¨ë¸ êµ¬í˜„
4. â¬œ Multi-task loss êµ¬í˜„
5. â¬œ Hard negative sampling êµ¬í˜„

### Phase 2: Training
6. â¬œ Training loop êµ¬í˜„
7. â¬œ Validation í‰ê°€
8. â¬œ Hyperparameter tuning (alpha, hard_neg_ratio, etc.)

### Phase 3: Evaluation & Analysis
9. â¬œ Test set í‰ê°€
10. â¬œ Ablation study (ê° ìš”ì†Œë³„ ê¸°ì—¬ë„)
11. â¬œ ê²°ê³¼ ì‹œê°í™”

### Phase 4: Further Improvements (ì‹œê°„ ìˆìœ¼ë©´)
12. â¬œ Attention mechanism ì¶”ê°€
13. â¬œ Ensemble (V8 + BPR-MF)
14. â¬œ ë‹¤ì–‘í•œ neg_ratio, alpha ê°’ ì‹¤í—˜

---

## Success Criteria

### Minimum Success
- [x] V8 êµ¬í˜„ ì™„ë£Œ
- [ ] Recall@10 > 0.08 (baseline ìˆ˜ì¤€)
- [ ] NDCG@10 > 0.13 (baseline ìˆ˜ì¤€)

### Expected Success
- [ ] Recall@10 > 0.09 (+12.5%)
- [ ] NDCG@10 > 0.14 (+7%)

### Outstanding Success
- [ ] Recall@10 > 0.10 (+25%)
- [ ] NDCG@10 > 0.15 (+15%)
- [ ] Ablation studyë¡œ ê° ìš”ì†Œ ê¸°ì—¬ë„ í™•ì¸

---

## Fallback Plan

ë§Œì•½ V8ì´ í° ê°œì„ ì„ ë³´ì´ì§€ ëª»í•˜ë©´:

### Plan B: Ensemble Approach
```python
# Simple weighted average
final_score = 0.5 * lightgcn_score + 0.5 * bpr_mf_score

# Learned ensemble
ensemble_score = mlp([lightgcn_score, bpr_mf_score, popularity_score])
```

### Plan C: Re-ranking
```python
# Diversity-aware re-ranking
# Coverage-aware re-ranking
# Calibration
```

### Plan D: Different Architecture
- GraphSAGE with LSTM aggregator
- GAT with multi-head attention
- NGCF (Neural Graph Collaborative Filtering)

---

## Timeline

- **Phase 1 (2h)**: Implementation
- **Phase 2 (2h)**: Training & tuning
- **Phase 3 (1h)**: Evaluation
- **Total**: 5 hours

---

## Notes

- BPR-MFê°€ LightGCNë³´ë‹¤ ì¢‹ë‹¤ëŠ” ê²ƒì€ graph structureê°€ ì´ ë°ì´í„°ì…‹ì—ì„œ ì¶©ë¶„íˆ í™œìš©ë˜ì§€ ëª»í–ˆë‹¤ëŠ” ì˜ë¯¸
- User/Item biasëŠ” MFì˜ í•µì‹¬ ìš”ì†Œì´ë¯€ë¡œ ì¶”ê°€í•˜ë©´ LightGCNë„ MF ìˆ˜ì¤€ ì„±ëŠ¥ ê¸°ëŒ€
- Multi-task learningì€ ratingì˜ rich informationì„ í™œìš©í•˜ë¯€ë¡œ íš¨ê³¼ì ì¼ ê²ƒ
- Hard negativeëŠ” íŠ¹íˆ sparse dataì—ì„œ íš¨ê³¼ì 

---

**Created**: 2025-11-13
**Status**: Ready to implement
**Expected Completion**: 2025-11-13 EOD
