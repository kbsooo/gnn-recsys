{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA1 - Binary Classification Approach\n",
    "\n",
    "## Strategy\n",
    "- **Ground Truth**: All user-item interactions are positive (rating ignored)\n",
    "- **Task**: Predict whether a user-item pair will have an interaction\n",
    "- **Graph**: Unweighted bipartite graph\n",
    "- **Model**: LightGCN (EMB_DIM=32, N_LAYERS=2)\n",
    "- **Loss**: BPR Loss with Hard Negative Mining (50% hard + 50% random)\n",
    "- **Evaluation**: AUC-ROC, F1, Precision, Recall\n",
    "- **Recommendation**: Hybrid (Threshold + Top-K)\n",
    "\n",
    "## Expected Performance\n",
    "- AUC-ROC: 0.88 ~ 0.92\n",
    "- F1: 0.80 ~ 0.85\n",
    "- Note: May be lower than B ver (no quality consideration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device setup (CUDA > MPS > CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Device: {device} ({torch.cuda.get_device_name()})')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f'Device: {device}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "### A ver Strategy:\n",
    "- Treat ALL interactions as positive (rating value ignored)\n",
    "- Any user-item connection = positive edge\n",
    "- No rating-based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "print(f\"Total interactions: {len(df):,}\")\n",
    "print(f\"Unique users: {df['user'].nunique()}\")\n",
    "print(f\"Unique items: {df['item'].nunique()}\")\n",
    "print(f\"\\nRating distribution (for reference, not used in A ver):\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "\n",
    "# ID mapping\n",
    "user2idx = {u: i for i, u in enumerate(sorted(df['user'].unique()))}\n",
    "item2idx = {it: i for i, it in enumerate(sorted(df['item'].unique()))}\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "n_users, n_items = len(user2idx), len(item2idx)\n",
    "\n",
    "df['user_idx'] = df['user'].map(user2idx)\n",
    "df['item_idx'] = df['item'].map(item2idx)\n",
    "\n",
    "print(f\"\\nUsers: {n_users}, Items: {n_items}\")\n",
    "print(f\"Sparsity: {(1 - len(df) / (n_users * n_items)) * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User별 interaction count 및 K값 계산\n",
    "user_interaction_count = df.groupby('user_idx').size().to_dict()\n",
    "\n",
    "MAX_K = 100  # K 상한선\n",
    "\n",
    "def get_k_for_user(count):\n",
    "    \"\"\"User별 추천 개수 K 계산 (평가 규칙: 20% 이하 추천)\"\"\"\n",
    "    if count <= 10:\n",
    "        return 2  # Cold user: 무조건 2개\n",
    "    k = max(2, int(count * 0.2))\n",
    "    return min(k, MAX_K)\n",
    "\n",
    "user_k = {u: get_k_for_user(c) for u, c in user_interaction_count.items()}\n",
    "\n",
    "# 통계\n",
    "k_values = list(user_k.values())\n",
    "print(f\"User K values statistics (MAX_K={MAX_K}):\")\n",
    "print(f\"  Min K: {min(k_values)}\")\n",
    "print(f\"  Max K: {max(k_values)}\")\n",
    "print(f\"  Mean K: {np.mean(k_values):.2f}\")\n",
    "print(f\"  Median K: {np.median(k_values):.2f}\")\n",
    "\n",
    "cold_users = sum(1 for c in user_interaction_count.values() if c <= 10)\n",
    "print(f\"\\nCold users (≤10 interactions): {cold_users} ({100*cold_users/n_users:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Val/Test Split\n",
    "\n",
    "### A ver Split:\n",
    "- ALL edges are split into train/val/test (7:1.5:1.5)\n",
    "- No rating-based filtering\n",
    "- User-wise split to ensure each user has data in all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test Split (70/15/15) - User-wise\n",
    "train_data, val_data, test_data = [], [], []\n",
    "\n",
    "for user_idx in range(n_users):\n",
    "    user_df = df[df['user_idx'] == user_idx][['user_idx', 'item_idx']]\n",
    "    n_interactions = len(user_df)\n",
    "    \n",
    "    if n_interactions >= 3:\n",
    "        user_df = user_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        train_end = int(0.7 * n_interactions)\n",
    "        val_end = train_end + int(0.15 * n_interactions)\n",
    "        \n",
    "        train_end = max(1, train_end)\n",
    "        val_end = max(train_end + 1, val_end)\n",
    "        \n",
    "        train_data.append(user_df.iloc[:train_end])\n",
    "        val_data.append(user_df.iloc[train_end:val_end])\n",
    "        test_data.append(user_df.iloc[val_end:])\n",
    "    elif n_interactions == 2:\n",
    "        user_df = user_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        train_data.append(user_df.iloc[:1])\n",
    "        val_data.append(user_df.iloc[1:])\n",
    "    elif n_interactions == 1:\n",
    "        train_data.append(user_df)\n",
    "\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "val_df = pd.concat(val_data, ignore_index=True) if val_data else pd.DataFrame(columns=['user_idx', 'item_idx'])\n",
    "test_df = pd.concat(test_data, ignore_index=True) if test_data else pd.DataFrame(columns=['user_idx', 'item_idx'])\n",
    "\n",
    "print(f\"Train edges: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Val edges: {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test edges: {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Total: {len(train_df) + len(val_df) + len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-computed tensors\n",
    "train_users = torch.LongTensor(train_df['user_idx'].values)\n",
    "train_items = torch.LongTensor(train_df['item_idx'].values)\n",
    "\n",
    "# User가 train에서 선택한 items (추천 시 제외용)\n",
    "user_train_items = defaultdict(set)\n",
    "for u, i in zip(train_df['user_idx'].values, train_df['item_idx'].values):\n",
    "    user_train_items[int(u)].add(int(i))\n",
    "\n",
    "print(f\"Train tensors ready: {len(train_users):,} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Construction\n",
    "\n",
    "### A ver Graph:\n",
    "- **Unweighted** bipartite graph (all edges have equal importance)\n",
    "- Symmetric normalization: 1/√(deg(u)) * 1/√(deg(v))\n",
    "- Simple and fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unweighted_graph():\n",
    "    \"\"\"\n",
    "    Unweighted bipartite graph with symmetric normalization\n",
    "    \"\"\"\n",
    "    users = train_df['user_idx'].values\n",
    "    items = train_df['item_idx'].values\n",
    "    \n",
    "    # Bidirectional edges\n",
    "    edge_u2i = np.array([users, items + n_users])\n",
    "    edge_i2u = np.array([items + n_users, users])\n",
    "    edge_index = torch.LongTensor(np.concatenate([edge_u2i, edge_i2u], axis=1))\n",
    "    \n",
    "    # Symmetric normalization\n",
    "    num_nodes = n_users + n_items\n",
    "    deg = torch.zeros(num_nodes).scatter_add(0, edge_index[0], torch.ones(edge_index.shape[1]))\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    \n",
    "    edge_weight = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]\n",
    "    \n",
    "    return edge_index.to(device), edge_weight.to(device)\n",
    "\n",
    "edge_index, edge_weight = build_unweighted_graph()\n",
    "print(f\"Unweighted Graph: {edge_index.shape[1]:,} edges\")\n",
    "print(f\"Edge weight statistics:\")\n",
    "print(f\"  Min: {edge_weight.min().item():.4f}\")\n",
    "print(f\"  Max: {edge_weight.max().item():.4f}\")\n",
    "print(f\"  Mean: {edge_weight.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LightGCN Model\n",
    "\n",
    "### Architecture:\n",
    "- Embedding dimension: 32 (low-dimensional modeling)\n",
    "- Layers: 2 (2-hop neighbors)\n",
    "- No self-connections, no activation functions\n",
    "- Final embedding = mean of all layer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "    \n",
    "    def forward(self, edge_index, edge_weight):\n",
    "        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for _ in range(self.n_layers):\n",
    "            row, col = edge_index\n",
    "            messages = all_emb[col] * edge_weight.unsqueeze(1)\n",
    "            all_emb = torch.zeros_like(all_emb).scatter_add(\n",
    "                0, row.unsqueeze(1).expand(-1, self.emb_dim), messages\n",
    "            )\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.mean(torch.stack(embs), dim=0)\n",
    "        return final_emb[:self.n_users], final_emb[self.n_users:]\n",
    "\n",
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    \"\"\"Bayesian Personalized Ranking Loss\"\"\"\n",
    "    diff = pos_scores.unsqueeze(1) - neg_scores\n",
    "    return -torch.log(torch.sigmoid(diff) + 1e-8).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup\n",
    "\n",
    "### Negative Sampling:\n",
    "- **Hybrid approach**: 50% Hard Negatives + 50% Random\n",
    "- Hard negatives: High-scoring but actually negative items\n",
    "- Random negatives: Diversity and stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_sample_negatives(batch_size, num_neg=4):\n",
    "    \"\"\"Random negative sampling\"\"\"\n",
    "    neg_items = torch.randint(0, n_items, (batch_size, num_neg))\n",
    "    return neg_items\n",
    "\n",
    "@torch.no_grad()\n",
    "def hard_negative_sampling(user_emb, item_emb, pos_users, num_neg=4, num_candidates=50):\n",
    "    \"\"\"\n",
    "    Hard Negative Mining: Sample high-scoring negative items\n",
    "    More informative for learning than random sampling\n",
    "    \"\"\"\n",
    "    batch_size = len(pos_users)\n",
    "    \n",
    "    # Generate many candidates\n",
    "    candidates = torch.randint(0, n_items, (batch_size, num_candidates), device=device)\n",
    "    \n",
    "    # Score all candidates\n",
    "    user_expanded = user_emb[pos_users].unsqueeze(1)  # (B, 1, D)\n",
    "    item_candidates = item_emb[candidates]  # (B, num_candidates, D)\n",
    "    scores = (user_expanded * item_candidates).sum(dim=2)  # (B, num_candidates)\n",
    "    \n",
    "    # Select top-K as hard negatives\n",
    "    _, top_indices = scores.topk(num_neg, dim=1)\n",
    "    hard_negs = candidates.gather(1, top_indices)\n",
    "    \n",
    "    return hard_negs\n",
    "\n",
    "# Configuration\n",
    "USE_HARD_NEGATIVE = True\n",
    "HARD_NEG_RATIO = 0.5  # 50% hard, 50% random\n",
    "\n",
    "print(f\"Negative Sampling: {'Hard Negative Mining' if USE_HARD_NEGATIVE else 'Random'}\")\n",
    "if USE_HARD_NEGATIVE:\n",
    "    print(f\"  Hard/Random ratio: {HARD_NEG_RATIO:.0%} / {1-HARD_NEG_RATIO:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_user_wise_topk(model, eval_df, sample_users=100):\n",
    "    \"\"\"\n",
    "    User-wise Top-K evaluation\n",
    "    For each user, recommend K items and check if actual positives are included\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    u_emb, i_emb = model(edge_index, edge_weight)\n",
    "    \n",
    "    # Sample users for evaluation\n",
    "    eval_users = eval_df['user_idx'].unique()\n",
    "    if len(eval_users) > sample_users:\n",
    "        eval_users = np.random.choice(eval_users, sample_users, replace=False)\n",
    "    \n",
    "    precisions, recalls, hits = [], [], []\n",
    "    \n",
    "    for user_idx in eval_users:\n",
    "        # Actual positive items\n",
    "        actual_items = set(eval_df[eval_df['user_idx'] == user_idx]['item_idx'].values)\n",
    "        if len(actual_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Candidate items (exclude train items)\n",
    "        train_items_set = user_train_items[int(user_idx)]\n",
    "        candidate_items = [i for i in range(n_items) if i not in train_items_set]\n",
    "        if len(candidate_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Score all candidates\n",
    "        candidate_tensor = torch.LongTensor(candidate_items).to(device)\n",
    "        user_tensor = torch.full((len(candidate_items),), user_idx, dtype=torch.long, device=device)\n",
    "        scores = (u_emb[user_tensor] * i_emb[candidate_tensor]).sum(dim=1)\n",
    "        \n",
    "        # Top-K selection\n",
    "        K = user_k[int(user_idx)]\n",
    "        K = min(K, len(candidate_items))\n",
    "        \n",
    "        _, top_k_indices = torch.topk(scores, K)\n",
    "        top_k_items = set([candidate_items[idx.item()] for idx in top_k_indices])\n",
    "        \n",
    "        # Metrics\n",
    "        hits_count = len(top_k_items & actual_items)\n",
    "        precision = hits_count / K if K > 0 else 0\n",
    "        recall = hits_count / len(actual_items) if len(actual_items) > 0 else 0\n",
    "        hit = 1.0 if hits_count > 0 else 0.0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        hits.append(hit)\n",
    "    \n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EMB_DIM = 32\n",
    "N_LAYERS = 2\n",
    "LR = 5e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "NUM_NEG = 4\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  emb_dim: {EMB_DIM}\")\n",
    "print(f\"  n_layers: {N_LAYERS}\")\n",
    "print(f\"  lr: {LR}\")\n",
    "print(f\"  weight_decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  epochs: {EPOCHS}\")\n",
    "print(f\"  batch_size: {BATCH_SIZE}\")\n",
    "print(f\"  num_neg: {NUM_NEG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightGCN(n_users, n_items, EMB_DIM, N_LAYERS).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "history = {'loss': [], 'precision': [], 'recall': [], 'hit': []}\n",
    "best_recall = 0\n",
    "n_train = len(train_users)\n",
    "\n",
    "# Pre-move to device\n",
    "train_u_gpu = train_users.to(device)\n",
    "train_i_gpu = train_items.to(device)\n",
    "\n",
    "print(f\"Training CCA1 with Hard Negative Mining...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    perm = torch.randperm(n_train, device=device)\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, n_train, BATCH_SIZE):\n",
    "        batch_idx = perm[i:i+BATCH_SIZE]\n",
    "        pos_u = train_u_gpu[batch_idx]\n",
    "        pos_i = train_i_gpu[batch_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        u_emb, i_emb = model(edge_index, edge_weight)\n",
    "        \n",
    "        # Negative sampling (Hybrid)\n",
    "        if USE_HARD_NEGATIVE:\n",
    "            n_hard = int(NUM_NEG * HARD_NEG_RATIO)\n",
    "            n_random = NUM_NEG - n_hard\n",
    "            \n",
    "            if n_hard > 0:\n",
    "                hard_negs = hard_negative_sampling(u_emb, i_emb, pos_u, num_neg=n_hard)\n",
    "            else:\n",
    "                hard_negs = None\n",
    "            \n",
    "            if n_random > 0:\n",
    "                random_negs = fast_sample_negatives(len(batch_idx), n_random).to(device)\n",
    "            else:\n",
    "                random_negs = None\n",
    "            \n",
    "            if hard_negs is not None and random_negs is not None:\n",
    "                neg_i = torch.cat([hard_negs, random_negs], dim=1)\n",
    "            elif hard_negs is not None:\n",
    "                neg_i = hard_negs\n",
    "            else:\n",
    "                neg_i = random_negs\n",
    "        else:\n",
    "            neg_i = fast_sample_negatives(len(batch_idx), NUM_NEG).to(device)\n",
    "        \n",
    "        # BPR Loss\n",
    "        pos_scores = (u_emb[pos_u] * i_emb[pos_i]).sum(dim=1)\n",
    "        neg_scores = (u_emb[pos_u].unsqueeze(1) * i_emb[neg_i]).sum(dim=2)\n",
    "        \n",
    "        loss = bpr_loss(pos_scores, neg_scores)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    history['loss'].append(epoch_loss / n_batches)\n",
    "    \n",
    "    # Evaluation every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        prec, rec, hit = evaluate_user_wise_topk(model, val_df, sample_users=200)\n",
    "        history['precision'].append(prec)\n",
    "        history['recall'].append(rec)\n",
    "        history['hit'].append(hit)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {epoch_loss/n_batches:.4f} | \"\n",
    "              f\"Prec@K: {prec:.4f} | Recall@K: {rec:.4f} | Hit@K: {hit:.4f}\")\n",
    "        \n",
    "        if rec > best_recall:\n",
    "            best_recall = rec\n",
    "            torch.save(model.state_dict(), '../cc_models/cca1_best.pt')\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Best Recall@K: {best_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].plot(history['loss'], 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Training Loss', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('BPR Loss')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "epochs_val = np.arange(5, EPOCHS+1, 5)[:len(history['precision'])]\n",
    "axes[0, 1].plot(epochs_val, history['precision'], 'g-o', linewidth=2)\n",
    "axes[0, 1].set_title('Precision@K', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(epochs_val, history['recall'], 'r-o', linewidth=2)\n",
    "axes[1, 0].set_title('Recall@K', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(epochs_val, history['hit'], 'm-o', linewidth=2)\n",
    "axes[1, 1].set_title('Hit@K', fontsize=14)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Hit Rate')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../cc_docs/cca1_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validation: AUC-ROC & Threshold Tuning\n",
    "\n",
    "### Strategy:\n",
    "1. Calculate AUC-ROC (threshold-independent metric)\n",
    "2. Find optimal threshold by maximizing F1 score\n",
    "3. Use this threshold for hybrid recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('../cc_models/cca1_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_emb, i_emb = model(edge_index, edge_weight)\n",
    "\n",
    "print(\"Step 1: Calculating AUC-ROC...\")\n",
    "\n",
    "# Positive samples: validation edges\n",
    "val_u_gpu = torch.LongTensor(val_df['user_idx'].values).to(device)\n",
    "val_i_gpu = torch.LongTensor(val_df['item_idx'].values).to(device)\n",
    "val_pos_scores = (u_emb[val_u_gpu] * i_emb[val_i_gpu]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "# Negative samples: random pairs not in train/val/test\n",
    "print(\"Generating negative samples (excluding all known edges)...\")\n",
    "val_test_edges = set()\n",
    "for u, i in zip(val_df['user_idx'].values, val_df['item_idx'].values):\n",
    "    val_test_edges.add((int(u), int(i)))\n",
    "for u, i in zip(test_df['user_idx'].values, test_df['item_idx'].values):\n",
    "    val_test_edges.add((int(u), int(i)))\n",
    "\n",
    "n_neg_samples = len(val_df)\n",
    "neg_users, neg_items = [], []\n",
    "sample_count = 0\n",
    "max_attempts = n_neg_samples * 100\n",
    "attempts = 0\n",
    "\n",
    "while sample_count < n_neg_samples and attempts < max_attempts:\n",
    "    user_idx = np.random.randint(0, n_users)\n",
    "    item_idx = np.random.randint(0, n_items)\n",
    "    attempts += 1\n",
    "    \n",
    "    if item_idx not in user_train_items[user_idx] and (user_idx, item_idx) not in val_test_edges:\n",
    "        neg_users.append(user_idx)\n",
    "        neg_items.append(item_idx)\n",
    "        sample_count += 1\n",
    "\n",
    "neg_u_gpu = torch.LongTensor(neg_users).to(device)\n",
    "neg_i_gpu = torch.LongTensor(neg_items).to(device)\n",
    "val_neg_scores = (u_emb[neg_u_gpu] * i_emb[neg_i_gpu]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "# AUC-ROC\n",
    "all_scores = np.concatenate([val_pos_scores, val_neg_scores])\n",
    "all_labels = np.concatenate([np.ones(len(val_pos_scores)), np.zeros(len(val_neg_scores))])\n",
    "\n",
    "val_auc_roc = roc_auc_score(all_labels, all_scores)\n",
    "print(f\"\\nValidation AUC-ROC: {val_auc_roc:.4f}\")\n",
    "print(f\"  → 1.0 = perfect separation, 0.5 = random\")\n",
    "print(f\"\\nScore distributions:\")\n",
    "print(f\"  Positive scores: mean={val_pos_scores.mean():.4f}, std={val_pos_scores.std():.4f}\")\n",
    "print(f\"  Negative scores: mean={val_neg_scores.mean():.4f}, std={val_neg_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Threshold tuning (F1 maximization)\n",
    "print(\"Step 2: Finding optimal threshold for F1 maximization...\")\n",
    "\n",
    "thresholds = np.linspace(all_scores.min(), all_scores.max(), 100)\n",
    "best_f1, best_th = 0, 0\n",
    "best_prec, best_rec = 0, 0\n",
    "\n",
    "results_list = []\n",
    "for th in thresholds:\n",
    "    preds = (all_scores > th).astype(int)\n",
    "    prec = precision_score(all_labels, preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, preds, zero_division=0)\n",
    "    o_ratio = preds.mean()\n",
    "    \n",
    "    results_list.append({\n",
    "        'threshold': th,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'o_ratio': o_ratio\n",
    "    })\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_th = th\n",
    "        best_prec = prec\n",
    "        best_rec = rec\n",
    "\n",
    "print(f\"\\nOptimal Threshold (F1 Maximization):\")\n",
    "print(f\"  Threshold: {best_th:.4f}\")\n",
    "print(f\"  Precision: {best_prec:.4f}\")\n",
    "print(f\"  Recall: {best_rec:.4f}\")\n",
    "print(f\"  F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "preds_at_best = (all_scores > best_th).astype(int)\n",
    "o_ratio_best = preds_at_best.mean()\n",
    "print(f\"  O ratio: {o_ratio_best*100:.1f}%\")\n",
    "\n",
    "OPTIMAL_THRESHOLD = best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "results_df = pd.DataFrame(results_list)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(results_df['threshold'], results_df['precision'], 'b-', label='Precision', linewidth=2)\n",
    "axes[0].plot(results_df['threshold'], results_df['recall'], 'r-', label='Recall', linewidth=2)\n",
    "axes[0].plot(results_df['threshold'], results_df['f1'], 'g-', label='F1', linewidth=3)\n",
    "axes[0].axvline(x=best_th, color='k', linestyle='--', label=f'Best θ={best_th:.4f}')\n",
    "axes[0].set_xlabel('Threshold', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Threshold vs Metrics', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(results_df['threshold'], results_df['o_ratio'], 'm-', linewidth=2)\n",
    "axes[1].axvline(x=best_th, color='k', linestyle='--')\n",
    "axes[1].set_xlabel('Threshold', fontsize=12)\n",
    "axes[1].set_ylabel('O Ratio', fontsize=12)\n",
    "axes[1].set_title('Threshold vs O Ratio', fontsize=14)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../cc_docs/cca1_threshold_tuning.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThreshold saved: OPTIMAL_THRESHOLD = {OPTIMAL_THRESHOLD:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Top-K evaluation (reference)\n",
    "test_prec, test_rec, test_hit = evaluate_user_wise_topk(model, test_df, sample_users=500)\n",
    "\n",
    "print(\"Test Set Performance (Top-K only) - Reference:\")\n",
    "print(f\"  Precision@K: {test_prec:.4f}\")\n",
    "print(f\"  Recall@K: {test_rec:.4f}\")\n",
    "print(f\"  Hit@K: {test_hit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AUC-ROC calculation\n",
    "print(\"Calculating Test Set AUC-ROC...\")\n",
    "\n",
    "test_u_gpu = torch.LongTensor(test_df['user_idx'].values).to(device)\n",
    "test_i_gpu = torch.LongTensor(test_df['item_idx'].values).to(device)\n",
    "test_pos_scores = (u_emb[test_u_gpu] * i_emb[test_i_gpu]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "# Test negative samples\n",
    "n_test_neg = len(test_df)\n",
    "test_neg_scores = []\n",
    "for _ in range(n_test_neg):\n",
    "    user_idx = np.random.randint(0, n_users)\n",
    "    item_idx = np.random.randint(0, n_items)\n",
    "    while (item_idx in user_train_items[user_idx] or \n",
    "           (user_idx, item_idx) in val_test_edges):\n",
    "        item_idx = np.random.randint(0, n_items)\n",
    "    score = (u_emb[user_idx] * i_emb[item_idx]).sum().item()\n",
    "    test_neg_scores.append(score)\n",
    "\n",
    "test_neg_scores = np.array(test_neg_scores)\n",
    "\n",
    "test_all_scores = np.concatenate([test_pos_scores, test_neg_scores])\n",
    "test_all_labels = np.concatenate([np.ones(len(test_pos_scores)), np.zeros(len(test_neg_scores))])\n",
    "\n",
    "test_auc_roc = roc_auc_score(test_all_labels, test_all_scores)\n",
    "\n",
    "print(f\"\\nTest Set AUC-ROC: {test_auc_roc:.4f}\")\n",
    "print(f\"Validation AUC-ROC: {val_auc_roc:.4f}\")\n",
    "\n",
    "# Test metrics with optimal threshold\n",
    "test_preds = (test_all_scores > OPTIMAL_THRESHOLD).astype(int)\n",
    "test_acc = (test_preds == test_all_labels).mean()\n",
    "test_prec_th = precision_score(test_all_labels, test_preds, zero_division=0)\n",
    "test_rec_th = recall_score(test_all_labels, test_preds, zero_division=0)\n",
    "test_f1 = f1_score(test_all_labels, test_preds, zero_division=0)\n",
    "test_o_ratio = test_preds.mean()\n",
    "\n",
    "print(f\"\\nTest Set with Threshold={OPTIMAL_THRESHOLD:.4f}:\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Precision: {test_prec_th:.4f}\")\n",
    "print(f\"  Recall: {test_rec_th:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "print(f\"  O Ratio: {test_o_ratio*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hybrid Recommendation (Threshold + Top-K)\n",
    "\n",
    "### Strategy:\n",
    "1. Filter items with score > threshold\n",
    "2. Among filtered items, select Top-K\n",
    "3. K varies by user (20% of interactions, min 2)\n",
    "4. Satisfies the evaluation rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hybrid(test_input_df, threshold):\n",
    "    \"\"\"\n",
    "    Hybrid recommendation: Threshold + Top-K\n",
    "    1. Score > threshold items only\n",
    "    2. Among them, select Top-K (0 to K items)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        u_emb, i_emb = model(edge_index, edge_weight)\n",
    "\n",
    "    results = []\n",
    "    stats = {'total_o': 0, 'total_items': 0, 'users_with_o': 0}\n",
    "\n",
    "    for user in test_input_df['user'].unique():\n",
    "        if user not in user2idx:\n",
    "            # Unknown user: all X\n",
    "            user_rows = test_input_df[test_input_df['user'] == user]\n",
    "            for _, row in user_rows.iterrows():\n",
    "                results.append({'user': row['user'], 'item': row['item'], 'recommend': 'X'})\n",
    "                stats['total_items'] += 1\n",
    "            continue\n",
    "\n",
    "        user_idx = user2idx[user]\n",
    "        user_rows = test_input_df[test_input_df['user'] == user]\n",
    "        train_items_set = user_train_items[user_idx]\n",
    "\n",
    "        items_to_score = []\n",
    "        item_info = []\n",
    "\n",
    "        for _, row in user_rows.iterrows():\n",
    "            item = row['item']\n",
    "            if item not in item2idx:\n",
    "                results.append({'user': user, 'item': item, 'recommend': 'X'})\n",
    "                stats['total_items'] += 1\n",
    "            elif item2idx[item] in train_items_set:\n",
    "                results.append({'user': user, 'item': item, 'recommend': 'X'})\n",
    "                stats['total_items'] += 1\n",
    "            else:\n",
    "                items_to_score.append(item2idx[item])\n",
    "                item_info.append(item)\n",
    "\n",
    "        if len(items_to_score) == 0:\n",
    "            continue\n",
    "\n",
    "        # Score items\n",
    "        item_tensor = torch.LongTensor(items_to_score).to(device)\n",
    "        user_tensor = torch.full((len(items_to_score),), user_idx, dtype=torch.long, device=device)\n",
    "        scores = (u_emb[user_tensor] * i_emb[item_tensor]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "        # Hybrid selection\n",
    "        above_threshold_mask = scores > threshold\n",
    "        above_threshold_indices = np.where(above_threshold_mask)[0]\n",
    "\n",
    "        K = user_k[user_idx]\n",
    "\n",
    "        if len(above_threshold_indices) > K:\n",
    "            above_scores = scores[above_threshold_indices]\n",
    "            top_k_in_above = np.argsort(above_scores)[-K:]\n",
    "            selected_indices = set(above_threshold_indices[top_k_in_above])\n",
    "        else:\n",
    "            selected_indices = set(above_threshold_indices)\n",
    "\n",
    "        user_o_count = 0\n",
    "        for i, item in enumerate(item_info):\n",
    "            if i in selected_indices:\n",
    "                recommend = 'O'\n",
    "                user_o_count += 1\n",
    "            else:\n",
    "                recommend = 'X'\n",
    "            results.append({'user': user, 'item': item, 'recommend': recommend})\n",
    "            stats['total_items'] += 1\n",
    "\n",
    "        stats['total_o'] += user_o_count\n",
    "        if user_o_count > 0:\n",
    "            stats['users_with_o'] += 1\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    o_ratio = stats['total_o'] / stats['total_items'] if stats['total_items'] > 0 else 0\n",
    "    print(f\"Hybrid Prediction Stats:\")\n",
    "    print(f\"  Total items: {stats['total_items']}\")\n",
    "    print(f\"  Total O: {stats['total_o']}\")\n",
    "    print(f\"  O ratio: {o_ratio*100:.2f}%\")\n",
    "    print(f\"  Users with ≥1 O: {stats['users_with_o']}\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Prediction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample1.csv (has rating column)\n",
    "sample1 = pd.read_csv('../data/sample1.csv')\n",
    "\n",
    "print(\"Sample1.csv Test (A ver expects all O since all have ratings):\")\n",
    "print(f\"Threshold: {OPTIMAL_THRESHOLD:.4f}\")\n",
    "predictions1 = predict_hybrid(sample1, OPTIMAL_THRESHOLD)\n",
    "print(f\"\\nPredictions:\")\n",
    "print(predictions1)\n",
    "print(f\"\\nExpected for A ver: OOOOO (all interactions are positive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample2.csv (no rating column)\n",
    "sample2 = pd.read_csv('../data/sample2.csv')\n",
    "\n",
    "print(\"Sample2.csv Test:\")\n",
    "print(f\"Threshold: {OPTIMAL_THRESHOLD:.4f}\")\n",
    "predictions2 = predict_hybrid(sample2, OPTIMAL_THRESHOLD)\n",
    "print(f\"\\nPredictions:\")\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CCA1 - Binary Classification Approach (A ver)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel Config:\")\n",
    "print(f\"  Architecture: LightGCN\")\n",
    "print(f\"  Embedding dim: {EMB_DIM}\")\n",
    "print(f\"  Layers: {N_LAYERS}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nStrategy:\")\n",
    "print(f\"  Ground Truth: All interactions (rating ignored)\")\n",
    "print(f\"  Graph: Unweighted bipartite\")\n",
    "print(f\"  Negative Sampling: Hard ({HARD_NEG_RATIO:.0%}) + Random ({1-HARD_NEG_RATIO:.0%})\")\n",
    "print(f\"  Loss: BPR Loss\")\n",
    "\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Total edges: {len(df):,}\")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Val: {len(val_df):,}\")\n",
    "print(f\"  Test: {len(test_df):,}\")\n",
    "\n",
    "print(f\"\\nOptimal Threshold (F1 Maximization):\")\n",
    "print(f\"  Threshold: {OPTIMAL_THRESHOLD:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  AUC-ROC: {test_auc_roc:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Precision: {test_prec_th:.4f}\")\n",
    "print(f\"  Recall: {test_rec_th:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nModel saved to: ../cc_models/cca1_best.pt\")\n",
    "print(f\"Threshold for inference: {OPTIMAL_THRESHOLD:.4f}\")\n",
    "print(\"\\nReady for final test set prediction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Test Inference (Uncomment when ready)\n",
    "\n",
    "```python\n",
    "# When professor provides test.csv:\n",
    "final_test = pd.read_csv('../data/test.csv')\n",
    "print(f\"Using threshold: {OPTIMAL_THRESHOLD:.4f}\")\n",
    "final_predictions = predict_hybrid(final_test, OPTIMAL_THRESHOLD)\n",
    "final_predictions.to_csv('../cc/cca1_predictions.csv', index=False)\n",
    "print(f\"\\nPredictions saved to ../cc/cca1_predictions.csv\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
