{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCN V8b - Rating-aware Hybrid Recommendation\n",
    "\n",
    "## 핵심 변경사항 (V8 대비)\n",
    "- **Graph**: 모든 구매를 edge로 사용 (V8과 동일)\n",
    "- **Ground Truth**: Rating >= 4만 positive (좋은 구매만 맞추면 정답)\n",
    "- **추천 제외**: 이미 구매한 모든 item (rating 무관)\n",
    "- **Hybrid 추천**: Threshold + Top-K (0~K개 추천)\n",
    "- **AUC-ROC 평가**: 좋은 구매 vs 비구매 구분 능력\n",
    "- GPU 최적화 유지 (MPS/CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# CUDA 우선\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Device: {device} ({torch.cuda.get_device_name()})')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f'Device: {device}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리 (Rating 무시, Edge 존재만 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions: 105,139\n",
      "Unique users: 668\n",
      "Unique items: 10321\n",
      "\n",
      "Rating distribution:\n",
      "rating\n",
      "0.5     1189\n",
      "1.0     3254\n",
      "1.5     1564\n",
      "2.0     7929\n",
      "2.5     5473\n",
      "3.0    21676\n",
      "3.5    12224\n",
      "4.0    28831\n",
      "4.5     8174\n",
      "5.0    14825\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Good purchases (rating >= 4): 51,830 (49.3%)\n",
      "\n",
      "Users: 668, Items: 10321\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# Rating 정보 포함\n",
    "print(f\"Total interactions: {len(df):,}\")\n",
    "print(f\"Unique users: {df['user'].nunique()}\")\n",
    "print(f\"Unique items: {df['item'].nunique()}\")\n",
    "\n",
    "# Rating 분포 확인\n",
    "print(f\"\\nRating distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "\n",
    "# 좋은 구매 정의: Rating >= 4\n",
    "GOOD_RATING_THRESHOLD = 4\n",
    "n_good_purchases = (df['rating'] >= GOOD_RATING_THRESHOLD).sum()\n",
    "print(f\"\\nGood purchases (rating >= {GOOD_RATING_THRESHOLD}): {n_good_purchases:,} ({100*n_good_purchases/len(df):.1f}%)\")\n",
    "\n",
    "# ID 매핑\n",
    "user2idx = {u: i for i, u in enumerate(sorted(df['user'].unique()))}\n",
    "item2idx = {it: i for i, it in enumerate(sorted(df['item'].unique()))}\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "n_users, n_items = len(user2idx), len(item2idx)\n",
    "\n",
    "df['user_idx'] = df['user'].map(user2idx)\n",
    "df['item_idx'] = df['item'].map(item2idx)\n",
    "\n",
    "print(f\"\\nUsers: {n_users}, Items: {n_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User K values statistics (MAX_K=100):\n",
      "  Min K: 4\n",
      "  Max K: 100\n",
      "  Mean K: 24.74\n",
      "  Median K: 14.00\n",
      "\n",
      "Cold users (≤10 interactions): 0 (0.0%)\n",
      "Users with K=MAX_K: 41 (6.1%)\n"
     ]
    }
   ],
   "source": [
    "# User별 interaction count 및 K값 계산\n",
    "user_interaction_count = df.groupby('user_idx').size().to_dict()\n",
    "\n",
    "MAX_K = 100  # K 상한선 (너무 많은 추천 방지)\n",
    "\n",
    "def get_k_for_user(count):\n",
    "    \"\"\"User별 추천 개수 K 계산 (상한선 적용)\"\"\"\n",
    "    if count <= 10:\n",
    "        return 2  # Cold user: 무조건 2개\n",
    "    k = max(2, int(count * 0.2))  # 20% of interactions\n",
    "    return min(k, MAX_K)  # 상한선 적용\n",
    "\n",
    "user_k = {u: get_k_for_user(c) for u, c in user_interaction_count.items()}\n",
    "\n",
    "# 통계\n",
    "k_values = list(user_k.values())\n",
    "print(f\"User K values statistics (MAX_K={MAX_K}):\")\n",
    "print(f\"  Min K: {min(k_values)}\")\n",
    "print(f\"  Max K: {max(k_values)}\")\n",
    "print(f\"  Mean K: {np.mean(k_values):.2f}\")\n",
    "print(f\"  Median K: {np.median(k_values):.2f}\")\n",
    "\n",
    "# Cold users (≤10 interactions)\n",
    "cold_users = sum(1 for c in user_interaction_count.values() if c <= 10)\n",
    "print(f\"\\nCold users (≤10 interactions): {cold_users} ({100*cold_users/n_users:.1f}%)\")\n",
    "\n",
    "# Users hitting MAX_K\n",
    "capped_users = sum(1 for u, k in user_k.items() if k == MAX_K)\n",
    "print(f\"Users with K=MAX_K: {capped_users} ({100*capped_users/n_users:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges (all purchases for graph): 89,294\n",
      "Val edges (good purchases only): 7,480\n",
      "Test edges (good purchases only): 8,365\n",
      "\n",
      "Good purchases in Val/Test: 15,845 / 51,830 (30.6%)\n"
     ]
    }
   ],
   "source": [
    "# Train/Val/Test Split (70/15/15)\n",
    "# 핵심 변경: Rating >= 4인 edge만 Val/Test로 분할\n",
    "# Rating < 4인 edge는 모두 Train에 포함 (그래프 학습용)\n",
    "\n",
    "train_data, val_data, test_data = [], [], []\n",
    "\n",
    "for user_idx in range(n_users):\n",
    "    user_df = df[df['user_idx'] == user_idx]\n",
    "    \n",
    "    # 좋은 구매 (rating >= 4)와 나쁜 구매 분리\n",
    "    good_purchases = user_df[user_df['rating'] >= GOOD_RATING_THRESHOLD][['user_idx', 'item_idx', 'rating']]\n",
    "    bad_purchases = user_df[user_df['rating'] < GOOD_RATING_THRESHOLD][['user_idx', 'item_idx', 'rating']]\n",
    "    \n",
    "    # 나쁜 구매는 모두 Train에 포함 (그래프 학습용, 평가 X)\n",
    "    if len(bad_purchases) > 0:\n",
    "        train_data.append(bad_purchases[['user_idx', 'item_idx']])\n",
    "    \n",
    "    # 좋은 구매만 Train/Val/Test로 분할\n",
    "    n_good = len(good_purchases)\n",
    "    \n",
    "    if n_good >= 3:\n",
    "        good_purchases = good_purchases.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        train_end = int(0.7 * n_good)\n",
    "        val_end = train_end + int(0.15 * n_good)\n",
    "        \n",
    "        train_end = max(1, train_end)\n",
    "        val_end = max(train_end + 1, val_end)\n",
    "        \n",
    "        train_data.append(good_purchases.iloc[:train_end][['user_idx', 'item_idx']])\n",
    "        val_data.append(good_purchases.iloc[train_end:val_end][['user_idx', 'item_idx']])\n",
    "        test_data.append(good_purchases.iloc[val_end:][['user_idx', 'item_idx']])\n",
    "    elif n_good == 2:\n",
    "        good_purchases = good_purchases.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        train_data.append(good_purchases.iloc[:1][['user_idx', 'item_idx']])\n",
    "        val_data.append(good_purchases.iloc[1:][['user_idx', 'item_idx']])\n",
    "    elif n_good == 1:\n",
    "        train_data.append(good_purchases[['user_idx', 'item_idx']])\n",
    "    # n_good == 0: 좋은 구매 없음, train graph에만 나쁜 구매 포함\n",
    "\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "val_df = pd.concat(val_data, ignore_index=True) if val_data else pd.DataFrame(columns=['user_idx', 'item_idx'])\n",
    "test_df = pd.concat(test_data, ignore_index=True) if test_data else pd.DataFrame(columns=['user_idx', 'item_idx'])\n",
    "\n",
    "print(f\"Train edges (all purchases for graph): {len(train_df):,}\")\n",
    "print(f\"Val edges (good purchases only): {len(val_df):,}\")\n",
    "print(f\"Test edges (good purchases only): {len(test_df):,}\")\n",
    "\n",
    "# 통계\n",
    "n_total_good = n_good_purchases\n",
    "n_val_test = len(val_df) + len(test_df)\n",
    "print(f\"\\nGood purchases in Val/Test: {n_val_test:,} / {n_total_good:,} ({100*n_val_test/n_total_good:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tensors ready: 89,294 edges\n"
     ]
    }
   ],
   "source": [
    "# Pre-computed tensors\n",
    "train_users = torch.LongTensor(train_df['user_idx'].values)\n",
    "train_items = torch.LongTensor(train_df['item_idx'].values)\n",
    "\n",
    "# User가 train에서 선택한 items (추천에서 제외용)\n",
    "user_train_items = defaultdict(set)\n",
    "for u, i in zip(train_df['user_idx'].values, train_df['item_idx'].values):\n",
    "    user_train_items[int(u)].add(int(i))\n",
    "\n",
    "print(f\"Train tensors ready: {len(train_users):,} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 178,588 edges\n"
     ]
    }
   ],
   "source": [
    "# Graph 구축 (Train edges only)\n",
    "def build_graph():\n",
    "    users = train_df['user_idx'].values\n",
    "    items = train_df['item_idx'].values\n",
    "    edge_u2i = np.array([users, items + n_users])\n",
    "    edge_i2u = np.array([items + n_users, users])\n",
    "    edge_index = torch.LongTensor(np.concatenate([edge_u2i, edge_i2u], axis=1))\n",
    "    \n",
    "    num_nodes = n_users + n_items\n",
    "    deg = torch.zeros(num_nodes).scatter_add(0, edge_index[0], torch.ones(edge_index.shape[1]))\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    edge_weight = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]\n",
    "    \n",
    "    return edge_index.to(device), edge_weight.to(device)\n",
    "\n",
    "edge_index, edge_weight = build_graph()\n",
    "print(f\"Graph: {edge_index.shape[1]:,} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGCN 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "    \n",
    "    def forward(self, edge_index, edge_weight):\n",
    "        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for _ in range(self.n_layers):\n",
    "            row, col = edge_index\n",
    "            messages = all_emb[col] * edge_weight.unsqueeze(1)\n",
    "            all_emb = torch.zeros_like(all_emb).scatter_add(0, row.unsqueeze(1).expand(-1, self.emb_dim), messages)\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.mean(torch.stack(embs), dim=0)\n",
    "        return final_emb[:self.n_users], final_emb[self.n_users:]\n",
    "\n",
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    diff = pos_scores.unsqueeze(1) - neg_scores\n",
    "    return -torch.log(torch.sigmoid(diff) + 1e-8).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_sample_negatives(batch_size, num_neg=4):\n",
    "    \"\"\"Vectorized negative sampling\"\"\"\n",
    "    neg_items = torch.randint(0, n_items, (batch_size, num_neg))\n",
    "    return neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_user_wise_topk(model, eval_df, sample_users=100):\n",
    "    \"\"\"\n",
    "    User-wise Top-K 평가\n",
    "    각 user별로 K개의 추천을 하고, 실제 positive가 포함되었는지 확인\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    u_emb, i_emb = model(edge_index, edge_weight)\n",
    "    \n",
    "    # Sample users for evaluation\n",
    "    eval_users = eval_df['user_idx'].unique()\n",
    "    if len(eval_users) > sample_users:\n",
    "        eval_users = np.random.choice(eval_users, sample_users, replace=False)\n",
    "    \n",
    "    precisions, recalls, hits = [], [], []\n",
    "    \n",
    "    for user_idx in eval_users:\n",
    "        # User의 실제 positive items (val/test)\n",
    "        actual_items = set(eval_df[eval_df['user_idx'] == user_idx]['item_idx'].values)\n",
    "        if len(actual_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # User가 train에서 이미 선택한 items (제외)\n",
    "        train_items_set = user_train_items[int(user_idx)]\n",
    "        \n",
    "        # 후보: train에서 선택하지 않은 모든 items\n",
    "        candidate_items = [i for i in range(n_items) if i not in train_items_set]\n",
    "        if len(candidate_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Score all candidates\n",
    "        candidate_tensor = torch.LongTensor(candidate_items).to(device)\n",
    "        user_tensor = torch.full((len(candidate_items),), user_idx, dtype=torch.long, device=device)\n",
    "        scores = (u_emb[user_tensor] * i_emb[candidate_tensor]).sum(dim=1)\n",
    "        \n",
    "        # Top-K selection\n",
    "        K = user_k[int(user_idx)]\n",
    "        K = min(K, len(candidate_items))  # K가 후보보다 클 수 없음\n",
    "        \n",
    "        _, top_k_indices = torch.topk(scores, K)\n",
    "        top_k_items = set([candidate_items[idx.item()] for idx in top_k_indices])\n",
    "        \n",
    "        # Metrics\n",
    "        hits_count = len(top_k_items & actual_items)\n",
    "        precision = hits_count / K if K > 0 else 0\n",
    "        recall = hits_count / len(actual_items) if len(actual_items) > 0 else 0\n",
    "        hit = 1.0 if hits_count > 0 else 0.0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        hits.append(hit)\n",
    "    \n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config:\n",
      "  emb_dim: 32\n",
      "  n_layers: 2\n",
      "  lr: 0.005\n",
      "  weight_decay: 1e-05\n",
      "  epochs: 50\n"
     ]
    }
   ],
   "source": [
    "# 학습 설정\n",
    "EMB_DIM = 32\n",
    "N_LAYERS = 2\n",
    "LR = 5e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "NUM_NEG = 4\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  emb_dim: {EMB_DIM}\")\n",
    "print(f\"  n_layers: {N_LAYERS}\")\n",
    "print(f\"  lr: {LR}\")\n",
    "print(f\"  weight_decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "======================================================================\n",
      "Epoch   5 | Loss: 0.2814 | Prec@K: 0.0421 | Recall@K: 0.1078 | Hit@K: 0.4400\n",
      "Epoch  10 | Loss: 0.2501 | Prec@K: 0.0544 | Recall@K: 0.1321 | Hit@K: 0.4900\n",
      "Epoch  15 | Loss: 0.2331 | Prec@K: 0.0547 | Recall@K: 0.1307 | Hit@K: 0.4850\n",
      "Epoch  20 | Loss: 0.2216 | Prec@K: 0.0542 | Recall@K: 0.1326 | Hit@K: 0.5300\n",
      "Epoch  25 | Loss: 0.2151 | Prec@K: 0.0640 | Recall@K: 0.1634 | Hit@K: 0.5250\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(n_users, n_items, EMB_DIM, N_LAYERS).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "history = {'loss': [], 'precision': [], 'recall': [], 'hit': []}\n",
    "best_recall = 0\n",
    "n_train = len(train_users)\n",
    "\n",
    "# Pre-move to GPU\n",
    "train_u_gpu = train_users.to(device)\n",
    "train_i_gpu = train_items.to(device)\n",
    "\n",
    "print(f\"Training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    perm = torch.randperm(n_train, device=device)\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, n_train, BATCH_SIZE):\n",
    "        batch_idx = perm[i:i+BATCH_SIZE]\n",
    "        pos_u = train_u_gpu[batch_idx]\n",
    "        pos_i = train_i_gpu[batch_idx]\n",
    "        neg_i = fast_sample_negatives(len(batch_idx), NUM_NEG).to(device)\n",
    "        \n",
    "        u_emb, i_emb = model(edge_index, edge_weight)\n",
    "        pos_scores = (u_emb[pos_u] * i_emb[pos_i]).sum(dim=1)\n",
    "        neg_scores = (u_emb[pos_u].unsqueeze(1) * i_emb[neg_i]).sum(dim=2)\n",
    "        \n",
    "        loss = bpr_loss(pos_scores, neg_scores)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    history['loss'].append(epoch_loss / n_batches)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        prec, rec, hit = evaluate_user_wise_topk(model, val_df, sample_users=200)\n",
    "        history['precision'].append(prec)\n",
    "        history['recall'].append(rec)\n",
    "        history['hit'].append(hit)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {epoch_loss/n_batches:.4f} | \"\n",
    "              f\"Prec@K: {prec:.4f} | Recall@K: {rec:.4f} | Hit@K: {hit:.4f}\")\n",
    "        \n",
    "        if rec > best_recall:\n",
    "            best_recall = rec\n",
    "            torch.save(model.state_dict(), 'best_lightgcn_v8b.pt')\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Best Recall@K: {best_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].plot(history['loss'], 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "epochs_val = np.arange(5, EPOCHS+1, 5)[:len(history['precision'])]\n",
    "axes[0, 1].plot(epochs_val, history['precision'], 'g-o', linewidth=2)\n",
    "axes[0, 1].set_title('Precision@K')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(epochs_val, history['recall'], 'r-o', linewidth=2)\n",
    "axes[1, 0].set_title('Recall@K')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(epochs_val, history['hit'], 'm-o', linewidth=2)\n",
    "axes[1, 1].set_title('Hit@K')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Set 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_lightgcn_v8b.pt'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_emb, i_emb = model(edge_index, edge_weight)\n",
    "\n",
    "# Test set 전체 평가 (기존 Top-K only) - 참고용\n",
    "test_prec, test_rec, test_hit = evaluate_user_wise_topk(model, test_df, sample_users=500)\n",
    "\n",
    "print(\"Test Set Performance (User-wise Top-K only) - For reference:\")\n",
    "print(f\"  Precision@K: {test_prec:.4f}\")\n",
    "print(f\"  Recall@K: {test_rec:.4f}\")\n",
    "print(f\"  Hit@K: {test_hit:.4f}\")\n",
    "print(\"  → 무조건 K개 추천했을 때의 성능\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC 평가 + Threshold 튜닝\n",
    "print(\"Step 1: Calculating AUC-ROC...\")\n",
    "\n",
    "# Positive samples: val edges\n",
    "val_u_gpu = torch.LongTensor(val_df['user_idx'].values).to(device)\n",
    "val_i_gpu = torch.LongTensor(val_df['item_idx'].values).to(device)\n",
    "val_pos_scores = (u_emb[val_u_gpu] * i_emb[val_i_gpu]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "# Negative samples: random (user, item) pairs not in train/val/test\n",
    "print(\"Generating negative samples (excluding train/val/test edges)...\")\n",
    "val_test_edges = set()\n",
    "for u, i in zip(val_df['user_idx'].values, val_df['item_idx'].values):\n",
    "    val_test_edges.add((int(u), int(i)))\n",
    "for u, i in zip(test_df['user_idx'].values, test_df['item_idx'].values):\n",
    "    val_test_edges.add((int(u), int(i)))\n",
    "\n",
    "n_neg_samples = len(val_df)\n",
    "neg_users, neg_items = [], []\n",
    "sample_count = 0\n",
    "while sample_count < n_neg_samples:\n",
    "    user_idx = np.random.randint(0, n_users)\n",
    "    item_idx = np.random.randint(0, n_items)\n",
    "    # Exclude train, val, test edges\n",
    "    if item_idx not in user_train_items[user_idx] and (user_idx, item_idx) not in val_test_edges:\n",
    "        neg_users.append(user_idx)\n",
    "        neg_items.append(item_idx)\n",
    "        sample_count += 1\n",
    "\n",
    "neg_u_gpu = torch.LongTensor(neg_users).to(device)\n",
    "neg_i_gpu = torch.LongTensor(neg_items).to(device)\n",
    "val_neg_scores = (u_emb[neg_u_gpu] * i_emb[neg_i_gpu]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "# AUC-ROC\n",
    "all_scores = np.concatenate([val_pos_scores, val_neg_scores])\n",
    "all_labels = np.concatenate([np.ones(len(val_pos_scores)), np.zeros(len(val_neg_scores))])\n",
    "\n",
    "val_auc_roc = roc_auc_score(all_labels, all_scores)\n",
    "print(f\"\\nValidation AUC-ROC: {val_auc_roc:.4f}\")\n",
    "print(f\"  → 1.0 = 완벽한 구분, 0.5 = 랜덤\")\n",
    "print(f\"  → 모델이 O/X를 얼마나 잘 구분하는지 측정\")\n",
    "\n",
    "# Score 분포 확인\n",
    "print(f\"\\nScore distributions:\")\n",
    "print(f\"  Positive (val) scores: mean={val_pos_scores.mean():.4f}, std={val_pos_scores.std():.4f}\")\n",
    "print(f\"  Negative scores: mean={val_neg_scores.mean():.4f}, std={val_neg_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Threshold 튜닝 (F1 최대화)\n",
    "print(\"Step 2: Finding optimal threshold for F1 maximization...\")\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 다양한 threshold 시도\n",
    "thresholds = np.linspace(all_scores.min(), all_scores.max(), 100)\n",
    "best_f1, best_th = 0, 0\n",
    "best_prec, best_rec = 0, 0\n",
    "\n",
    "results_list = []\n",
    "for th in thresholds:\n",
    "    preds = (all_scores > th).astype(int)\n",
    "    prec = precision_score(all_labels, preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, preds, zero_division=0)\n",
    "    o_ratio = preds.mean()\n",
    "    \n",
    "    results_list.append({\n",
    "        'threshold': th,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'o_ratio': o_ratio\n",
    "    })\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_th = th\n",
    "        best_prec = prec\n",
    "        best_rec = rec\n",
    "\n",
    "print(f\"\\nOptimal Threshold (F1 Maximization):\")\n",
    "print(f\"  Threshold: {best_th:.4f}\")\n",
    "print(f\"  Precision: {best_prec:.4f}\")\n",
    "print(f\"  Recall: {best_rec:.4f}\")\n",
    "print(f\"  F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "# O ratio at best threshold\n",
    "preds_at_best = (all_scores > best_th).astype(int)\n",
    "o_ratio_best = preds_at_best.mean()\n",
    "print(f\"  O ratio: {o_ratio_best*100:.1f}%\")\n",
    "\n",
    "# 시각화\n",
    "results_df = pd.DataFrame(results_list)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(results_df['threshold'], results_df['precision'], 'b-', label='Precision', linewidth=2)\n",
    "axes[0].plot(results_df['threshold'], results_df['recall'], 'r-', label='Recall', linewidth=2)\n",
    "axes[0].plot(results_df['threshold'], results_df['f1'], 'g-', label='F1', linewidth=3)\n",
    "axes[0].axvline(x=best_th, color='k', linestyle='--', label=f'Best θ={best_th:.4f}')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Threshold vs Metrics')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(results_df['threshold'], results_df['o_ratio'], 'm-', linewidth=2)\n",
    "axes[1].axvline(x=best_th, color='k', linestyle='--')\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('O Ratio')\n",
    "axes[1].set_title('Threshold vs O Ratio')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save threshold\n",
    "OPTIMAL_THRESHOLD = best_th\n",
    "print(f\"\\nThreshold saved: OPTIMAL_THRESHOLD = {OPTIMAL_THRESHOLD:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hybrid(test_input_df, threshold):\n",
    "    \"\"\"\n",
    "    Hybrid 추천: Threshold + Top-K 제약\n",
    "    1. Score > threshold인 item만 후보로 선택\n",
    "    2. 후보 중 Top-K만 O (0~K개 추천)\n",
    "\n",
    "    이렇게 하면:\n",
    "    - Threshold를 넘지 못하면 추천 안함 (O 남발 방지)\n",
    "    - 동시에 K개 이하로 제한 (과다 추천 방지)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        u_emb, i_emb = model(edge_index, edge_weight)\n",
    "\n",
    "    results = []\n",
    "    stats = {'total_o': 0, 'total_items': 0, 'users_with_o': 0}\n",
    "\n",
    "    # User별로 처리\n",
    "    for user in test_input_df['user'].unique():\n",
    "        if user not in user2idx:\n",
    "            # Unknown user: 모두 X\n",
    "            user_rows = test_input_df[test_input_df['user'] == user]\n",
    "            for _, row in user_rows.iterrows():\n",
    "                results.append({'user': row['user'], 'item': row['item'], 'recommend': 'X'})\n",
    "                stats['total_items'] += 1\n",
    "            continue\n",
    "\n",
    "        user_idx = user2idx[user]\n",
    "        user_rows = test_input_df[test_input_df['user'] == user]\n",
    "\n",
    "        # User가 이미 선택한 items\n",
    "        train_items_set = user_train_items[user_idx]\n",
    "\n",
    "        # Test에서 주어진 items scoring\n",
    "        items_to_score = []\n",
    "        item_info = []  # (original_item, item_idx)\n",
    "\n",
    "        for _, row in user_rows.iterrows():\n",
    "            item = row['item']\n",
    "            if item not in item2idx:\n",
    "                # Unknown item: X\n",
    "                results.append({'user': user, 'item': item, 'recommend': 'X'})\n",
    "                stats['total_items'] += 1\n",
    "            elif item2idx[item] in train_items_set:\n",
    "                # 이미 선택한 item: X (추천 불가)\n",
    "                results.append({'user': user, 'item': item, 'recommend': 'X'})\n",
    "                stats['total_items'] += 1\n",
    "            else:\n",
    "                items_to_score.append(item2idx[item])\n",
    "                item_info.append(item)\n",
    "\n",
    "        if len(items_to_score) == 0:\n",
    "            continue\n",
    "\n",
    "        # Score items\n",
    "        item_tensor = torch.LongTensor(items_to_score).to(device)\n",
    "        user_tensor = torch.full((len(items_to_score),), user_idx, dtype=torch.long, device=device)\n",
    "        scores = (u_emb[user_tensor] * i_emb[item_tensor]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "        # Hybrid selection:\n",
    "        # Step 1: Filter by threshold\n",
    "        above_threshold_mask = scores > threshold\n",
    "        above_threshold_indices = np.where(above_threshold_mask)[0]\n",
    "\n",
    "        # Step 2: Top-K selection among threshold-passing items\n",
    "        K = user_k[user_idx]\n",
    "\n",
    "        if len(above_threshold_indices) > K:\n",
    "            # Threshold 넘는게 K개 초과 → Top-K만 선택\n",
    "            above_scores = scores[above_threshold_indices]\n",
    "            top_k_in_above = np.argsort(above_scores)[-K:]\n",
    "            selected_indices = set(above_threshold_indices[top_k_in_above])\n",
    "        else:\n",
    "            # Threshold 넘는게 K개 이하 → 모두 선택\n",
    "            selected_indices = set(above_threshold_indices)\n",
    "\n",
    "        # Mark O/X\n",
    "        user_o_count = 0\n",
    "        for i, item in enumerate(item_info):\n",
    "            if i in selected_indices:\n",
    "                recommend = 'O'\n",
    "                user_o_count += 1\n",
    "            else:\n",
    "                recommend = 'X'\n",
    "            results.append({'user': user, 'item': item, 'recommend': recommend})\n",
    "            stats['total_items'] += 1\n",
    "\n",
    "        stats['total_o'] += user_o_count\n",
    "        if user_o_count > 0:\n",
    "            stats['users_with_o'] += 1\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 통계 출력\n",
    "    o_ratio = stats['total_o'] / stats['total_items'] if stats['total_items'] > 0 else 0\n",
    "    print(f\"\\nHybrid Prediction Stats:\")\n",
    "    print(f\"  Total items: {stats['total_items']}\")\n",
    "    print(f\"  Total O: {stats['total_o']}\")\n",
    "    print(f\"  O ratio: {o_ratio*100:.2f}%\")\n",
    "    print(f\"  Users with ≥1 O: {stats['users_with_o']}\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test with Hybrid prediction\n",
    "sample_test = pd.read_csv('../data/sample1.csv')\n",
    "\n",
    "print(\"Sample Test with Hybrid Prediction:\")\n",
    "print(f\"Threshold: {OPTIMAL_THRESHOLD:.4f}\")\n",
    "predictions = predict_hybrid(sample_test, OPTIMAL_THRESHOLD)\n",
    "\n",
    "print(f\"\\nPredictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set에서 AUC-ROC 계산\n",
    "print(\"Calculating Test Set AUC-ROC...\")\n",
    "\n",
    "# Test positive samples\n",
    "test_u_gpu = torch.LongTensor(test_df['user_idx'].values).to(device)\n",
    "test_i_gpu = torch.LongTensor(test_df['item_idx'].values).to(device)\n",
    "test_pos_scores = (u_emb[test_u_gpu] * i_emb[test_i_gpu]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "# Test negative samples (random pairs not in train/val/test)\n",
    "n_test_neg = len(test_df)\n",
    "test_neg_scores = []\n",
    "for _ in range(n_test_neg):\n",
    "    user_idx = np.random.randint(0, n_users)\n",
    "    item_idx = np.random.randint(0, n_items)\n",
    "    # Exclude train, val, test\n",
    "    while (item_idx in user_train_items[user_idx] or \n",
    "           (user_idx, item_idx) in val_test_edges):\n",
    "        item_idx = np.random.randint(0, n_items)\n",
    "    score = (u_emb[user_idx] * i_emb[item_idx]).sum().item()\n",
    "    test_neg_scores.append(score)\n",
    "\n",
    "test_neg_scores = np.array(test_neg_scores)\n",
    "\n",
    "# Calculate Test AUC-ROC\n",
    "test_all_scores = np.concatenate([test_pos_scores, test_neg_scores])\n",
    "test_all_labels = np.concatenate([np.ones(len(test_pos_scores)), np.zeros(len(test_neg_scores))])\n",
    "\n",
    "test_auc_roc = roc_auc_score(test_all_labels, test_all_scores)\n",
    "\n",
    "print(f\"\\nTest Set AUC-ROC: {test_auc_roc:.4f}\")\n",
    "print(f\"Validation AUC-ROC: {val_auc_roc:.4f}\")\n",
    "\n",
    "# Test Accuracy, Precision, Recall, F1 with optimal threshold\n",
    "test_preds = (test_all_scores > OPTIMAL_THRESHOLD).astype(int)\n",
    "test_acc = (test_preds == test_all_labels).mean()\n",
    "test_prec_th = precision_score(test_all_labels, test_preds, zero_division=0)\n",
    "test_rec_th = recall_score(test_all_labels, test_preds, zero_division=0)\n",
    "test_f1 = f1_score(test_all_labels, test_preds, zero_division=0)\n",
    "test_o_ratio = test_preds.mean()\n",
    "\n",
    "print(f\"\\nTest Set with Threshold={OPTIMAL_THRESHOLD:.4f}:\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Precision: {test_prec_th:.4f}\")\n",
    "print(f\"  Recall: {test_rec_th:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "print(f\"  O Ratio: {test_o_ratio*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LightGCN V8b - Rating-aware Hybrid Recommendation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel Config:\")\n",
    "print(f\"  emb_dim: {EMB_DIM}\")\n",
    "print(f\"  n_layers: {N_LAYERS}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Total edges: {len(df):,}\")\n",
    "print(f\"  Good purchases (rating >= {GOOD_RATING_THRESHOLD}): {n_good_purchases:,}\")\n",
    "print(f\"  Train (graph): {len(train_df):,}\")\n",
    "print(f\"  Val (rating >= 4 only): {len(val_df):,}\")\n",
    "print(f\"  Test (rating >= 4 only): {len(test_df):,}\")\n",
    "\n",
    "print(f\"\\nUser K statistics (MAX_K={MAX_K}):\")\n",
    "print(f\"  Min K: {min(k_values)}\")\n",
    "print(f\"  Max K: {max(k_values)}\")\n",
    "print(f\"  Mean K: {np.mean(k_values):.2f}\")\n",
    "\n",
    "print(f\"\\nOptimal Threshold (F1 Maximization):\")\n",
    "print(f\"  Threshold: {OPTIMAL_THRESHOLD:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Performance (Rating >= 4 as Ground Truth):\")\n",
    "print(f\"  AUC-ROC: {test_auc_roc:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Precision: {test_prec_th:.4f}\")\n",
    "print(f\"  Recall: {test_rec_th:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nKey Features:\")\n",
    "print(f\"  • Graph: 모든 구매 edge 포함 (rating 무관)\")\n",
    "print(f\"  • Ground Truth: Rating >= {GOOD_RATING_THRESHOLD}만 정답\")\n",
    "print(f\"  • Hybrid 추천: Threshold({OPTIMAL_THRESHOLD:.4f}) + Top-K\")\n",
    "print(f\"  • User별 K 상한: MAX_K={MAX_K}\")\n",
    "print(f\"  • 모든 구매 item 추천 제외 (rating 무관)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test inference (uncomment when ready)\n",
    "# final_test = pd.read_csv('../data/test.csv')\n",
    "# print(f\"Using threshold: {OPTIMAL_THRESHOLD:.4f}\")\n",
    "# final_predictions = predict_hybrid(final_test, OPTIMAL_THRESHOLD)\n",
    "# final_predictions.to_csv('predictions_v8b.csv', index=False)\n",
    "# print(f\"\\nPredictions saved to predictions_v8b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
