{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCN V4 - 하이퍼파라미터 최적화\n",
    "\n",
    "## 목표\n",
    "- V3 구조 유지 (덧붙이기 금지)\n",
    "- 하이퍼파라미터 그리드 서치로 최적 조합 탐색\n",
    "- 실험 결과 분석 및 최종 모델 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리 (V3와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 668, Items: 10321\n",
      "Train: 41,214, Val: 10,616\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "# ID 매핑\n",
    "user2idx = {u: i for i, u in enumerate(sorted(df['user'].unique()))}\n",
    "item2idx = {it: i for i, it in enumerate(sorted(df['item'].unique()))}\n",
    "n_users, n_items = len(user2idx), len(item2idx)\n",
    "\n",
    "df['user_idx'] = df['user'].map(user2idx)\n",
    "df['item_idx'] = df['item'].map(item2idx)\n",
    "df['label'] = (df['rating'] >= 4.0).astype(int)\n",
    "\n",
    "# Positive만 사용\n",
    "positive_df = df[df['label'] == 1].copy()\n",
    "\n",
    "# Train/Val Split\n",
    "train_data, val_data = [], []\n",
    "for user_idx in range(n_users):\n",
    "    user_pos = positive_df[positive_df['user_idx'] == user_idx]\n",
    "    if len(user_pos) >= 2:\n",
    "        user_pos = user_pos.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "        split_idx = max(1, int(0.8 * len(user_pos)))\n",
    "        train_data.append(user_pos.iloc[:split_idx])\n",
    "        val_data.append(user_pos.iloc[split_idx:])\n",
    "    elif len(user_pos) == 1:\n",
    "        train_data.append(user_pos)\n",
    "\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "val_df = pd.concat(val_data, ignore_index=True)\n",
    "\n",
    "print(f\"Users: {n_users}, Items: {n_items}\")\n",
    "print(f\"Train: {len(train_df):,}, Val: {len(val_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User positive items\n",
    "user_pos_items = defaultdict(set)\n",
    "for _, row in train_df.iterrows():\n",
    "    user_pos_items[int(row['user_idx'])].add(int(row['item_idx']))\n",
    "\n",
    "def sample_negatives(user_idx, num_neg=4):\n",
    "    pos = user_pos_items[user_idx]\n",
    "    neg_cands = list(set(range(n_items)) - pos)\n",
    "    if len(neg_cands) < num_neg:\n",
    "        return neg_cands\n",
    "    return np.random.choice(neg_cands, size=num_neg, replace=False).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 & 학습 함수 (V3와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(train_df, n_users, n_items):\n",
    "    users = train_df['user_idx'].values\n",
    "    items = train_df['item_idx'].values\n",
    "    edge_u2i = np.array([users, items + n_users])\n",
    "    edge_i2u = np.array([items + n_users, users])\n",
    "    edge_index = torch.LongTensor(np.concatenate([edge_u2i, edge_i2u], axis=1))\n",
    "    \n",
    "    num_nodes = n_users + n_items\n",
    "    deg = torch.zeros(num_nodes).scatter_add(0, edge_index[0], torch.ones(edge_index.shape[1]))\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    edge_weight = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]\n",
    "    \n",
    "    return edge_index.to(device), edge_weight.to(device)\n",
    "\n",
    "edge_index, edge_weight = build_graph(train_df, n_users, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=32, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "    \n",
    "    def forward(self, edge_index, edge_weight):\n",
    "        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
    "        embs = [all_emb]\n",
    "        \n",
    "        for _ in range(self.n_layers):\n",
    "            row, col = edge_index\n",
    "            messages = all_emb[col] * edge_weight.unsqueeze(1)\n",
    "            all_emb = torch.zeros_like(all_emb).scatter_add(0, row.unsqueeze(1).expand(-1, self.emb_dim), messages)\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        final_emb = torch.mean(torch.stack(embs), dim=0)\n",
    "        return final_emb[:self.n_users], final_emb[self.n_users:]\n",
    "    \n",
    "    def predict(self, u_idx, i_idx, u_emb, i_emb):\n",
    "        return (u_emb[u_idx] * i_emb[i_idx]).sum(dim=1)\n",
    "\n",
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    diff = pos_scores.unsqueeze(1) - neg_scores\n",
    "    return -torch.log(torch.sigmoid(diff) + 1e-8).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, edge_index, edge_weight, val_df, k=10, n_neg=99):\n",
    "    model.eval()\n",
    "    u_emb, i_emb = model(edge_index, edge_weight)\n",
    "    \n",
    "    hits, ndcgs = [], []\n",
    "    for user_idx in val_df['user_idx'].unique():\n",
    "        for _, row in val_df[val_df['user_idx'] == user_idx].iterrows():\n",
    "            pos_item = int(row['item_idx'])\n",
    "            neg_items = sample_negatives(user_idx, n_neg)\n",
    "            if len(neg_items) < n_neg:\n",
    "                continue\n",
    "            \n",
    "            candidates = [pos_item] + neg_items\n",
    "            u_t = torch.full((len(candidates),), user_idx, dtype=torch.long).to(device)\n",
    "            i_t = torch.LongTensor(candidates).to(device)\n",
    "            scores = model.predict(u_t, i_t, u_emb, i_emb).cpu().numpy()\n",
    "            \n",
    "            rank = (scores > scores[0]).sum() + 1\n",
    "            hits.append(1.0 if rank <= k else 0.0)\n",
    "            ndcgs.append(1.0 / np.log2(rank + 1) if rank <= k else 0.0)\n",
    "    \n",
    "    return np.mean(hits), np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(emb_dim, n_layers, lr, weight_decay, epochs=30, batch_size=512, num_neg=4, verbose=False):\n",
    "    \"\"\"하나의 하이퍼파라미터 조합으로 모델 학습\"\"\"\n",
    "    model = LightGCN(n_users, n_items, emb_dim, n_layers).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    best_hit = 0\n",
    "    patience_cnt = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_shuffled = train_df.sample(frac=1, random_state=SEED+epoch).reset_index(drop=True)\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for i in range(0, len(train_shuffled), batch_size):\n",
    "            batch = train_shuffled.iloc[i:i+batch_size]\n",
    "            pos_u = torch.LongTensor(batch['user_idx'].values).to(device)\n",
    "            pos_i = torch.LongTensor(batch['item_idx'].values).to(device)\n",
    "            neg_items = [sample_negatives(int(u), num_neg) for u in batch['user_idx'].values]\n",
    "            neg_i = torch.LongTensor(neg_items).to(device)\n",
    "            \n",
    "            u_emb, i_emb = model(edge_index, edge_weight)\n",
    "            pos_scores = model.predict(pos_u, pos_i, u_emb, i_emb)\n",
    "            neg_u = pos_u.unsqueeze(1).expand(-1, num_neg).reshape(-1)\n",
    "            neg_scores = model.predict(neg_u, neg_i.reshape(-1), u_emb, i_emb).reshape(-1, num_neg)\n",
    "            \n",
    "            loss = bpr_loss(pos_scores, neg_scores)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Evaluate every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            hit, ndcg = evaluate(model, edge_index, edge_weight, val_df)\n",
    "            if verbose:\n",
    "                print(f\"  Epoch {epoch+1}: Loss={epoch_loss/n_batches:.4f}, Hit@10={hit:.4f}\")\n",
    "            \n",
    "            if hit > best_hit:\n",
    "                best_hit = hit\n",
    "                best_ndcg = ndcg\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "            \n",
    "            if patience_cnt >= 3:\n",
    "                break\n",
    "    \n",
    "    return best_hit, best_ndcg, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 하이퍼파라미터 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments: 81\n",
      "This would take too long, so we'll do a smarter search...\n"
     ]
    }
   ],
   "source": [
    "# 실험할 하이퍼파라미터\n",
    "param_grid = {\n",
    "    'emb_dim': [16, 32, 64],\n",
    "    'n_layers': [1, 2, 3],\n",
    "    'lr': [1e-3, 5e-3, 1e-2],\n",
    "    'weight_decay': [1e-6, 1e-5, 1e-4]\n",
    "}\n",
    "\n",
    "# 전체 조합 수\n",
    "total_experiments = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"Total experiments: {total_experiments}\")\n",
    "print(\"This would take too long, so we'll do a smarter search...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 1: Finding optimal emb_dim & n_layers\n",
      "==================================================\n",
      "Testing emb_dim=16, n_layers=1... Hit@10=0.7861, NDCG@10=0.5282\n",
      "Testing emb_dim=16, n_layers=2... Hit@10=0.7814, NDCG@10=0.5208\n",
      "Testing emb_dim=16, n_layers=3... Hit@10=0.7689, NDCG@10=0.5024\n",
      "Testing emb_dim=32, n_layers=1... Hit@10=0.7923, NDCG@10=0.5383\n",
      "Testing emb_dim=32, n_layers=2... Hit@10=0.7865, NDCG@10=0.5284\n",
      "Testing emb_dim=32, n_layers=3... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n_layers \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTesting emb_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memb_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, n_layers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     hit, ndcg, _ = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     results.append({\n\u001b[32m     14\u001b[39m         \u001b[33m'\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m'\u001b[39m: emb_dim, \u001b[33m'\u001b[39m\u001b[33mn_layers\u001b[39m\u001b[33m'\u001b[39m: n_layers, \n\u001b[32m     15\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m5e-3\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1e-5\u001b[39m,\n\u001b[32m     16\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhit@10\u001b[39m\u001b[33m'\u001b[39m: hit, \u001b[33m'\u001b[39m\u001b[33mndcg@10\u001b[39m\u001b[33m'\u001b[39m: ndcg\n\u001b[32m     17\u001b[39m     })\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHit@10=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhit\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, NDCG@10=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndcg\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(emb_dim, n_layers, lr, weight_decay, epochs, batch_size, num_neg, verbose)\u001b[39m\n\u001b[32m     18\u001b[39m pos_u = torch.LongTensor(batch[\u001b[33m'\u001b[39m\u001b[33muser_idx\u001b[39m\u001b[33m'\u001b[39m].values).to(device)\n\u001b[32m     19\u001b[39m pos_i = torch.LongTensor(batch[\u001b[33m'\u001b[39m\u001b[33mitem_idx\u001b[39m\u001b[33m'\u001b[39m].values).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m neg_items = [\u001b[43msample_negatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[33m'\u001b[39m\u001b[33muser_idx\u001b[39m\u001b[33m'\u001b[39m].values]\n\u001b[32m     21\u001b[39m neg_i = torch.LongTensor(neg_items).to(device)\n\u001b[32m     23\u001b[39m u_emb, i_emb = model(edge_index, edge_weight)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36msample_negatives\u001b[39m\u001b[34m(user_idx, num_neg)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(neg_cands) < num_neg:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m neg_cands\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_cands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_neg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:1001\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/학교/2025-2/그래프신경망과빅데이터/gnn-recsys/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3323\u001b[39m, in \u001b[36m_prod_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3309\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3310\u001b[39m \u001b[33;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[32m   3311\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3317\u001b[39m \u001b[33;03m    ndarray.min : equivalent method\u001b[39;00m\n\u001b[32m   3318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np.minimum, \u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[32m   3320\u001b[39m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[32m-> \u001b[39m\u001b[32m3323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prod_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3324\u001b[39m                      initial=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   3325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   3328\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[32m   3329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   3330\u001b[39m          initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 단계별 탐색 (효율적)\n",
    "results = []\n",
    "\n",
    "# Step 1: Embedding dim & Layers (lr=5e-3, wd=1e-5 고정)\n",
    "print(\"=\" * 50)\n",
    "print(\"Step 1: Finding optimal emb_dim & n_layers\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for emb_dim in [16, 32, 64]:\n",
    "    for n_layers in [1, 2, 3]:\n",
    "        print(f\"Testing emb_dim={emb_dim}, n_layers={n_layers}...\", end=\" \")\n",
    "        hit, ndcg, _ = train_model(emb_dim, n_layers, lr=5e-3, weight_decay=1e-5, epochs=30)\n",
    "        results.append({\n",
    "            'emb_dim': emb_dim, 'n_layers': n_layers, \n",
    "            'lr': 5e-3, 'weight_decay': 1e-5,\n",
    "            'hit@10': hit, 'ndcg@10': ndcg\n",
    "        })\n",
    "        print(f\"Hit@10={hit:.4f}, NDCG@10={ndcg:.4f}\")\n",
    "\n",
    "# 최고 조합 찾기\n",
    "best_arch = max(results, key=lambda x: x['hit@10'])\n",
    "print(f\"\\nBest architecture: emb_dim={best_arch['emb_dim']}, n_layers={best_arch['n_layers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Learning rate 탐색\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Step 2: Finding optimal learning rate\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "lr_results = []\n",
    "for lr in [1e-3, 5e-3, 1e-2]:\n",
    "    print(f\"Testing lr={lr}...\", end=\" \")\n",
    "    hit, ndcg, _ = train_model(\n",
    "        best_arch['emb_dim'], best_arch['n_layers'], \n",
    "        lr=lr, weight_decay=1e-5, epochs=30\n",
    "    )\n",
    "    lr_results.append({'lr': lr, 'hit@10': hit, 'ndcg@10': ndcg})\n",
    "    print(f\"Hit@10={hit:.4f}\")\n",
    "\n",
    "best_lr = max(lr_results, key=lambda x: x['hit@10'])['lr']\n",
    "print(f\"\\nBest LR: {best_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Weight decay 탐색\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Step 3: Finding optimal weight decay\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "wd_results = []\n",
    "for wd in [1e-6, 1e-5, 1e-4]:\n",
    "    print(f\"Testing weight_decay={wd}...\", end=\" \")\n",
    "    hit, ndcg, _ = train_model(\n",
    "        best_arch['emb_dim'], best_arch['n_layers'], \n",
    "        lr=best_lr, weight_decay=wd, epochs=30\n",
    "    )\n",
    "    wd_results.append({'weight_decay': wd, 'hit@10': hit, 'ndcg@10': ndcg})\n",
    "    print(f\"Hit@10={hit:.4f}\")\n",
    "\n",
    "best_wd = max(wd_results, key=lambda x: x['hit@10'])['weight_decay']\n",
    "print(f\"\\nBest Weight Decay: {best_wd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 결과 정리\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Experiment Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nArchitecture Search Results:\")\n",
    "print(results_df.sort_values('hit@10', ascending=False).to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\nFinal Best Configuration:\")\n",
    "print(f\"  emb_dim: {best_arch['emb_dim']}\")\n",
    "print(f\"  n_layers: {best_arch['n_layers']}\")\n",
    "print(f\"  lr: {best_lr}\")\n",
    "print(f\"  weight_decay: {best_wd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 최종 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 하이퍼파라미터로 더 오래 학습\n",
    "print(\"=\" * 50)\n",
    "print(\"Training Final Model with Best Hyperparameters\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "FINAL_EMB_DIM = best_arch['emb_dim']\n",
    "FINAL_N_LAYERS = best_arch['n_layers']\n",
    "FINAL_LR = best_lr\n",
    "FINAL_WD = best_wd\n",
    "FINAL_EPOCHS = 50\n",
    "\n",
    "print(f\"Config: emb={FINAL_EMB_DIM}, layers={FINAL_N_LAYERS}, lr={FINAL_LR}, wd={FINAL_WD}\")\n",
    "print(\"\")\n",
    "\n",
    "final_model = LightGCN(n_users, n_items, FINAL_EMB_DIM, FINAL_N_LAYERS).to(device)\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=FINAL_LR, weight_decay=FINAL_WD)\n",
    "\n",
    "history = {'loss': [], 'hit@10': [], 'ndcg@10': []}\n",
    "best_hit = 0\n",
    "\n",
    "for epoch in range(FINAL_EPOCHS):\n",
    "    final_model.train()\n",
    "    train_shuffled = train_df.sample(frac=1, random_state=SEED+epoch).reset_index(drop=True)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i in range(0, len(train_shuffled), 512):\n",
    "        batch = train_shuffled.iloc[i:i+512]\n",
    "        pos_u = torch.LongTensor(batch['user_idx'].values).to(device)\n",
    "        pos_i = torch.LongTensor(batch['item_idx'].values).to(device)\n",
    "        neg_items = [sample_negatives(int(u), 4) for u in batch['user_idx'].values]\n",
    "        neg_i = torch.LongTensor(neg_items).to(device)\n",
    "        \n",
    "        u_emb, i_emb = final_model(edge_index, edge_weight)\n",
    "        pos_scores = final_model.predict(pos_u, pos_i, u_emb, i_emb)\n",
    "        neg_u = pos_u.unsqueeze(1).expand(-1, 4).reshape(-1)\n",
    "        neg_scores = final_model.predict(neg_u, neg_i.reshape(-1), u_emb, i_emb).reshape(-1, 4)\n",
    "        \n",
    "        loss = bpr_loss(pos_scores, neg_scores)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    history['loss'].append(epoch_loss / n_batches)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        hit, ndcg = evaluate(final_model, edge_index, edge_weight, val_df)\n",
    "        history['hit@10'].append(hit)\n",
    "        history['ndcg@10'].append(ndcg)\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {epoch_loss/n_batches:.4f} | Hit@10: {hit:.4f} | NDCG@10: {ndcg:.4f}\")\n",
    "        \n",
    "        if hit > best_hit:\n",
    "            best_hit = hit\n",
    "            torch.save(final_model.state_dict(), 'best_lightgcn_v4.pt')\n",
    "\n",
    "print(f\"\\nFinal Best Hit@10: {best_hit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['loss'], 'b-', linewidth=2)\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "epochs_val = np.arange(5, len(history['loss'])+1, 5)[:len(history['hit@10'])]\n",
    "axes[1].plot(epochs_val, history['hit@10'], 'g-o', linewidth=2)\n",
    "axes[1].set_title('Hit@10')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(epochs_val, history['ndcg@10'], 'r-o', linewidth=2)\n",
    "axes[2].set_title('NDCG@10')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threshold 튜닝 & O/X 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "final_model.load_state_dict(torch.load('best_lightgcn_v4.pt'))\n",
    "final_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_emb, i_emb = final_model(edge_index, edge_weight)\n",
    "\n",
    "# Positive scores\n",
    "val_scores, val_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for _, row in val_df.iterrows():\n",
    "        u_idx = torch.LongTensor([int(row['user_idx'])]).to(device)\n",
    "        i_idx = torch.LongTensor([int(row['item_idx'])]).to(device)\n",
    "        score = final_model.predict(u_idx, i_idx, u_emb, i_emb).item()\n",
    "        val_scores.append(score)\n",
    "        val_labels.append(1)\n",
    "\n",
    "# Negative scores\n",
    "with torch.no_grad():\n",
    "    for user_idx in val_df['user_idx'].unique():\n",
    "        n_pos = (val_df['user_idx'] == user_idx).sum()\n",
    "        neg_items = sample_negatives(int(user_idx), n_pos)\n",
    "        for neg_item in neg_items:\n",
    "            u_idx = torch.LongTensor([int(user_idx)]).to(device)\n",
    "            i_idx = torch.LongTensor([neg_item]).to(device)\n",
    "            score = final_model.predict(u_idx, i_idx, u_emb, i_emb).item()\n",
    "            val_scores.append(score)\n",
    "            val_labels.append(0)\n",
    "\n",
    "val_scores = np.array(val_scores)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "print(f\"Validation: {len(val_scores)} samples (Pos: {val_labels.sum()}, Neg: {len(val_labels)-val_labels.sum()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold search\n",
    "thresholds = np.percentile(val_scores, [30, 40, 50, 60, 70, 80, 90])\n",
    "\n",
    "print(\"Threshold Tuning:\")\n",
    "print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_prec, best_th, best_f1 = 0, 0, 0\n",
    "for th in thresholds:\n",
    "    preds = (val_scores >= th).astype(int)\n",
    "    tp = ((preds == 1) & (val_labels == 1)).sum()\n",
    "    fp = ((preds == 1) & (val_labels == 0)).sum()\n",
    "    fn = ((preds == 0) & (val_labels == 1)).sum()\n",
    "    \n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "    \n",
    "    print(f\"{th:<12.4f} {prec:<12.4f} {rec:<12.4f} {f1:<12.4f}\")\n",
    "    \n",
    "    if prec >= 0.6 and prec > best_prec:\n",
    "        best_prec, best_th, best_f1 = prec, th, f1\n",
    "    elif best_prec < 0.6 and f1 > best_f1:\n",
    "        best_f1, best_th, best_prec = f1, th, prec\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Selected: {best_th:.4f} (Precision: {best_prec:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ox(test_df):\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for _, row in test_df.iterrows():\n",
    "            user, item = row['user'], row['item']\n",
    "            if user not in user2idx or item not in item2idx:\n",
    "                recommend = 'X'\n",
    "            else:\n",
    "                u_idx = torch.LongTensor([user2idx[user]]).to(device)\n",
    "                i_idx = torch.LongTensor([item2idx[item]]).to(device)\n",
    "                score = final_model.predict(u_idx, i_idx, u_emb, i_emb).item()\n",
    "                recommend = 'O' if score >= best_th else 'X'\n",
    "            results.append({'user': user, 'item': item, 'recommend': recommend})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test on validation\n",
    "preds = predict_ox(val_df[['user', 'item']])\n",
    "o_ratio = (preds['recommend'] == 'O').mean()\n",
    "print(f\"\\nO ratio: {100*o_ratio:.1f}%\")\n",
    "print(preds.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 최종 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"LightGCN V4 Final Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nOptimal Hyperparameters:\")\n",
    "print(f\"  Embedding dim: {FINAL_EMB_DIM}\")\n",
    "print(f\"  GCN layers: {FINAL_N_LAYERS}\")\n",
    "print(f\"  Learning rate: {FINAL_LR}\")\n",
    "print(f\"  Weight decay: {FINAL_WD}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in final_model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Best Hit@10: {best_hit:.4f}\")\n",
    "print(f\"  Best NDCG@10: {max(history['ndcg@10']):.4f}\")\n",
    "print(f\"  Precision: {best_prec:.4f}\")\n",
    "print(f\"  O ratio: {100*o_ratio:.1f}%\")\n",
    "\n",
    "print(f\"\\nV3 vs V4 Comparison:\")\n",
    "print(f\"  V3 Hit@10: 0.7894\")\n",
    "print(f\"  V4 Hit@10: {best_hit:.4f}\")\n",
    "print(f\"  Improvement: {100*(best_hit - 0.7894)/0.7894:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 파일 추론\n",
    "# test_df = pd.read_csv('data/test.csv')\n",
    "# final_preds = predict_ox(test_df)\n",
    "# final_preds.to_csv('predictions_gnn_v4.csv', index=False)\n",
    "\n",
    "print(\"Test inference code ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
