"""
Breakthrough Analysis for GNN-RecSys
ë¶„ì„ ëª©í‘œ: ì™œ ì„±ëŠ¥ì´ ë‚®ì€ì§€, ì–´ë–¤ breakthroughê°€ í•„ìš”í•œì§€ íŒŒì•…
"""

import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict

# ì„¤ì •
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (20, 12)

# ë°ì´í„° ë¡œë“œ
data_dir = '../data/processed'

# ID mappings
with open(os.path.join(data_dir, 'id_mappings.pkl'), 'rb') as f:
    mappings = pickle.load(f)

train_df = pd.read_csv(os.path.join(data_dir, 'train_split_v3.csv'))
valid_df = pd.read_csv(os.path.join(data_dir, 'valid_split_v3.csv'))
test_df = pd.read_csv(os.path.join(data_dir, 'test_split_v3.csv'))

print("=" * 80)
print("BREAKTHROUGH ANALYSIS: ì™œ ì„±ëŠ¥ì´ ë‚®ì€ê°€?")
print("=" * 80)

# 1. ê¸°ë³¸ í†µê³„
print("\n[1] ê¸°ë³¸ ë°ì´í„° í†µê³„")
print("-" * 80)
print(f"Train: {len(train_df):,} samples ({len(train_df)/len(pd.concat([train_df, valid_df, test_df]))*100:.1f}%)")
print(f"Valid: {len(valid_df):,} samples ({len(valid_df)/len(pd.concat([train_df, valid_df, test_df]))*100:.1f}%)")
print(f"Test:  {len(test_df):,} samples ({len(test_df)/len(pd.concat([train_df, valid_df, test_df]))*100:.1f}%)")
print(f"\nTotal users: {len(mappings['user_id_map'])}")
print(f"Total items: {len(mappings['item_id_map'])}")

# 2. Cold-Start ë¶„ì„
print("\n[2] Cold-Start ì•„ì´í…œ ë¶„ì„")
print("-" * 80)

train_items = set(train_df['item_id'].unique())
valid_items = set(valid_df['item_id'].unique())
test_items = set(test_df['item_id'].unique())
train_valid_items = train_items | valid_items

test_cold_items = test_items - train_valid_items
test_warm_items = test_items & train_valid_items

print(f"Trainì—ë§Œ ìˆëŠ” ì•„ì´í…œ: {len(train_items - test_items):,}")
print(f"Testì—ë§Œ ìˆëŠ” ì•„ì´í…œ (ì™„ì „ cold): {len(test_cold_items):,}")
print(f"Train+Validì™€ ê²¹ì¹˜ëŠ” Test ì•„ì´í…œ: {len(test_warm_items):,}")
print(f"\nTestì˜ cold-start ë¹„ìœ¨: {len(test_cold_items)/len(test_items)*100:.2f}%")

# Testì—ì„œ cold-start ì•„ì´í…œì´ í¬í•¨ëœ ìƒ˜í”Œ ìˆ˜
test_cold_samples = test_df[test_df['item_id'].isin(test_cold_items)]
print(f"Test ìƒ˜í”Œ ì¤‘ cold-start ì•„ì´í…œ í¬í•¨: {len(test_cold_samples):,} / {len(test_df):,} ({len(test_cold_samples)/len(test_df)*100:.2f}%)")

# 3. Item ì¸ê¸°ë„ ë¶„ì„
print("\n[3] ì•„ì´í…œ ì¸ê¸°ë„ ë¶„ì„ (Trainì—ì„œì˜ ë“±ì¥ íšŸìˆ˜)")
print("-" * 80)

train_item_counts = train_df['item_id'].value_counts()
test_item_popularity = []

for item_id in test_df['item_id'].unique():
    if item_id in train_item_counts.index:
        test_item_popularity.append(train_item_counts[item_id])
    else:
        test_item_popularity.append(0)

test_item_popularity = np.array(test_item_popularity)

print(f"Test ì•„ì´í…œì˜ Train ë“±ì¥ íšŸìˆ˜:")
print(f"  - í‰ê· : {test_item_popularity.mean():.2f}")
print(f"  - ì¤‘ì•™ê°’: {np.median(test_item_popularity):.2f}")
print(f"  - ìµœì†Œ: {test_item_popularity.min()}")
print(f"  - ìµœëŒ€: {test_item_popularity.max()}")
print(f"  - í‘œì¤€í¸ì°¨: {test_item_popularity.std():.2f}")

# Long-tail ë¶„í¬
print(f"\nTest ì•„ì´í…œ ì¤‘:")
print(f"  - Trainì— 0ë²ˆ ë“±ì¥ (cold): {(test_item_popularity == 0).sum()} ({(test_item_popularity == 0).sum()/len(test_item_popularity)*100:.1f}%)")
print(f"  - Trainì— 1-5ë²ˆ: {((test_item_popularity >= 1) & (test_item_popularity <= 5)).sum()} ({((test_item_popularity >= 1) & (test_item_popularity <= 5)).sum()/len(test_item_popularity)*100:.1f}%)")
print(f"  - Trainì— 6-10ë²ˆ: {((test_item_popularity >= 6) & (test_item_popularity <= 10)).sum()} ({((test_item_popularity >= 6) & (test_item_popularity <= 10)).sum()/len(test_item_popularity)*100:.1f}%)")
print(f"  - Trainì— 11-50ë²ˆ: {((test_item_popularity >= 11) & (test_item_popularity <= 50)).sum()} ({((test_item_popularity >= 11) & (test_item_popularity <= 50)).sum()/len(test_item_popularity)*100:.1f}%)")
print(f"  - Trainì— 50ë²ˆ ì´ìƒ: {(test_item_popularity > 50).sum()} ({(test_item_popularity > 50).sum()/len(test_item_popularity)*100:.1f}%)")

# 4. User í™œë™ë„ ë¶„ì„
print("\n[4] ì‚¬ìš©ì í™œë™ë„ ë¶„ì„")
print("-" * 80)

train_user_counts = train_df['user_id'].value_counts()
test_user_counts = test_df['user_id'].value_counts()

print(f"Train - Userë‹¹ í‰ê·  ìƒí˜¸ì‘ìš©: {train_user_counts.mean():.2f}")
print(f"Test - Userë‹¹ í‰ê·  ìƒí˜¸ì‘ìš©: {test_user_counts.mean():.2f}")
print(f"\nTrain - User ìƒí˜¸ì‘ìš© ë²”ìœ„: {train_user_counts.min()} ~ {train_user_counts.max()}")
print(f"Test - User ìƒí˜¸ì‘ìš© ë²”ìœ„: {test_user_counts.min()} ~ {test_user_counts.max()}")

# 5. Rating ë¶„í¬ ë¹„êµ
print("\n[5] Rating ë¶„í¬ ë¹„êµ")
print("-" * 80)

train_rating_dist = train_df['rating'].value_counts(normalize=True).sort_index()
test_rating_dist = test_df['rating'].value_counts(normalize=True).sort_index()

print(f"Train Rating ë¶„í¬:")
for rating, pct in train_rating_dist.items():
    print(f"  {rating}: {pct*100:.2f}%")

print(f"\nTest Rating ë¶„í¬:")
for rating, pct in test_rating_dist.items():
    print(f"  {rating}: {pct*100:.2f}%")

print(f"\nTrain í‰ê·  Rating: {train_df['rating'].mean():.3f}")
print(f"Test í‰ê·  Rating: {test_df['rating'].mean():.3f}")

# 6. Graph ì—°ê²°ì„± ë¶„ì„
print("\n[6] Graph ì—°ê²°ì„± ë¶„ì„")
print("-" * 80)

# Userë³„ unique item ìˆ˜
train_user_items = train_df.groupby('user_id')['item_id'].apply(set).to_dict()
test_user_items = test_df.groupby('user_id')['item_id'].apply(set).to_dict()

# Test userê°€ Trainì—ì„œ ë³¸ ì•„ì´í…œê³¼ì˜ ì¤‘ë³µ
overlap_ratios = []
for user_id in test_user_items.keys():
    train_items_set = train_user_items.get(user_id, set())
    test_items_set = test_user_items[user_id]

    # ì´ userì˜ test ì•„ì´í…œ ì¤‘ trainì— ë“±ì¥í•œ ì•„ì´í…œ ë¹„ìœ¨
    test_items_in_train = sum(1 for item in test_items_set if item in train_item_counts.index)
    overlap_ratios.append(test_items_in_train / len(test_items_set))

print(f"Test userì˜ ì•„ì´í…œì´ Train ì „ì²´ì— ë“±ì¥í•œ ë¹„ìœ¨:")
print(f"  - í‰ê· : {np.mean(overlap_ratios)*100:.2f}%")
print(f"  - ì¤‘ì•™ê°’: {np.median(overlap_ratios)*100:.2f}%")

# 7. Sparsity ë¶„ì„
print("\n[7] Sparsity ë¶„ì„")
print("-" * 80)

n_users = len(mappings['user_id_map'])
n_items = len(mappings['item_id_map'])

train_sparsity = 1 - (len(train_df) / (n_users * n_items))
test_sparsity = 1 - (len(test_df) / (n_users * n_items))

print(f"Train sparsity: {train_sparsity*100:.4f}%")
print(f"Test sparsity: {test_sparsity*100:.4f}%")
print(f"Overall sparsity: {(1 - (len(train_df) + len(test_df)) / (n_users * n_items))*100:.4f}%")

# 8. ì‹œê°í™”
print("\n[8] ì‹œê°í™” ìƒì„± ì¤‘...")
print("-" * 80)

fig = plt.figure(figsize=(20, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# 8-1. Test ì•„ì´í…œì˜ Train ë“±ì¥ íšŸìˆ˜ ë¶„í¬
ax1 = fig.add_subplot(gs[0, 0])
bins = [0, 1, 5, 10, 20, 50, 100, max(test_item_popularity)+1]
ax1.hist(test_item_popularity, bins=bins, edgecolor='black', alpha=0.7)
ax1.set_xlabel('Trainì—ì„œì˜ ë“±ì¥ íšŸìˆ˜')
ax1.set_ylabel('Test ì•„ì´í…œ ìˆ˜')
ax1.set_title('Test ì•„ì´í…œì˜ Train ì¸ê¸°ë„ ë¶„í¬')
ax1.set_xscale('symlog')
ax1.grid(True, alpha=0.3)

# 8-2. Rating ë¶„í¬ ë¹„êµ
ax2 = fig.add_subplot(gs[0, 1])
ratings = sorted(set(train_df['rating'].unique()) | set(test_df['rating'].unique()))
train_counts = [train_rating_dist.get(r, 0) for r in ratings]
test_counts = [test_rating_dist.get(r, 0) for r in ratings]
x = np.arange(len(ratings))
width = 0.35
ax2.bar(x - width/2, train_counts, width, label='Train', alpha=0.7)
ax2.bar(x + width/2, test_counts, width, label='Test', alpha=0.7)
ax2.set_xlabel('Rating')
ax2.set_ylabel('ë¹„ìœ¨')
ax2.set_title('Train vs Test Rating ë¶„í¬')
ax2.set_xticks(x)
ax2.set_xticklabels([f'{r:.1f}' for r in ratings])
ax2.legend()
ax2.grid(True, alpha=0.3)

# 8-3. User í™œë™ë„ ë¹„êµ
ax3 = fig.add_subplot(gs[0, 2])
ax3.boxplot([train_user_counts.values, test_user_counts.values],
            labels=['Train', 'Test'])
ax3.set_ylabel('Userë‹¹ ìƒí˜¸ì‘ìš© ìˆ˜')
ax3.set_title('User í™œë™ë„ ë¹„êµ')
ax3.set_yscale('log')
ax3.grid(True, alpha=0.3)

# 8-4. Cold-start ìƒ˜í”Œ ë¹„ìœ¨
ax4 = fig.add_subplot(gs[1, 0])
labels = ['Warm Items', 'Cold Items']
sizes = [len(test_df) - len(test_cold_samples), len(test_cold_samples)]
colors = ['#66b3ff', '#ff9999']
ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
ax4.set_title('Test Set: Cold vs Warm Items')

# 8-5. Item popularity distribution (log scale)
ax5 = fig.add_subplot(gs[1, 1])
train_pop_sorted = sorted(train_item_counts.values, reverse=True)
ax5.plot(train_pop_sorted, linewidth=2)
ax5.set_xlabel('Item Rank')
ax5.set_ylabel('ë“±ì¥ íšŸìˆ˜ (log scale)')
ax5.set_title('Item ì¸ê¸°ë„ Long-tail ë¶„í¬ (Train)')
ax5.set_yscale('log')
ax5.grid(True, alpha=0.3)

# 8-6. Test ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜
ax6 = fig.add_subplot(gs[1, 2])
categories = ['Cold\n(0íšŒ)', '1-5íšŒ', '6-10íšŒ', '11-50íšŒ', '50+íšŒ']
counts = [
    (test_item_popularity == 0).sum(),
    ((test_item_popularity >= 1) & (test_item_popularity <= 5)).sum(),
    ((test_item_popularity >= 6) & (test_item_popularity <= 10)).sum(),
    ((test_item_popularity >= 11) & (test_item_popularity <= 50)).sum(),
    (test_item_popularity > 50).sum()
]
ax6.bar(categories, counts, alpha=0.7, color='coral')
ax6.set_ylabel('Test ì•„ì´í…œ ìˆ˜')
ax6.set_title('Test ì•„ì´í…œì˜ Train ë“±ì¥ íšŸìˆ˜ ë¶„í¬')
ax6.grid(True, alpha=0.3, axis='y')

# 8-7. User overlap ratio distribution
ax7 = fig.add_subplot(gs[2, 0])
ax7.hist(overlap_ratios, bins=20, edgecolor='black', alpha=0.7, color='lightgreen')
ax7.set_xlabel('Test ì•„ì´í…œì´ Trainì— ë“±ì¥í•œ ë¹„ìœ¨')
ax7.set_ylabel('User ìˆ˜')
ax7.set_title('Userë³„ Test-Train ì•„ì´í…œ Overlap')
ax7.grid(True, alpha=0.3)

# 8-8. Train vs Test ì•„ì´í…œ ì§‘í•©
ax8 = fig.add_subplot(gs[2, 1])
from matplotlib_venn import venn2
venn2([train_items, test_items], set_labels=('Train Items', 'Test Items'), ax=ax8)
ax8.set_title('Train vs Test ì•„ì´í…œ Overlap')

# 8-9. Sparsity ë¹„êµ
ax9 = fig.add_subplot(gs[2, 2])
sparsity_data = [train_sparsity * 100, test_sparsity * 100]
bars = ax9.bar(['Train', 'Test'], sparsity_data, alpha=0.7, color=['skyblue', 'salmon'])
ax9.set_ylabel('Sparsity (%)')
ax9.set_title('Train vs Test Sparsity')
ax9.set_ylim([99.0, 100.0])
ax9.grid(True, alpha=0.3, axis='y')
for bar, val in zip(bars, sparsity_data):
    height = bar.get_height()
    ax9.text(bar.get_x() + bar.get_width()/2., height,
            f'{val:.4f}%', ha='center', va='bottom')

plt.savefig('../results/breakthrough_analysis.png', dpi=300, bbox_inches='tight')
print("âœ… ì‹œê°í™” ì €ì¥: results/breakthrough_analysis.png")

# 9. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½
print("\n" + "=" * 80)
print("í•µì‹¬ ì¸ì‚¬ì´íŠ¸ & BREAKTHROUGH ë°©í–¥")
print("=" * 80)

print("\nğŸš¨ ë¬¸ì œì :")
print(f"1. Cold-start ì‹¬ê°: Test ìƒ˜í”Œì˜ {len(test_cold_samples)/len(test_df)*100:.1f}%ê°€ cold-start ì•„ì´í…œ")
print(f"2. Long-tail ì‹¬ê°: Test ì•„ì´í…œì˜ {(test_item_popularity <= 10).sum()/len(test_item_popularity)*100:.1f}%ê°€ Trainì—ì„œ â‰¤10ë²ˆ ë“±ì¥")
print(f"3. Extreme sparsity: {train_sparsity*100:.4f}% â†’ GNN message passing ì œí•œì ")
print(f"4. Testê°€ ë” ì–´ë ¤ì›€: í‰ê·  rating {test_df['rating'].mean():.3f} vs {train_df['rating'].mean():.3f}")

print("\nğŸ’¡ BREAKTHROUGH ì•„ì´ë””ì–´:")
print("\n[A] Data-centric Approaches:")
print("  1. User/Item Bias ì¶”ê°€: ê°œì¸ë³„/ì•„ì´í…œë³„ ì„ í˜¸ë„ í¸í–¥ ëª¨ë¸ë§")
print("  2. Ratingì„ continuousë¡œ í™œìš©: Threshold ëŒ€ì‹  regression")
print("  3. User normalization: í‰ì  ìŠ¤ì¼€ì¼ ì°¨ì´ ë³´ì •")
print("  4. Train ë°ì´í„° augmentation: Popular itemë„ í•™ìŠµ ê°•í™”")

print("\n[B] Model Architecture:")
print("  5. Attention mechanism: ì¤‘ìš”í•œ neighbor ê°•ì¡° (GAT)")
print("  6. Higher-order connectivity: 3-4 layerë¡œ ì¦ê°€")
print("  7. Residual connections: Layer ê°„ ì •ë³´ ë³´ì¡´")
print("  8. Node feature enrichment: Degree, popularity ë“± ì¶”ê°€ feature")

print("\n[C] Training Strategy:")
print("  9. Hard negative sampling: Low-rating itemì„ negativeë¡œ ì‚¬ìš©")
print(" 10. Curriculum learning: Easy â†’ Hard ìˆœì„œë¡œ í•™ìŠµ")
print(" 11. Multi-task learning: Rating regression + ranking ë™ì‹œ í•™ìŠµ")
print(" 12. Contrastive learning: Self-supervised pretraining")

print("\n[D] Inference Strategy:")
print(" 13. Ensemble: LightGCN + MF + Popularity ì¡°í•©")
print(" 14. Re-ranking: Diversity, coverage ê³ ë ¤")
print(" 15. Calibration: Score calibrationìœ¼ë¡œ threshold ìµœì í™”")

print("\n[E] Hybrid Approaches:")
print(" 16. Content-based features: ë§Œì•½ ë©”íƒ€ë°ì´í„° ìˆìœ¼ë©´ í™œìš©")
print(" 17. User/Item clustering: Community detection")
print(" 18. Transfer learning: Pretrained embedding í™œìš©")

print("\nğŸ¯ ì¦‰ì‹œ ì‹œë„í•  TOP 3 ì•„ì´ë””ì–´:")
print("  â­ #1: User/Item Bias ì¶”ê°€ - ê°œì¸ë³„ ì„ í˜¸ë„ í¸í–¥ ëª…ì‹œì  ëª¨ë¸ë§")
print("  â­ #2: Rating Regression + Ranking ë©€í‹°íƒœìŠ¤í¬ - Rating ì •ë³´ ì§ì ‘ í™œìš©")
print("  â­ #3: Hard Negative Sampling - Low-ratingì„ negativeë¡œ ì‚¬ìš©")

print("\n" + "=" * 80)
print("ë¶„ì„ ì™„ë£Œ!")
print("=" * 80)
