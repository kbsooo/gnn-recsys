# GNN ê¸°ë°˜ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ ì „ëµ ë¬¸ì„œ

## ğŸ“‹ Executive Summary

**í”„ë¡œì íŠ¸ ëª©í‘œ:** 668ëª… ì‚¬ìš©ìì™€ 10,321ê°œ ì˜í™”ì˜ ìƒí˜¸ì‘ìš© ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GNNì„ í™œìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶•

**í•µì‹¬ ë„ì „ê³¼ì œ:**
- ê·¹ì‹¬í•œ ë°ì´í„° í¬ì†Œì„± (1.5% density)
- ë¡±í…Œì¼ ë¶„í¬ (ì˜í™”ì˜ 70%ê°€ 5íšŒ ì´í•˜ ì‹œì²­)
- ì‚¬ìš©ìë³„ ìƒí˜¸ì‘ìš© í¸ì°¨ (20~5,672ê°œ)
- Cold-start ë¬¸ì œ

**ì¶”ì²œ ì ‘ê·¼ë²•:** LightGCN + BPR Loss + Negative Sampling ì „ëµ

---

## 1. ë°ì´í„° ë¶„ì„ ë° íŠ¹ì„±

### 1.1 ê¸°ë³¸ í†µê³„
```
ì´ ìƒí˜¸ì‘ìš©: 105,139ê°œ
ì‚¬ìš©ì ìˆ˜: 668ëª…
ì˜í™” ìˆ˜: 10,321ê°œ
í¬ì†Œë„: ~1.5% (ê°€ëŠ¥í•œ ì¡°í•©ì˜ 98.5%ê°€ ë¹ˆ ê³µê°„)

ì‚¬ìš©ìë‹¹ í‰ê·  ìƒí˜¸ì‘ìš©: 157ê°œ (ì¤‘ì•™ê°’: 70ê°œ)
ì˜í™”ë‹¹ í‰ê·  ìƒí˜¸ì‘ìš©: 10ê°œ (ì¤‘ì•™ê°’: 3ê°œ)
```

### 1.2 ë°ì´í„° ë¶„í¬ íŠ¹ì„±

**Rating ë¶„í¬ (ê¸ì • í¸í–¥):**
- 4.0ì : 27.4% (ê°€ì¥ ë§ìŒ)
- 3.0ì : 20.6%
- 5.0ì : 14.1%
- â‰¥3.5ì : ì•½ 64% (ê¸ì •ì  í‰ê°€ ë¹„ì¤‘ ë†’ìŒ)

**ë¬¸ì œì :**
1. **íŒŒì›Œìœ ì € ì¡´ì¬**: ìµœëŒ€ 5,672ê°œ ì‹œì²­ (í‰ê· ì˜ 36ë°°)
2. **ë¡±í…Œì¼ ì˜í™”**: 7,200ê°œ ì´ìƒì˜ ì˜í™”ê°€ 5íšŒ ì´í•˜ë§Œ ì‹œì²­
3. **í¬ì†Œì„±**: ëŒ€ë¶€ë¶„ì˜ user-item ì¡°í•©ì´ ê´€ì¸¡ë˜ì§€ ì•ŠìŒ

### 1.3 ê·¸ë˜í”„ êµ¬ì¡° ê´€ì 

```
Bipartite Graph:
[User Nodes] â†â”€â”€â”€ edges â”€â”€â”€â†’ [Item Nodes]
    668ê°œ                        10,321ê°œ
           105,139ê°œ edges
```

**ê·¸ë˜í”„ íŠ¹ì„±:**
- í‰ê·  user degree: 157
- í‰ê·  item degree: 10
- ë§¤ìš° ë¶ˆê· í˜•í•œ degree distribution
- ë§ì€ leaf nodes (ì—°ê²°ì´ 1~2ê°œì¸ ì˜í™”)

---

## 2. ë¬¸ì œ ì •ì˜ ë° ëª©í‘œ ì„¤ì •

### 2.1 Task ì •ì˜

**Primary Task:** Binary Classification (ì¶”ì²œ O/X)
- Input: (user_id, item_id) ìŒ
- Output: {O, X} (ì¶”ì²œ ì—¬ë¶€)

**Secondary Task:** Rating Prediction (í•™ìŠµ ì¤‘)
- Rating ì •ë³´ë¥¼ ë²„ë¦¬ì§€ ì•Šê³  í•™ìŠµì— í™œìš©
- ìµœì¢… ì¶”ë¡  ì‹œì—ë§Œ binaryë¡œ ë³€í™˜

### 2.2 Threshold ì „ëµ

```python
# ì œì•ˆ 1: Rating ê¸°ë°˜ (ì¶”ì²œ)
if predicted_rating >= 3.5:
    recommend = 'O'
else:
    recommend = 'X'

# ì œì•ˆ 2: Score ê¸°ë°˜
if model_score >= threshold:  # thresholdëŠ” validationìœ¼ë¡œ ê²°ì •
    recommend = 'O'
```

**ê·¼ê±°:** í˜„ì¬ ë°ì´í„°ì˜ 64%ê°€ 3.5ì  ì´ìƒ â†’ ì´ë¥¼ "ì¶”ì²œí•  ë§Œí•¨"ìœ¼ë¡œ ê°„ì£¼

### 2.3 í‰ê°€ ëª©í‘œ

**ìš°ì„ ìˆœìœ„:**
1. **Recall@10** â‰¥ 0.30 (ì‚¬ìš©ìê°€ ì¢‹ì•„í•  ë§Œí•œ ì˜í™”ë¥¼ ë†“ì¹˜ì§€ ì•Šê¸°)
2. **NDCG@10** â‰¥ 0.35 (ìƒìœ„ ì¶”ì²œì˜ í’ˆì§ˆ)
3. **Coverage** â‰¥ 30% (ë‹¤ì–‘í•œ ì˜í™” ì¶”ì²œ)

---

## 3. ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 3.1 Train/Validation/Test Split

**ì „ëµ A: Random Split (ì¶”ì²œ)**
```python
ê° ì‚¬ìš©ìë³„ë¡œ:
- Train: 80% (ìƒí˜¸ì‘ìš© ëœë¤ ì„ íƒ)
- Validation: 10%
- Test: 10%

ìµœì†Œ ìƒí˜¸ì‘ìš© ë³´ì¥: ê° setì— ìµœì†Œ 1ê°œ ì´ìƒ
```

**ì „ëµ B: Temporal Split** (timestamp ìˆë‹¤ë©´)
```python
ì‹œê°„ìˆœ ì •ë ¬ í›„:
- Train: ì²« 80%
- Validation: ë‹¤ìŒ 10%
- Test: ë§ˆì§€ë§‰ 10%
```

### 3.2 Negative Sampling

**ì¤‘ìš”ì„±:** GNN í•™ìŠµì— negative examples í•„ìˆ˜ (ì‚¬ìš©ìê°€ ì•ˆ ë³¸ ì˜í™”)

**ë°©ë²• 1: Random Negative Sampling**
```python
ê° positive interactionë§ˆë‹¤:
- ì‚¬ìš©ìê°€ ìƒí˜¸ì‘ìš©í•˜ì§€ ì•Šì€ ì˜í™” ì¤‘ ëœë¤ ìƒ˜í”Œë§
- ë¹„ìœ¨: positive 1 : negative 3~4
```

**ë°©ë²• 2: Hard Negative Sampling** (ê³ ê¸‰)
```python
- ì¸ê¸°ìˆì§€ë§Œ ì‚¬ìš©ìê°€ ì•ˆ ë³¸ ì˜í™” (ë” ì–´ë ¤ìš´ negative)
- Epochë§ˆë‹¤ ì¬ìƒ˜í”Œë§ (dynamic negative sampling)
```

**ì¶”ì²œ:** Randomìœ¼ë¡œ ì‹œì‘, ì„±ëŠ¥ ì •ì²´ë˜ë©´ Hardë¡œ ì „í™˜

### 3.3 ì „ì²˜ë¦¬ ìˆœì„œ

```python
1. ê²°ì¸¡ì¹˜ í™•ì¸ (ì—†ìŒ, OK)
2. User/Item ID ì—°ì†ì ìœ¼ë¡œ re-indexing (0ë¶€í„° ì‹œì‘)
3. Rating normalization (ì„ íƒì‚¬í•­):
   - Per-user z-score normalization
   - ë˜ëŠ” Min-Max scaling [0, 1]
4. ê·¹ë‹¨ì¹˜ ì²˜ë¦¬:
   - 5,000ê°œ ì´ìƒ ìƒí˜¸ì‘ìš© ì‚¬ìš©ì â†’ ìƒ˜í”Œë§ or ê°€ì¤‘ì¹˜ ê°ì†Œ
   - 1~2íšŒ ì‹œì²­ ì˜í™” â†’ ì œê±° ê²€í† 
5. Train/Val/Test split
6. Negative sampling dataset ìƒì„±
7. Graph êµ¬ì¶• (edge list í˜•íƒœ)
```

### 3.4 ë°ì´í„° ì¦ê°• (ì„ íƒ)

**Graph Augmentation:**
1. **Item-Item Similarity Edges**
   - ê°™ì´ ìì£¼ ì‹œì²­ëœ ì˜í™”ë¼ë¦¬ ì—°ê²°
   - Jaccard similarity > thresholdì¸ ê²½ìš°

2. **User-User Similarity Edges**
   - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì‚¬ìš©ì ì—°ê²°
   - Top-K similar usersë§Œ ì—°ê²°

**ì£¼ì˜:** ê³„ì‚° ë¹„ìš© vs ì„±ëŠ¥ í–¥ìƒ trade-off ê³ ë ¤

---

## 4. ëª¨ë¸ ì•„í‚¤í…ì²˜

### 4.1 LightGCN ì„ íƒ ê·¼ê±°

**ì¥ì :**
1. ë‹¨ìˆœì„±: Unnecessary transformation ì œê±°
2. íš¨ìœ¨ì„±: ë¹ ë¥¸ í•™ìŠµ ë° ì¶”ë¡ 
3. íš¨ê³¼ì„±: ì¶”ì²œ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥
4. í•´ì„ê°€ëŠ¥ì„±: Layerë³„ ì •ë³´ ì „íŒŒ ì¶”ì  ê°€ëŠ¥

**LightGCN í•µì‹¬ ì›ë¦¬:**
```
1. Userì™€ Itemì„ embedding spaceì— í‘œí˜„
2. Graph convolutionì„ í†µí•´ ì´ì›ƒ ì •ë³´ ì§‘ê³„
3. Multiple layersë¡œ multi-hop neighborhood ì •ë³´ ìˆ˜ì§‘
4. Layer-wise embeddingsë¥¼ weighted sum
```

### 4.2 ì•„í‚¤í…ì²˜ ìƒì„¸

```python
class LightGCN:
    Input:
        - User-Item bipartite graph (edge_index)
        - User embedding matrix: [num_users, embedding_dim]
        - Item embedding matrix: [num_items, embedding_dim]
    
    Layers:
        - L layers of graph convolution (L=2 or 3)
        - Layer l: e^(l) = Aggregate(e^(l-1), neighbors)
    
    Output:
        - Final embedding: weighted average of all layers
        - e_final = Î±â‚€*eâ° + Î±â‚*eÂ¹ + Î±â‚‚*eÂ² + ...
        - ì¼ë°˜ì ìœ¼ë¡œ Î± = 1/(L+1) (uniform weight)
    
    Prediction:
        - score(u, i) = embedding_user[u]áµ€ Â· embedding_item[i]
```

### 4.3 í•˜ì´í¼íŒŒë¼ë¯¸í„°

**ì´ˆê¸° ì„¤ì •:**
```python
embedding_dim = 64        # 32ëŠ” too small, 128ì€ overfitting ìœ„í—˜
num_layers = 3            # 2~3ì´ ì ì ˆ (ë„ˆë¬´ ë§ìœ¼ë©´ over-smoothing)
learning_rate = 1e-3      # Adam optimizer
batch_size = 1024         # Large batch (negative sampling ë•Œë¬¸)
dropout = 0.1             # Light regularization
weight_decay = 1e-4       # L2 regularization
```

**íƒìƒ‰ ë²”ìœ„:**
- embedding_dim: [32, 64, 128, 256]
- num_layers: [2, 3, 4]
- learning_rate: [1e-4, 5e-4, 1e-3]
- negative_ratio: [2, 3, 4, 5]

### 4.4 ëŒ€ì•ˆ ëª¨ë¸ (ë¹„êµêµ°)

**1. NGCF (Neural Graph Collaborative Filtering)**
- LightGCNë³´ë‹¤ ë³µì¡ (feature transformation í¬í•¨)
- ì´ë¡ ì ìœ¼ë¡œ ë” í‘œí˜„ë ¥ì´ ë†’ì§€ë§Œ ì‹¤ì œë¡  LightGCNê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ëª»í•¨

**2. GraphSAGE**
- Inductive learning ê°€ëŠ¥ (ìƒˆ ë…¸ë“œ ì²˜ë¦¬)
- Node feature í•„ìš” (ì—¬ê¸°ì„  ì—†ìŒ)

**3. GAT (Graph Attention)**
- Attention mechanismìœ¼ë¡œ ì¤‘ìš”í•œ ì´ì›ƒ ê°€ì¤‘ì¹˜
- ê³„ì‚° ë¹„ìš© ë†’ê³ , ì‘ì€ ë°ì´í„°ì…‹ì—ì„  ì´ì  ì ìŒ

**ì¶”ì²œ ì „ëµ:** LightGCN ë¨¼ì €, ì„±ëŠ¥ ë¶€ì¡±í•˜ë©´ NGCF ì‹œë„

---

## 5. ì†ì‹¤ í•¨ìˆ˜ ì„¤ê³„

### 5.1 BPR Loss (Bayesian Personalized Ranking) - ì£¼ ì¶”ì²œ

**ìˆ˜ì‹:**
```
L_BPR = -Î£ ln Ïƒ(Å·_ui - Å·_uj)

where:
- Å·_ui: positive item iì— ëŒ€í•œ ì˜ˆì¸¡ ì ìˆ˜
- Å·_uj: negative item jì— ëŒ€í•œ ì˜ˆì¸¡ ì ìˆ˜
- Ïƒ: sigmoid function
```

**ì¥ì :**
- Ranking ìµœì í™”ì— ì§ì ‘ì 
- Implicit feedbackì— ì í•©
- Pairwise ë¹„êµë¡œ ì•ˆì •ì  í•™ìŠµ

**êµ¬í˜„:**
```python
def bpr_loss(pos_scores, neg_scores):
    # pos_scores: [batch_size, 1]
    # neg_scores: [batch_size, num_negatives]
    
    diff = pos_scores - neg_scores  # Broadcasting
    loss = -torch.log(torch.sigmoid(diff) + 1e-8).mean()
    return loss
```

### 5.2 Weighted MSE Loss (ëŒ€ì•ˆ 1)

**ìˆ˜ì‹:**
```
L_MSE = Î£ w_ui Â· (r_ui - Å·_ui)Â²

where:
- w_ui = r_ui (rating ê°’ ìì²´ë¥¼ weightë¡œ)
- ë˜ëŠ” w_ui = 1 + Î±Â·r_ui
```

**ì¥ì :**
- Rating ì •ë³´ì˜ granularity ë³´ì¡´
- ë†’ì€ ratingì— ë” ì§‘ì¤‘

**ë‹¨ì :**
- Regression taskê°€ ë˜ì–´ ranking ìµœì í™”ì— ê°„ì ‘ì 

### 5.3 Multi-Task Loss (ê³ ê¸‰)

**ê²°í•© ì „ëµ:**
```python
L_total = Î»â‚Â·L_BPR + Î»â‚‚Â·L_MSE + Î»â‚ƒÂ·L_reg

where:
- L_BPR: Ranking loss
- L_MSE: Rating prediction loss
- L_reg: Regularization (L2)
- Î»: ê°€ì¤‘ì¹˜ (Î»â‚=1.0, Î»â‚‚=0.5, Î»â‚ƒ=1e-4)
```

**íš¨ê³¼:** ë‘ ëª©í‘œë¥¼ ë™ì‹œì— ìµœì í™”

### 5.4 ìµœì¢… ì¶”ì²œ

**Phase 1:** BPR Lossë§Œ ì‚¬ìš© (ë‹¨ìˆœì„±)
**Phase 2:** ì„±ëŠ¥ plateauë˜ë©´ Multi-Task Loss ì‹œë„

---

## 6. í•™ìŠµ ì „ëµ

### 6.1 Training Loop

```python
for epoch in range(num_epochs):
    model.train()
    
    # 1. Mini-batch sampling
    for batch in dataloader:
        users, pos_items, neg_items = batch
        
        # 2. Forward pass
        pos_scores = model(users, pos_items)
        neg_scores = model(users, neg_items)
        
        # 3. Compute loss
        loss = bpr_loss(pos_scores, neg_scores)
        loss += weight_decay * model.get_l2_reg()
        
        # 4. Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # 5. Validation
    if epoch % 5 == 0:
        val_recall = evaluate(model, val_data)
        if val_recall > best_recall:
            save_model(model, 'best_model.pt')
            best_recall = val_recall
        
        # Early stopping check
        if no_improvement_for_N_epochs:
            break
```

### 6.2 ìµœì í™” ì„¤ì •

**Optimizer:** Adam (adaptive learning rate)
```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-4
)
```

**Learning Rate Scheduler:**
```python
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='max',
    factor=0.5,
    patience=10,
    verbose=True
)
```

**Early Stopping:**
- Patience: 20 epochs
- Monitor: Validation Recall@10

### 6.3 ë°°ì¹˜ êµ¬ì„±

```python
Batch = {
    'users': [u1, u2, ..., u_B],
    'pos_items': [i_p1, i_p2, ..., i_pB],
    'neg_items': [
        [i_n11, i_n12, ..., i_n1K],
        [i_n21, i_n22, ..., i_n2K],
        ...
    ]  # Kê°œ negative samples per user
}

Batch size B = 1024
Negative ratio K = 4
```

---

## 7. í‰ê°€ ì „ëµ

### 7.1 í‰ê°€ ì§€í‘œ

**Recall@K** (ìµœìš°ì„ )
```
Recall@K = |Recommended âˆ© Relevant| / |Relevant|

K = 10 ë˜ëŠ” 20
í•´ì„: "ì¶”ì²œí•œ Kê°œ ì¤‘ ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ì¢‹ì•„í•œ ë¹„ìœ¨"
```

**NDCG@K** (Normalized Discounted Cumulative Gain)
```
DCG@K = Î£ (2^rel_i - 1) / logâ‚‚(i+1)
NDCG@K = DCG@K / IDCG@K

í•´ì„: "ìƒìœ„ ì¶”ì²œì¼ìˆ˜ë¡ ë” ì¤‘ìš”í•˜ê²Œ í‰ê°€"
```

**Hit Rate@K**
```
HR@K = |Users with at least 1 hit| / |Total users|

í•´ì„: "ìµœì†Œ 1ê°œë¼ë„ ë§ì¶˜ ì‚¬ìš©ì ë¹„ìœ¨"
```

**Precision@K**
```
Precision@K = |Recommended âˆ© Relevant| / K

ì£¼ì˜: Recallë³´ë‹¤ ëœ ì¤‘ìš” (ì¶”ì²œ ì‹œìŠ¤í…œì—ì„ )
```

**Coverage**
```
Coverage = |Unique items recommended| / |Total items|

í•´ì„: "ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ ì˜í™”ë¥¼ ì¶”ì²œí–ˆëŠ”ê°€"
ëª©í‘œ: > 30% (ë„ˆë¬´ ë‚®ìœ¼ë©´ ì¸ê¸° ì˜í™”ë§Œ ì¶”ì²œ)
```

### 7.2 í‰ê°€ í”„ë¡œí† ì½œ

**Leave-One-Out Strategy:**
```python
for user in test_users:
    # 1. ì‚¬ìš©ìê°€ ì•ˆ ë³¸ ì˜í™” 99ê°œ ëœë¤ ìƒ˜í”Œ
    # 2. Test setì˜ positive item 1ê°œ ì¶”ê°€ (ì´ 100ê°œ)
    # 3. 100ê°œ ì¤‘ Top-K ì¶”ì²œ
    # 4. Positive itemì´ Top-Kì— ìˆëŠ”ì§€ í™•ì¸
```

**Full Ranking Strategy:** (ë” ì—„ê²©)
```python
for user in test_users:
    # 1. ì‚¬ìš©ìê°€ ì•ˆ ë³¸ ëª¨ë“  ì˜í™” (~10,000ê°œ)
    # 2. ëª¨ë‘ ì ìˆ˜ ë§¤ê¸°ê¸°
    # 3. Top-K ì¶”ì²œ
    # 4. Test set positive itemsì™€ ë¹„êµ
```

**ì¶”ì²œ:** ê³„ì‚° ë¹„ìš©ì„ ê³ ë ¤í•´ Leave-One-Outìœ¼ë¡œ ì‹œì‘

### 7.3 Baseline ë¹„êµ

**í•„ìˆ˜ Baseline:**
1. **Random:** ëœë¤ ì¶”ì²œ
2. **Popularity:** ì¸ê¸° ì˜í™” ìˆœìœ„ëŒ€ë¡œ ì¶”ì²œ
3. **Matrix Factorization (SVD):** ì „í†µì  ë°©ë²•
4. **User-KNN:** User-based collaborative filtering

**ëª©í‘œ:** GNNì´ ëª¨ë“  baselineì„ ì´ê²¨ì•¼ í•¨

---

## 8. Breakthrough ì „ëµ

### 8.1 Negative Sampling ê°œì„ 

**ì „ëµ 1: Hard Negative Mining**
```python
# Epochë§ˆë‹¤ í˜„ì¬ ëª¨ë¸ë¡œ ì ìˆ˜ ë†’ì§€ë§Œ ì‹¤ì œë¡  negativeì¸ ìƒ˜í”Œ ì„ íƒ
hard_negatives = items_with_high_scores_but_not_interacted

íš¨ê³¼: ëª¨ë¸ì´ ì–´ë ¤ìš´ ì¼€ì´ìŠ¤ì— ì§‘ì¤‘ í•™ìŠµ
```

**ì „ëµ 2: Popularity-based Sampling**
```python
# ì¸ê¸° ì˜í™” ì¤‘ì—ì„œ negative sampling (ë” ì–´ë ¤ìš´ negative)
prob = popularity^Î±  # Î±=0.75 (popular items ë” ìì£¼ ìƒ˜í”Œ)

íš¨ê³¼: ì¸ê¸° í¸í–¥ ì¤„ì´ê³  discrimination í–¥ìƒ
```

### 8.2 Graph Augmentation

**Item-Item Graph ì¶”ê°€:**
```python
# Co-occurrence graph
if (user watched item_i AND item_j):
    add_edge(item_i, item_j, weight=frequency)

íš¨ê³¼: ì˜í™” ê°„ ìœ ì‚¬ë„ ì •ë³´ í™œìš©
```

**êµ¬í˜„:**
```python
# Heterogeneous graph
edges = {
    ('user', 'watches', 'item'): user_item_edges,
    ('item', 'similar', 'item'): item_item_edges
}
```

### 8.3 Ensemble ì „ëµ

**Model Ensemble:**
```python
# ë‹¤ë¥¸ random seedë¡œ 5ê°œ ëª¨ë¸ í•™ìŠµ
models = [model_seed1, model_seed2, ..., model_seed5]

# Prediction averaging
final_score = mean([m.predict(u, i) for m in models])

íš¨ê³¼: Variance ê°ì†Œ, ì•ˆì •ì  ì„±ëŠ¥
```

**Method Ensemble:**
```python
# LightGCN + Matrix Factorization
score_final = 0.7 * score_gnn + 0.3 * score_mf

íš¨ê³¼: ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì˜ ì¥ì  ê²°í•©
```

### 8.4 ê³ ê¸‰ ê¸°ë²•

**1. Contrastive Learning**
- ê°™ì€ ì‚¬ìš©ìì˜ ë‹¤ë¥¸ augmented viewë¥¼ ê°€ê¹ê²Œ í•™ìŠµ
- Self-supervised learningìœ¼ë¡œ representation í–¥ìƒ

**2. Knowledge Distillation**
- í° teacher model â†’ ì‘ì€ student model
- ì¶”ë¡  ì†ë„ í–¥ìƒ

**3. Meta-Learning**
- Cold-start ë¬¸ì œ í•´ê²°
- Few-shot learningìœ¼ë¡œ ìƒˆ ì•„ì´í…œ ë¹ ë¥´ê²Œ í•™ìŠµ

**ìš°ì„ ìˆœìœ„:** 1 > 2 > 3 (ì„±ëŠ¥ vs ë³µì¡ë„)

---

## 9. ì‹¤í—˜ ê³„íš (4ì£¼)

### Week 1: ê¸°ë°˜ êµ¬ì¶•
```
Day 1-2: ë°ì´í„° ì „ì²˜ë¦¬
- EDA ì‹¬í™” ë¶„ì„
- Train/Val/Test split
- Negative sampling dataset ìƒì„±

Day 3-4: Baseline êµ¬í˜„
- Random, Popularity baseline
- Matrix Factorization (SVD)
- í‰ê°€ ì§€í‘œ êµ¬í˜„

Day 5-7: í‰ê°€ í”„ë ˆì„ì›Œí¬
- Recall@K, NDCG@K êµ¬í˜„
- Visualization (í•™ìŠµ ê³¡ì„ , ë¶„í¬ ë“±)
```

### Week 2: GNN êµ¬í˜„
```
Day 1-3: LightGCN êµ¬í˜„
- PyTorch Geometric í™œìš©
- Graph êµ¬ì¶•
- Forward/Backward pass ê²€ì¦

Day 4-5: BPR Loss í•™ìŠµ
- Training loop
- Validation monitoring

Day 6-7: ì²« ë²ˆì§¸ ì‹¤í—˜
- Baselineê³¼ ë¹„êµ
- ë¬¸ì œì  íŒŒì•…
```

### Week 3: ìµœì í™”
```
Day 1-3: Hyperparameter Tuning
- Grid search or Random search
- Embedding dim, layers, lr ë“±

Day 4-5: Ablation Study
- Layer ìˆ˜ ì˜í–¥
- Negative sampling ratio ì˜í–¥
- Rating ì‚¬ìš© ìœ ë¬´

Day 6-7: Advanced Techniques
- Hard negative sampling ì‹œë„
- Graph augmentation ì‹¤í—˜
```

### Week 4: ì™„ì„±
```
Day 1-2: Ensemble êµ¬í˜„
- Multiple models í•™ìŠµ
- Ensemble ì „ëµ ì„ ì •

Day 3-4: ìµœì¢… ëª¨ë¸ ì„ ì •
- Validation ê²°ê³¼ ì¢…í•©
- Best configuration í™•ì •

Day 5-6: Test ë° ì œì¶œ ì¤€ë¹„
- Test data ì˜ˆì¸¡
- ì¶œë ¥ í˜•ì‹ êµ¬í˜„ (user, item, O/X)
- ëª¨ë¸ ì €ì¥ (.pt)

Day 7: ë¬¸ì„œí™” ë° ê²€ì¦
- ì½”ë“œ ì •ë¦¬
- README ì‘ì„±
- ìµœì¢… ì ê²€
```

---

## 10. êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°ì´í„° ì²˜ë¦¬
- [ ] Train/Val/Test split (80/10/10)
- [ ] User/Item ID re-indexing
- [ ] Negative sampling êµ¬í˜„
- [ ] Graph edge list ìƒì„±
- [ ] Data loader êµ¬í˜„

### ëª¨ë¸
- [ ] LightGCN êµ¬í˜„
- [ ] Embedding layer ì´ˆê¸°í™”
- [ ] Graph convolution layer
- [ ] Prediction layer
- [ ] L2 regularization

### í•™ìŠµ
- [ ] BPR Loss êµ¬í˜„
- [ ] Training loop
- [ ] Validation loop
- [ ] Checkpoint ì €ì¥
- [ ] Early stopping

### í‰ê°€
- [ ] Recall@K êµ¬í˜„
- [ ] NDCG@K êµ¬í˜„
- [ ] Hit Rate@K êµ¬í˜„
- [ ] Coverage ê³„ì‚°
- [ ] Baseline ë¹„êµ

### ìµœì¢… ì œì¶œ
- [ ] Test data ë¡œë“œ
- [ ] ì˜ˆì¸¡ ìˆ˜í–‰
- [ ] O/X í˜•ì‹ ì¶œë ¥
- [ ] ëª¨ë¸ ì €ì¥ (.pt)
- [ ] í†µê³„ ì¶œë ¥ (Total recommends)

---

## 11. ì˜ˆìƒ ë„ì „ê³¼ì œ ë° í•´ê²°ì±…

### ë„ì „ê³¼ì œ 1: ê·¹ì‹¬í•œ í¬ì†Œì„±
**ë¬¸ì œ:** 1.5% density â†’ ëŒ€ë¶€ë¶„ì˜ user-item ì¡°í•© ë¯¸ê´€ì¸¡
**í•´ê²°ì±…:**
- Negative sampling ratio ë†’ì´ê¸° (1:4)
- Graph convolutionìœ¼ë¡œ indirect connection í™œìš©
- Regularization ê°•í™” (dropout, weight decay)

### ë„ì „ê³¼ì œ 2: ë¡±í…Œì¼ ë¶„í¬
**ë¬¸ì œ:** ëŒ€ë¶€ë¶„ ì˜í™”ê°€ 3íšŒ ì´í•˜ë§Œ ì‹œì²­
**í•´ê²°ì±…:**
- ìµœì†Œ ìƒí˜¸ì‘ìš© threshold ì„¤ì • (5íšŒ ë¯¸ë§Œ ì˜í™” ì œê±° ê³ ë ¤)
- Popularity debiasing
- Item-item graphë¡œ ì •ë³´ ë³´ê°•

### ë„ì „ê³¼ì œ 3: Overfitting
**ë¬¸ì œ:** íŒŒì›Œìœ ì € ëª‡ ëª…ì´ ë°ì´í„° ì§€ë°°
**í•´ê²°ì±…:**
- íŒŒì›Œìœ ì € ë°ì´í„° ìƒ˜í”Œë§
- Strong regularization (L2, dropout)
- Early stopping (patience=20)

### ë„ì „ê³¼ì œ 4: Cold-start
**ë¬¸ì œ:** ìƒˆë¡œìš´ ì‚¬ìš©ì/ì˜í™” ì¶”ì²œ ì–´ë ¤ì›€
**í•´ê²°ì±…:**
- Popularity-based fallback
- Graph augmentationìœ¼ë¡œ indirect information
- Meta-learning (ê³ ê¸‰)

---

## 12. ì„±ê³µ ê¸°ì¤€

### ìµœì†Œ ëª©í‘œ (í•„ìˆ˜)
- [x] Recall@10 â‰¥ 0.20
- [x] NDCG@10 â‰¥ 0.25
- [x] Baseline (MF) ëŒ€ë¹„ 10% ì´ìƒ ì„±ëŠ¥ í–¥ìƒ
- [x] Coverage â‰¥ 20%

### ëª©í‘œ (ê¸°ëŒ€)
- [x] Recall@10 â‰¥ 0.30
- [x] NDCG@10 â‰¥ 0.35
- [x] Baseline ëŒ€ë¹„ 20% ì´ìƒ ì„±ëŠ¥ í–¥ìƒ
- [x] Coverage â‰¥ 30%

### ìš°ìˆ˜ ëª©í‘œ (ë„ì „)
- [x] Recall@10 â‰¥ 0.40
- [x] NDCG@10 â‰¥ 0.45
- [x] Ensembleë¡œ ì¶”ê°€ 5% í–¥ìƒ
- [x] Coverage â‰¥ 40%

---

## 13. ì°¸ê³  ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
1. **LightGCN** (SIGIR 2020): "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation"
2. **BPR** (UAI 2009): "BPR: Bayesian Personalized Ranking from Implicit Feedback"
3. **NGCF** (SIGIR 2019): "Neural Graph Collaborative Filtering"

### êµ¬í˜„ ì°¸ê³ 
- PyTorch Geometric: https://pytorch-geometric.readthedocs.io/
- RecBole (ì¶”ì²œ ì‹œìŠ¤í…œ ë¼ì´ë¸ŒëŸ¬ë¦¬): https://recbole.io/

### ë°ì´í„°ì…‹ ë²¤ì¹˜ë§ˆí¬
- MovieLens (ìœ ì‚¬ ë°ì´í„°): ì¼ë°˜ì ìœ¼ë¡œ Recall@20 = 0.3~0.5
- Gowalla (ì²´í¬ì¸ ë°ì´í„°): Recall@20 = 0.15~0.25

**í˜„ì¬ ë°ì´í„° ì˜ˆìƒ ì„±ëŠ¥:**
- Baseline (MF): Recall@10 â‰ˆ 0.18
- LightGCN: Recall@10 â‰ˆ 0.25~0.35
- LightGCN + Ensemble: Recall@10 â‰ˆ 0.30~0.40

---

## 14. ê²°ë¡  ë° í•µì‹¬ ì›ì¹™

### í•µì‹¬ ì›ì¹™
1. **ë‹¨ìˆœí•¨ì´ ê°•ë ¥í•˜ë‹¤**: LightGCNì´ ë³µì¡í•œ ëª¨ë¸ë³´ë‹¤ ìì£¼ ì´ê¹€
2. **ë°ì´í„°ê°€ ì™•ì´ë‹¤**: Negative sampling ì „ëµì´ ì„±ëŠ¥ ì¢Œìš°
3. **ê²€ì¦ì´ ì¤‘ìš”í•˜ë‹¤**: Validationìœ¼ë¡œ overfitting ë°©ì§€
4. **ë¹„êµê°€ í•„ìˆ˜ë‹¤**: Baseline ì—†ì´ëŠ” ì„±ê³µ íŒë‹¨ ë¶ˆê°€

### ìµœì¢… ì¶”ì²œ Configuration
```python
model = LightGCN(
    num_users=668,
    num_items=10321,
    embedding_dim=64,
    num_layers=3,
    dropout=0.1
)

optimizer = Adam(lr=1e-3, weight_decay=1e-4)
loss_fn = BPR_Loss()
negative_ratio = 4
batch_size = 1024
epochs = 200 (with early stopping)
```

### ì„±ê³µ ê³µì‹
```
Good Data Preprocessing
+ Simple but Effective Model (LightGCN)
+ Smart Negative Sampling
+ Careful Hyperparameter Tuning
+ Ensemble (if needed)
= Top Performance
```

**í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸš€**